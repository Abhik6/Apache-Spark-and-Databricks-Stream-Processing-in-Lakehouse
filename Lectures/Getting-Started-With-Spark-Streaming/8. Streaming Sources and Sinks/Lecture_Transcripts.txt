Welcome back.

In this lecture we want to learn about spark streaming sources and sinks.

Spark Structured Streaming offers you a bunch of sources and sinks.

Sources are those where you can connect and consume data in a streaming mode, and sinks are those where

you can save your results or send your data after doing all the processing.

So let's try to understand what sources and sinks are available for spark structured streaming.

So let's start.

We already learned how Spark structured streaming application works.

We learn that when we start our spark Structured streaming application, it will start a background

thread, which is known as Streaming Query and streaming query is responsible for triggering Microbatches,

and each micro batch is also configured with the input data for that micro batch.

And once configured, the read stream inside your micro batch will go and read a specified input data

from a streaming data source, right?

We learned about one streaming data source, which is a directory location.

In the earlier examples, we learn to read data from a landing zone where we can ingest data files,

and this read stream can read data from the landing zone, but spark supports some other streaming sources.

Landing zone or the directory is not the only streaming source.

Similarly, each microbatch once it completes its processing, the write stream can write data to a

streaming sink.

Right?

We learned about table write table as a streaming sink.

So we learned how to write.

A stream can write data to a target table.

But target table is not the only supported streaming sink.

We have a couple of more other streaming sink supported by Spark structured streaming.

So let's try to understand what all the streaming sources and streaming sinks are supported by Spark

Structured Streaming.

Here it is.

The list is not very long.

So the first type of source is a directory source or a file source.

So we can have a landing zone directory defined as our streaming source.

And a spark ready stream can read data files from inside the directory.

That's the most commonly used streaming source in real time spark structured streaming applications.

If you have integrated spark with the Delta Lake right then Delta Table is also a supported streaming

source, so you can read data from a delta table.

You can read incremental data from a delta table and start processing it.

So delta table is another type of streaming source.

Other than these two is Park implicitly or Apache spark comes with a support for Kafka, so you can

use Kafka as a streaming source and read data incremental data from the Kafka using Spark read Stream

API.

Other than these, there are many other possibilities other type of source connectors available, but

those are available through the external connect.

So different, different vendors, they come up with the connectors for their systems.

So that Spark Read Stream can connect to their system and read incremental data.

Right.

Databricks, as a company is also engaged in offering and developing different types of connectors for

different or some popular data integration systems or some popular source systems.

So there are many other, but those are available as an external connector.

Similarly, a spark right stream supports some sinks here.

We have already used Delta Table as a sink because we are using Databricks for our development or learning,

and Databricks Runtime Environment comes integrated with the delta tables.

Right.

Delta Lake.

And that's why we can easily use Delta Table as a streaming sink.

Right.

But other than delta table a directory.

Can be used as a streaming sink also, so you can have a target landing zone directory or a target directory

where you can use write stream to write data files and write stream will write data files into the target

directory so that one is known as file sink or a directory sink.

Other than these two, spark also comes with the Kafka connector Kafka streaming connector, so you

can write the output into a Kafka topic also.

So that's another option.

Spark also comes with a for each connector.

So for each connector is a Swiss Army knife.

Right.

So if you want to send data to some kind of system which is not supported, which is not in this list,

or there is no external connector already pre-built for that system, you can use for each to write

to any arbitrary system.

So this one is very flexible.

It gives us a lot of other flexibilities.

Also, there are a lot of techniques that we can use using the for each sink.

And we will keep on learning that how to leverage the power of for each, because it's a Swiss army

knife of spark sinks.

So that's all about it.

We have very limited connectors available out of the box, and others as a source connector or as a

sink connector are available or provided by the third party vendors.

And that's all about the introduction of sources and sinks.

In the coming lectures, we will continue using all of these sources and sinks.

So you have a hands on experience with those sources and sinks.

See you again.
