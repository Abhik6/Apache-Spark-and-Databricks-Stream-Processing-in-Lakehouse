Welcome back.

Spark structured streaming is extremely powerful.

It allows you to write a single, unified piece of code and execute it as a batch process or as a streaming

process.

You don't need to write two different pieces of code or make any changes to your code.

Same code using spark structured streaming APIs can be executed as a batch or as a stream processing

job.

How to do that?

Let's try to understand that using one example in this lecture.

So let's start.

In the earlier lectures we created one example.

Let me quickly show you what we did.

We created an example which reads data from the landing zone.

The data comes as invoice in Json format.

So we receive data files in the landing zone and each file contains multiple invoices.

Each invoice is a Json format record as shown here.

So we wrote one process which reads data from the landing zone, creates raw data frame, and then applies

some processing to produce the result in a target table.

The processing was as simple as take the each invoice and flatten it.

Flatten it to take out each line item from the invoice.

So each line item should be converted into a single record with some common details taken from the invoice

itself.

So we wrote it.

We wrote a streaming application for that and we started.

We executed it as a stream processing application.

Now we want to rewrite that same application once again, or take the same code and make some changes

in that application to make it a unified code, which can be executed as a batch processing job, or

the same code without any changes, can be executed as a stream processing job as a continuous stream,

right?

So how do we do that?

That's what we want to learn in this lecture.

And then we will also rewrite our test cases so that we can test the batch processing job approach,

as well as the stream processing job approach.

Since the same application can be executed in two modes batch processing mode and stream processing

mode.

So we want to write two test cases for the same one test case to test it in batch mode, and one more

test case to test it in stream processing mode.

So let's start coding.

Okay, so I'm in my work space.

So let me go to my spark streaming project.

And open my old code invoice stream.

And also create a new notebook.

Let's call it.

Streaming batch.

We will create an application in this notebook, which can be executed as a streaming application,

and it can be also executed as a batch processing job.

So.

Let's look at the old code.

So here is my invoice stream code.

And this code is reading data from landing zone.

It's reading invoices using a well defined schema and then exploding the invoices, then flattening

the invoices and finally appending into the invoice line items table.

And here is my process code.

Right.

So I'm assuming that you already understand this code.

We created it in the earlier lecture.

So let me copy it and paste it here.

And we will modify this same code to work like a batch processing job and also work like a stream processing

job.

So let's start making changes and you will understand how little changes are required to make this application

as a unified application, which can be executed as a batch and can also be executed as a stream processing

job.

So first thing, let me change the class name.

Um NYSE stream batch.

That should be good.

The constructor doesn't require any change.

His schema is not changing, so we don't need to make any change in the schema.

Read invoices is also not changing.

We are reading from the same landing zone and rest of the code also remains the same.

Exploding.

The invoice is also the same because there is no change in the business logic, so flattening and exploding

will not change.

The changes will happen in the append invoices because that's where we are doing the write history.

So whatever trigger we want to specify should go along with the write stream.

So all the changes will be there.

So.

This append invoice takes one argument, which is flattened, and write that flatten into a target table,

which is invoice line items.

So let's add one more parameter to this function and let's call it trigger.

And I want to give a default value as batch.

So if we don't pass any trigger it is assumed to be a batch processing trigger.

Right.

And.

Instead of returning the query from here, what I want to take, just hold the query.

For a while.

We will return it at the end.

But for now, let's hold the query.

And then at the end what do we want to do?

We want to check if.

If trigger is batch, then we will return the query.

But before returning the query, we will make some changes into the query.

So what do we want to do?

We want to add a trigger route to the query, which we have not added here while defining the right

stream and while defining the query.

So it is also possible that you define the query first, and then later you add a trigger to that query

or add some additional options and configuration.

All that is possible.

So let's add a trigger here.

So I want to add a trigger.

As available now.

Why?

Because the trigger type is batch.

So if this function is called with the trigger type batch, we want to run it as a batch.

Or we want to configure the trigger type as available now trigger.

And.

We have to take that action from here out.

Because if we define the action here, the query will immediately start executing.

So instead of defining the action here, we take the query and then add the trigger and finally add

the action.

And let me put all this into a pair of.

Parentheses and then we return it.

And then we add an else condition.

Right and same thing will be given here also.

Return the query, but this time we want to.

Change the trigger type as processing time.

And let's say processing time equal to trigger.

So what do we expect here?

While calling the append invoices function we will pass trigger as a parameter.

And trigger can take two types of values.

Default batch right.

Or it can take some.

Trigger interval, let's say five seconds.

10s 30s.

Right.

So if trigger type is equal to batch, what do we want to do.

Take the query definition.

Query is not triggered here right.

Because we don't have an action here.

We are just giving right stream format option output mode etcetera.

All these are defining the query.

It's not going to trigger the query until unless we add an action at the end.

So we take out the action from there.

And we add that action later.

And before adding the action we specify the trigger.

So if trigger type equal to batch we will configure it to run as a batch processing job with available

now equal to true.

So what it will do it will take all the data available in the landing zone, execute the one micro batch

and finish and then streaming query will also stop automatically.

We don't have to manually stop it right.

So that's how it will work.

And if trigger type is not batch then we assume that trigger type is given some time duration for example

30s.

So in the else we configure the query with trigger type as processing type processing time equal to

the trigger time whatever time is given, and then add the action.

Finally we return the query from here.

So that's all.

That's all we need to make change in our write stream.

Now our write stream or this streaming query is now configured with the trigger.

And we have two options here.

Either it will run as a single batch, or it will run as a continuous stream at a regular processing

time interval.

I want to take it as an opportunity to help you understand with one more additional option.

So let me type that option here and then I'll explain it.

So the spark is streaming also gives you an option to define max files per trigger, and I'm setting

it as one.

So what will happen if we execute a continuous streaming trigger?

Like with the processing time 30s, the streaming query will start reading data from the landing zone,

but each micro-batch will process only one file, right?

So let's assume in our landing zone if we have ten files already ingested.

So and we are starting the query.

So if we start the query, the first micro-batch will read only one file and process only one file.

Then the next micro-batch will read one more file, and the third Micro-batch will read third file like

that.

So we are limiting the number of input files.

And this is very important because sometimes you want to make sure that each micro-batch finishes in

a desired time.

Let's say 10s right.

And you don't want each micro-batch to take whatever data is available.

You want to define a finite limit that each micro-batch should take one file or five files or ten files.

Default is 1000 files, right?

So if you don't specify that max files trigger default value is 1000.

So it can take up to 1000 files if they are available in the streaming source or in your landing zone.

So but we can limit it.

Right.

So one is very small number.

You should not be doing one file unless until you know that each file is significantly large, right.

Usually in a streaming ingestion we have a small small files.

So 1000 is a reasonably good number.

But what I want to make sure is that we know that max file trigger is there as an option, and which

allows us to limit the input for each micro batch.

Right.

So if it is an processing time trigger or unspecified trigger, each micro batch.

In our example will read only one file and keep on processing file by file each micro-batch.

Right.

But what about the one time trigger?

If one time trigger or available now trigger is configured and let's say you have ten files already

ingested in your landing zone.

What this available now will do available now will make sure all ten files are processed before the

streaming query is stops.

That's why the trigger name is available now.

So when the streaming query starts at that point of time, whatever data is available in the landing

zone, all will be processed.

Whether it takes one micro batch or it takes ten micro batch.

But streaming query will not stop unless until all the data currently available in the landing zone

is processed successfully.

So in this example, let's say before starting my query, if I have ingested ten files and I configured

my streaming query to run as available now.

So the streaming query will start one micro batch read only one file because I configured max file trigger

equal to one, and then it will also start second micro batch to process the second file.

Once second file is processed, it will start third Micro-batch fourth micro batch like that.

It will make sure all the ten files which were already available at the time of starting the query are

processed, and then it will stop.

It will not process any new data arriving at the landing zone.

Right.

And that's why it is known as available now trigger whatever is available as of now, process all that.

I hope that makes sense.

Great.

So we configured our append invoices.

Now what's next?

The process is the final method which we will be calling from outside.

So we have a little changes here.

Also the first thing is we need to define the trigger as a parameter here also.

So let's make it default batch.

Right because we will be calling process method from outside.

And so process method should also get the trigger so that it can pass trigger while calling the append

invoices.

And we are calling append invoices here.

So we need to pass the trigger to the append invoices.

And that is all.

That's all we need to do.

And this code is now ready to be executed as a batch or as a continuous stream.

Right.

All we have to do is to pass an appropriate trigger.

So let me connect to my cluster and run it once.

So if there are any thing, any typos or something which can be detected is detected.

So executed successfully.

Most likely I've not made any typos.

Now what is next?

We are done with this.

Let's write a test suit for this.

So I'll go to.

Invoices.

Stream test suit because I'll copy this invoice stream test suit and recreate a new test suit based

on that.

Right.

Modifying or adding some additional code.

So I opened that here.

Now let me open create a new notebook.

And let's call it.

A streaming batch test suit and what I want to do.

I want to, um, copy all this code, but import will be different.

So.

I want to import my streaming batch notebook, and then I'll copy all this code the test suit, code

the entire thing and paste it here.

And then this is also required.

So I'll copy that also and paste it in the next cell.

And then we will start modifying it.

Right.

So what changes do we want to make?

Let's change the name of the test suit.

Invoice stream.

Test suit.

Maybe change it to.

Streaming batch test suit.

And where do you think we need to make changes.

So at very less places.

So constructor remains the same.

Clean test should be same.

I don't expect any change in the clean test because we are testing the same kind of data, which we

already tested using the old code.

So everything remains the same.

Data ingestion is also same.

Assertion is also same.

Wait for micro batch is also same.

There is no change and run test is also same.

But let's rename it to.

Run stream tests.

Right.

So if we want to execute our streaming batch.

Application and test it using the streaming mode, the code for this will remain as it is, right?

The only difference is I'll have to change the object name.

So let me open.

My streaming batch test here.

What is the object name?

Invoice stream batch?

So we'll instantiate the invoice stream batch here.

And while calling the invoice stream batch process method, we need to pass the trigger.

So this is this test is going to execute the streaming batch as a stream.

Right.

So to execute it as a stream I need to pass some time interval.

So let's pass 30s.

So I want my application to run as a continuous stream where each micro batch is triggered after every

30s.

And that's all.

We'll test the first iteration.

And for testing, first iteration will ingest one data file, wait for micro batch to pick up data and

process it, and then we will assert the results.

First iteration is passed.

Similarly, second iteration and third iteration.

And then we will stop the streaming query.

That's how we will create a streaming mode test case.

But we also want to create one more test case which runs this application in the batch mode.

Right.

So let's define one new function for that.

Let's call it run batch test.

What do we want to do?

Similar things.

Right.

Run the cleanup, instantiate it.

At least these two things we need to do here also.

So we run the cleanup and create a instance of our application.

And then we want to start testing the iterations.

It is not a streaming query right.

So I will not start the streaming query like I started before testing my iteration.

Because if I start streaming query with the batch mode, it will start, will not find any data in the

landing zone and then immediately it will stop.

So what do we want to do?

We want to ingest some data first and then start the streaming query.

So it starts finds data at the landing zone processes that and stops.

Right.

And then we again execute the second iteration after ingesting some more data and then execute it,

start it again.

And then it should process the second batch of the data and stop it automatically.

Right.

So let me copy code.

Here from there to here and we will modify it accordingly.

Right.

So let me change the message here.

Testing first batch of invoice stream.

Right.

And we ingest one batch of data.

Let me just one more file there.

Right.

Instead of ingesting only one.

Let's ingest one more file.

So we know that initially there will be two files to process.

But we defined max files for trigger equal to one.

So the first micro batch will process only one file.

Then the second micro batch will also start because we want to process it as available now.

And then after second micro batch it should automatically stop.

So we ingest two sets of data.

Then before we wait.

We will start the query.

Right.

So how to start the query.

Here is the code.

But we don't want to start it as a 32nd microbatches, right?

We want to start it as batch.

Batch is the default parameter, but let's specify it explicitly.

So we start it as a batch then wait for 30s.

So the micro batch processes both the files.

And once we finish processing both the files we want to assert the result.

But since we are processing two files, the result should have 2506 records.

Not 12491249 is after processing one file.

And if that is done, we are done processing or we are done validating the first batch.

Let's copy it and create a one more round of validation.

And this time.

We want to ingest the third file and testing second batch of invoices.

Stream right.

Ingesting data, third file.

And then we start it and then wait for 30s.

And finally we validate the final outcome.

The final outcome is this.

Right.

So.

Change this and then validate it.

And we don't need to stop the query like we are stopping here in the streaming test.

We don't need to stop it in the batch test, because in the batch streaming query will start here and

in 30s.

I'm expecting this streaming query to process both the data files.

We will do the validation and then streaming query should automatically stop.

And that's why we need to start it again here after ingesting the second data file or the third data

file.

And then we wait for another 30s for this micro batch to finish the processing.

And then we validate and that's all we are done.

So now let's try to run it.

Uh, what is my class name.

A streaming batch test suit.

So we.

And just this.

And.

Let's call it streaming batch and then streaming batch.

And first we want to run the run stream test.

Right.

So first we want to test the streaming mode.

And once streaming mode is done in a separate cell I want to test the batch mode.

So.

Let's call the.

Run batch tests.

Let me connect to my cluster and try running it.

Hoping I don't have any error or typo.

Let's run it.

So my extreme test is running.

Clean up is started.

Great.

So we finished executing all the three iterations of streaming mode, right?

All passed.

Now it's running the batch mode.

So let's wait for it to complete.

Great.

So both the iterations passed.

Great.

So we wanted to learn and understand the mechanics of using available now trigger or writing a unified

code which can be executed either in batch mode or in streaming mode.

And we write the code using spark structured streaming APIs so that we can implement the incremental

data processing without any extra code write without implementing it manually.

And that's what we learned in this lecture.

See you again.
