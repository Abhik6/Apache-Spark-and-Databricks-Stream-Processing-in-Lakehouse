Welcome back.

In this lecture, we will try to create our first streaming solution, but we will go step by step.

In the first step, we will try to understand and frame a problem statement.

And in the next step we will create a batch processing solution for the same.

And then we will analyze what problems do we have in the batch processing solution and how we can overcome

those problems with a streaming solution.

And finally, we will transform our batch processing solution into a streaming solution.

So let's get started.

Okay.

So let me start with the problem definition and solution overview.

We want to build a word count application.

Simple.

So we have a landing zone here, Data slash text.

This is our landing zone directory.

We will ingest data files in this landing zone directory, and our application starts from here onwards.

Right.

So in the first iteration, whatever data files are ingested in the landing zone, we want to read those

files, create a raw data frame, then apply some data quality rules to eliminate poor quality data.

And then do the processing.

The processing is simple.

We simply want to do the word count.

And once we have the word count, we will push the result into a word count table.

And finally, our consumer applications want to build a report based on the word count table.

Simple.

So let me show the report once.

So here is the report.

On the x axis, we have letters, alphabetical letters, and on the y axis we have the count.

So the report simply shows how many words start with alphabet A write.

This is the count of the words starting with a.

Similarly, this is the count of words starting with S.

That's what we want to do.

I hope you understand the problem.

Very simple problem.

You have a data file.

Read the data file.

Do the word count and save the word count result into a word count table.

That's all.

That's the first iteration.

And in the next iteration, again, read all the new files.

Process the data, calculate the word count and save it into a word count table.

That's all.

Let's start doing that.

Let me log in to my databricks environment.

Here I am.

Or doing anything, we will need two things.

One, a compute cluster and some sample data.

So let me go to compute menu and create my compute cluster.

So while my cluster is starting.

Let's arrange for the sample data.

Go to data menu, click the Dbfs Come to file store.

And upload the sample data directory.

So I already have a sample data directory on my desktop.

I'll simply drag and drop that directory here and it will start the upload process.

The sample data directory is included in your course.

It's available in the resources, so you can also do the same if you want to follow along.

That's all.

All the data is imported.

Click.

Done.

And you are done.

I uploaded three data files and I'll be using them for my word count as a sample data.

Let's close this.

Go to work space.

And come to your user home directory.

Inside Home Directory, I'll create one new folder.

And inside the folder, let's create a notebook so we can start writing code.

Let me zoom it a little bit.

Change the notebook name.

Good.

So we are now ready to write a spark application which reads Data from the landing zone applies some

processing logic to calculate the word count and then finally save the word count into a target table.

So let me start coding it and you can watch me doing that.

So first step is to define a base directory location.

I'll be using this base directory location.

Now let's start coding.

So step one is to read the data from the landing zone and create a data frame.

That's all.

I have a code which reads the data.

Data format is text file and we use dot as a line separator in the text file and we load it from the

base directory slash data slash text.

That's our landing zone location.

So step one is done.

I'm done loading the data.

What is this step to start processing it?

So let me write code for processing.

I'll these functions while processing.

So I'm just importing them.

So what I want to do, take the lines data frame, which is my.

Lines read from the text file and then select the value column.

Value column is where each line is loaded and what we want to do.

We want to split the line.

By space into words.

So let's do that.

Right.

So I'm splitting the lines by space to create an array of words, but it's an array.

We want rows.

So let me explode it.

Then.

So we split the lines, then explode it and create an exploded column.

So that column let me give an alias to the exploded column.

That's all.

So I have the raw words.

The second step We want to create a quality words.

So what does it mean?

We want to eliminate those words which are null or those words which starts with numeric one, two or

special characters or symbols.

We want to eliminate all those garbage records and create a quality data frame.

So let's do the quality data frame creation.

Okay, so I'm taking words.

And then first thing I want to do is to trim it.

So let's do the trim.

That's the first operation of implementing quality.

Then what next?

I want to convert everything into lowercase.

Right.

That's also one kind of quality check.

So trim is done.

Lower is done here.

So let's give Alias again.

And then I want to apply some filters.

Remove the null words.

Then.

Apply a regular expression check to take only those words that start with an alphabetical letters,

and then we can put all this into a.

Parentheses.

That's all.

So I have the quality words Now, what is next?

The final step of the processing.

Once we have the quality data, I can apply my aggregation logic.

So let me do that.

That's all.

That's all my processing.

So in the first step, I'm taking the lines that I ingested from the landing zone and creating a raw

data frame.

Raw words.

Then on the raw words, I'm applying some quality check rules and I'll get the quality words.

Finally, I'll calculate word counts, which is as simple as group by word and then take count.

So my data processing is done.

What is next?

The last step is to save this into a table.

So how do you do that?

Simple.

Take these word counts.

Dot, right?

Let's choose.

Delta format.

Then moored.

Overwrite.

And finally.

Save as.

Table.

Table name is.

Word count.

Table.

And maybe I can put all this into.

Parentheses, so don't get line continuation errors.

And that's all.

That's all my coding activity.

I just kept on typing.

We may have some typos or some kind of error, so I want to run it once.

But before executing this one thing, I need to make sure that this directory exists.

Right.

This directory does not exist yet, so I'll have to do some kind of initial setup activity.

So let me create a new notebook here.

So I'll go back to workspace.

And create a new notebook.

Give a name to the notebook.

I'll write one word count ingestion logic here.

But before that, I'll do the setup and clean up activity at the in the same notebook.

So let me write some code for setup and cleanup.

I need to define my base directory location.

Then next step is to drop the word count table.

Let me give a comment here so you understand.

Clean up and set up.

Right?

So define the base directory here.

Then I'm executing drop table if exists word count table.

So before I start running my application, I want to make sure this word count table does not exist.

I get a clean environment.

Then I want to clean the wordcount table directory also.

Then I want to delete two more directories checkpoint directory and data text directories.

These directories we will be creating in our application.

So as a cleanup activity, I want to make sure these are deleted before we start running our application

and we get the clean environment.

And finally, I want to create the landing zone before we start it.

So that's my cleanup code.

So let me connect my cluster here.

And try running this.

If all this runs successfully, my cleanup is done and my landing zone directory is created and I'm

ready to test my application that I coded in the previous notebook.

Done.

So this is done.

Now, let me go back to my.

A streaming word count notebook.

I coded it.

Let me run it once, at least.

First line.

It worked.

Now run this.

This also worked.

And finally our processing logic.

This also worked.

And let's try if this also works.

Okay.

It's running, so it looks like I didn't make any typo.

Good.

Okay, so everything worked.

Let me.

Clear the cell outputs once and.

Finish the notebook.

So we are now ready to test this write test and understand if everything works correctly.

But before that, let's go back to our solution diagram and check if we have done everything correctly.

Great.

So this is what we wanted to do.

We wanted to ingest data from the landing zone and create a raw data frame, then apply some quality

rules and create a high quality data frame and then apply the aggregate and create the word count and

finally save it to the target table.

And that's what we have done.

And here is the code.

We are reading data from the source directory.

Then applying all the transformations and completing our processing and finally writing it to the target

table.

What is the next step?

The next step is to test this code.

But how do we test it?

There are two approaches manual testing and automated testing.

Manual testing is not a recommended way of doing the testing.

So we need to write test cases for this code, right?

So that's the next activity we need to perform.

But how do we write test cases for being able to write automated test cases or unit test cases for your

code?

You must have structured your code as callable functions so that you can call a function, get the result

and verify the result and pass or fail the test case.

So this approach of coding is not a recommended approach where we keep on writing a straightforward

code in the notebook and everything is dumped there as a sequence of code.

So for being able to write test cases, we must refactor our code, refactor it to create functions,

or maybe create a class and add methods in the class and move all the functionality in those methods

so that we can call the methods, get the result, and then validate whether the method is performing

as expected and pass or fail the test case.

So the next activity is to refactor this code.

I've already done that refactoring, so let me show you the refactored code.

The code is same.

Nothing is changing, but I'm putting it into a structure.

We will be creating one class and inside the class we will create various small, small methods for

doing one specific thing.

And then those methods can be tested writing automated test cases.

So let's do that.

So here is the refactored code.

What I did, I created a class.

Class name, age batch, word count.

Right.

And I have a class constructor which defines the base data directory location.

And then have created some functions inside the class.

First function is get row data.

Second function is get quality data.

Third one is get word count, fourth one is overwrite word count, and each function is doing a specific

thing.

The code is still the same that we wrote earlier.

So let's look at the get raw data, what we are doing.

I am reading raw lines from the landing zone and creating the lines data frame.

And then we take that lines data frame and convert it into words.

A data frame of words and return that data frame.

So this function, when we call it, it reads the data from the landing zone and returns a data frame

of words, and then we can test it.

We can write unit test cases for this get raw data function.

It would be very simple.

Call the function, get a data frame, and then validate the data frame.

If it is as per your expectation based on the test data, your data frame matches with your expectation.

You can pass the test.

If it doesn't match, you can fail the test.

We will do that in a minute.

Similarly, the second function is get quality data, which takes the raw data.

We read the data using the get raw data function and then pass that raw data data frame to the get quality

data.

And the get quality data function is responsible for applying the data quality rules.

Filter out nulls, make sure every word is starting with an alphabet and things like that.

Then finally we have a function for get word count, which takes the quality data frame as an input

and returns the aggregated result.

And the last one is to write the aggregated results to the target table.

That's all.

So we have four functions.

We broke the functionality into four functions, right?

And these functions are testable.

And finally, at the end I have a main method and name method name is word count.

So word count is stitches.

All these three functions for functions together.

Right?

First step we call the get raw data and get the raw data frame.

Then we call the get quality data, passing the raw data, and in return we take the quality data and

then we pass the quality data to get the results.

And last step is to overwrite the word count table with the result data frame.

That's all done.

So this is the approach we should be taking for making sure that whatever code we are writing, whatever

application we are developing, can be tested.

We can write unit tests, we can write integration tests, and we can automate our testing for doing

those kind of things.

We need to make sure that our code is well structured and that's what we have done here.

Now the next step is to start writing the automation test case, right?

So let's do that in the new notebook or maybe go to my old notebook word count ingestion, Right?

This is the notebook I created where I'm doing cleanup and setup.

Before I run my word count application, I want to make sure that cleanup and setup is done.

Uh, we get a clean environment for running our application.

So let's modify this itself to write our test cases and give a proper structure for the cleanup and

setup also.

So let me change the name of this notebook.

Word count, maybe test suit.

Right.

And then we start creating our test suit.

Let me zoom it a little bit.

Okay.

So let's create a new cell and define a class for.

Our test suit.

Right.

So, class and.

That's all.

The class definition is done.

Then let me create a constructor for the class.

I'll keep on pasting some code to save some time, but I hope you understand that.

So what do we want to do in the constructor?

We want to initialize the base data directory variable in the constructor.

And.

That's all My constructor is done.

And then what I want to do, move all these cleanup and setup activity also inside a function.

So let's define a function.

Click.

So what do I want to do?

Move all the cleanup code inside this function.

So everything from here.

Goes inside the.

Function.

That's all.

And now we don't need this cell.

It is sometimes a good practice to put some debugging messages.

So let me add some debugging messages here.

With a print message before we start cleanup And.

Put another print message after the clean up.

So clean tests will clean up the environment.

What is next?

We want to ingest data, right?

We haven't written any code yet to data ingestion.

So let's write some code for data ingestion.

I'll create one function.

So this is the data ingestion function.

We call this function ingest data.

And what it will do, it will copy some test data into the landing zone.

Right.

And R is the parameter which tells which iteration we want to implement.

So when we pass one, that means the first iteration of the data ingestion will be performed.

When we pass to the second iteration of the data, ingestion will be performed.

And when we pass three, then third iteration of the data ingestion is performed.

Right.

And what is the data ingestion for testing purpose?

We will copy test data from our test data directory to the landing zone.

That's all.

So let me paste some code for that.

That's it.

So starting ingestion and at the end, we are done.

And but what I'm doing, I'm using databricks utilities file system, utility copy command.

Right.

And what I'm doing in the copy command taking.

Our sample data.

The sample data file name is text data one dot txt for the first iteration.

Text data two dot txt for the second iteration.

Copy it from this source directory, which is my test data directory and paste it to the base data directory

data slash text, which is our landing zone.

So we are done with the data ingestion.

What else we want to do?

We want to write a function for doing the testing.

Let me write it or paste it and then it will make more sense.

Right.

So what I've done here, I've created a function called assert result, which takes a parameter which

is expected count.

Right.

And what we are doing.

We are querying the word count table, which is our target table, and calculating the sum of count

where word starts with s right and we collect that result into actual count.

And finally we use the assert to validate the expected count is equal to the actual count.

If both are not equal, then test failed and we print that message.

So what do we have by now?

We created a batch word count, test suit, class.

In the class we have clean tests.

Function or method, which will clean up the environment.

We have ingest data method which can ingest data for multiple iterations.

We have also created an assert result function or method which will validate the expected count with

the final actual count.

We can use this to validate the results after every iteration.

Now let's create the actual test execution method.

So.

Let me create a run tests method.

So run tests method is the actual method which we can call to execute all the tests.

What do we want to do in the run test?

First thing we want to do is to call the clean test before we start executing our tests.

We want to clean the environment.

So we have done that.

The second thing that I want to do is to create our word count.

Object, right?

We created our word count class here.

Right.

Let me open it in a new tab.

So we created batch word count class here.

What I want to do instantiate that class into a variable called WC so that we can call our word count

class methods.

But for this line to work, we must import this notebook into the current notebook.

Right.

So let's go at the top and add one cell here and import the streaming word count notebook.

And that's how we import the notebook inside a notebook.

We do percent run magic command.

And in the current directory this is the notebook name.

So what spark will do or what Databricks notebook environment will do?

It will import the given notebook into the current notebook and execute all the code inside that notebook,

which will bring the class definition into this notebook.

And then I can use the class definition to instantiate a word count variable.

So we have the word count object.

Now what is next?

Let's start executing our test case.

So let me give a some print message testing first iteration of the batch word count.

So what do I want to do?

I want to ingest some data file and then run the batch word count.

And finally I'll validate the results.

So let me do that.

Ingest the data using ingest data.

We have already defined that method here, which takes an iteration number.

So I'm giving first iteration, right?

So what it will do, it will ingest one data file into the landing zone.

Data is ingested.

What is next?

Execute the word count, right?

We have defined the word count here.

This method.

So execute the word count.

So what we will we are doing executing everything and producing result into the word count table.

So.

Once that is done, next step is to validate the result.

That's all a result.

And assert result we have defined here which takes expected count.

So what I'm doing here, I'm passing 25 because I know after the first test data file ingestion, the

number of words is starting with s should be 25.

So the assert result will validate.

And finally we are done with the first iteration.

That's all.

That's all about the first iteration of the word count.

Now, same thing.

We want to do two, three iterations for for any incremental or for any workflow.

Idea is to test at least two iterations.

So we know that first iteration is working and second iteration is working.

And then we can assume that third, fourth, fifth, all the iterations will keep on working as expected.

So at least two.

But best is to test three iterations.

So, you know, everything is working up to three iterations.

So it is expected to work beyond that also.

So let's do the third iteration.

Second iteration code will be same.

I'll simply copy paste this same code here and.

Testing.

Second iteration of backward count ingest.

Second iteration, calculate the word count and assert the result.

And I know my data.

I know my test data.

So I know after the second iteration, the expected count is 32.

And that's all.

This second iteration of Batch wordCount completed.

Let's do one more iteration.

This thing.

Third iteration.

Ingest third file.

And after the execution, I expect 37 records or 37 words, starting with S.

That's all.

We are done creating our test suit, right?

All looks fine.

Do you want to run the test case?

Let me do it.

Connect to a running cluster.

And then we can write code here to execute the test.

How to execute that?

Let's create a variable batch.

Word count test suit equal to.

Instantiate our.

Test suit variable.

Right.

And then.

Take this guy.

Run tests.

That's all done.

And you can hit the run all button.

Let's see if everything works fine.

Oops.

Named self is not defined.

Let's see where I made a mistake.

Class definition contains self.

Okay.

Okay.

So this is not required.

Let's run it again.

Okay, so running now and we can see the.

Oops.

One more error.

Base data is not defined.

So in the run tests in clean up.

Clean tests.

Okay.

I think I know.

So base data should be prefixed with self.

So they're all.

I'm using it here.

Here.

Here.

Here it is already prefixed.

Anywhere else.

No, that's fine.

Let's run it again.

Okay.

Starting cleanup.

Done testing.

First iteration of the batch.

Word count is starting ingestion.

Done executing word count.

So we are running word count for the first iteration.

Still running.

Okay, this is done.

Now comes the validation.

Passed.

So first iteration is complete.

Now we are on the second iteration.

Done.

Third iteration.

Great.

So everything worked.

And that's all we created a word count application.

Simple application.

But we followed all the best practices for creating this word count application.

What is the next step?

We created the application.

We coded it.

We applied all the best practices.

We executed the test cases.

Everything is working fine.

What is the final step?

The final step is to create a workflow and schedule it to run on a 15 minute frequency, because that's

the objective.

So how will you do that?

That part is also simple.

We will create one job here, the actual data ingestion job, which will ingest data from the source

system and dump it into the landing zone.

That will be our first job.

And then everything else is a single piece of code or single flow of code so we can create one single

job from starting from the landing zone till inserting into the word count table.

So we will have two jobs and then we can create a pipeline job one.

After that execute job to build this pipeline and schedule it to run every 15 minutes.

And we are done with the batch processing application for word counts.

Now comes the second part and the most important part of the discussion.

What challenges or what problems do we see in this batch processing application?

What if our customer wants to refresh this report?

In less than 15 minutes, maybe every one minute or maybe ASAP, as soon as a new data is ingested from

this source system to your landing zone, we want to refresh the result as soon as possible, maybe

in milliseconds or maybe in seconds.

How do you translate this word count solution into a stream processing solution?

And if we don't, then what are the challenges we are going to face while reducing the frequency from

15 minutes to maybe one minute or one second or less than a second?

That's the next topic and I'll talk about it in the next video.

And we will transform this into a stream processing application.

That's all for this lecture.

See you in the next lecture and we'll start talking about stream processing.

The interesting stuff.

See you again.
