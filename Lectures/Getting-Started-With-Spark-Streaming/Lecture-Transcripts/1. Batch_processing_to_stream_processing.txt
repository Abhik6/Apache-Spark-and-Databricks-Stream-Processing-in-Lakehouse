Welcome back.

I'm assuming that you already know Spark programming.

You know the Spark data frame APIs you already know Spark SQL.

And in this course we want to learn Spark structured streaming APIs.

So the objective here is to understand Spark streaming, structured API, how they work and how do we

translate our knowledge about Spark DataFrame APIs and Spark SQL in the structured streaming world?

And in this lecture, I want to help you understand two things.

When do we need stream processing and what are the challenges associated with the stream processing?

When we transition from batch processing to stream processing, there are new challenges that we need

to take care of.

So this lecture will try to help you understand those challenges.

And then going forward in the rest of the course, we will learn how Spark is structured.

Streaming helps us to address those challenges.

So let's start.

Let's start with an example.

I want to put up a scenario in front of you and start building a solution for the same.

And we will be using batch processing approach to design a solution.

And then we will try to understand if this thing has to be done using stream processing, what are the

new challenges that we will have to address?

So let's start with the example.

Assume you are working for a large stock brokerage company.

Your company provides brokerage facility to end users and you have different types of systems to help

customers do the trading activity.

They can use desktop based, dedicated trading terminals.

They can use web based applications.

They can do it from their laptops.

And you also have a mobile app.

All these systems are used by the end users to do the trading.

Your company wants to build a report using the trading data, and that's your main requirement, right?

Your company wants to build this kind of trade summary chart.

Let me explain this chart so you understand what we want to achieve as the.

So here is your requirement.

On the x axis.

We have time.

So it starts with nine, nine, 15, 930, nine, 45, ten, and so on.

On the y axis you have amount.

So 5000.

10,000.

15,000.

20,000.

25,000.

So you are collecting trade data and users can perform two types of trade.

They can sell some stocks and they can buy some new stocks.

Right?

So the blue line represents total buy at a given point of time.

So at this stage, the blue line represents at 11.

What is the total purchase made by the customers?

Orange line represents the total sold amount.

Right.

So at that same point of time, let's say at 1115, this point represents what is the total sales.

Right.

And the difference between the purchase and sale is your settlement amount and which is represented

by the gray line.

So if you look at the same point, the gray line at this point represents what is the amount due for

settlement.

So that's the report we want to build.

And you have data coming from your sources.

Your sources are desktop based or mobile or laptop or web based trading systems.

Right.

So how do you go about building this solution?

Let's start thinking about it as a data engineer, we always try to frame the data engineering project

into these three compartments, right?

In these three blocks, the first one is how do we ingest data?

So your data is being generated here.

It is being captured here.

How do we ingest that data and bring into our data engineering platform?

Once ingestion is done, then we do all the processing and prepare the data to generate the required

results.

Right.

And once the result is prepared, we store it in some tables or in some format for our consumer applications

to consume it.

So our consumer application is our customer As a data engineering architect or as a data engineering

team, our consumers are our customers.

So they gave us the requirement and then we start working backward from that requirement and start thinking

how do we collect and what data we collect bring into our platform and how do we fulfill their requirement.

So once I have the result here, my consumer application can connect to my system and pull the data

to prepare these kind of charts.

So that's how we start thinking about our data engineering application.

So now let's start building the solution step by step.

So the first problem that we have, how do we ingest data from these terminals or these apps?

Right.

Let's assume.

That we have some magical system here.

Which is already in place, which collects data from all these systems.

Right.

All these data producers.

So when they do a transaction, either buy or sell, the data is automatically captured and collected

at some magical system.

We don't want to go into those details yet.

We will cover those things later.

So as of now, let's assume there is some magical system here which already collects data from all these

different different terminals, different different systems, mobile apps, web based applications and

all that.

And data is present at one central place.

Now we want to bring that into our platform, right?

So the next step is to ingest data from this system and bring it into our platform.

Let's say we decided to create a landing zone directory in our platform and we build one data ingestion

pipeline.

A small data ingestion application which connects to this system, pulls data from this magical system

and saves it into our data engineering platform in a landing zone as data files.

So let's say every 15 minutes we are connecting to this system, pulling data and whatever new data

is collected in that 15 minutes we collect that, we pull that from this system and save it as a data

file in the landing zone.

That's our first step.

Once this step is done, data entered in our data engineering platform.

Now we have the data and everything else we can do within our data engineering platform.

We don't have to interact with any external system.

So what is the next step?

We want to build the solution using best practices and follow medallion architecture pattern.

So the next step would be to write one more application or job to read data from the landing zone and

save that data into our bronze layer table.

That's our raw data table.

So this represents the raw.

So we are ingesting data and putting it into raw data tables, right?

And then once data is inside the raw data tables, that becomes our source of truth.

Everything from now onwards will be based on this data.

Whatever we want to do will be based on this data because that becomes our source of truth.

So what is the next step?

The next step is to create rest of the process, right?

Prepare data preparation, data quality, data processing and all that.

So we are in the next step.

We again read data from our raw data tables and write one job to create a high quality data table,

right?

And we call it a silver layer table.

So we have one more job here which will read this trade data or transactions from the raw table and

filter out the poor quality data or fix some data quality issues, standardize the data format and create

a high quality data table.

We call it Q table.

Once we have the high quality data, we start the actual processing.

The processing which is required for these guys to build this report.

Right.

So next step is to read data from the high quality data table and do the processing.

Maybe during the processing we may need to join this transaction data with some secondary tables or

enrich the data so that their requirements are fulfilled.

So that's what we are doing here.

And once all this is done, we want to save the results in the final results table.

And that will be all for us as a data engineering team to build this solution.

So we prepare results as per these customer's expectation and create those results in the final layer.

Once the result is there, they can connect to this table, pull the data and draw these charts or visualization

or reports or whatever they want.

That's how we want to build the data engineering solutions in a typical way.

So we build this, right?

We design this, but how do we run it for running this solution?

We will have to use some kind of orchestration tool, right?

We will have to build a pipeline which starts maybe from here till this stage, right?

So how do we build that pipeline?

We will need.

Uh, data orchestration tool or a workflow tool or an additional tool which can help us to build the

pipeline.

So let's assume this entire thing.

I mark it as jobs.

So this is my job zero, which is my first job.

First thing we should do is to ingest data from this magical system, bring it to our landing zone.

Right?

So.

So this is job zero.

Then next stage is job one, job to job three, job four.

And once all these jobs are executed in a sequence, we will have our results prepared.

Right?

So we need to build a pipeline of something like this, right?

Jobs, which starts from job zero and ends at the job four.

And we will be using some workflow design tool or data orchestration tool for that.

So all that is done, we build this solution.

We know it is going to work, but then comes the next and the most important question what is the pipeline

frequency?

Right?

This is the pipeline that we want to run.

We have built the solution.

We want to run the pipeline to prepare the results here.

But what is the pipeline frequency?

We want to run it daily.

We want to run it hourly when this pipeline will start and run from this stage to this stage and when

next time it will start and run from this stage to this stage.

That's the most critical question, right?

We talk to our customer and our customer is saying we need this report to refresh every 15 minutes.

So what they want is something like this.

Start with a blank report at maybe 9 or 915 and every 15 minutes we want to refresh this report.

We want to.

Keep on getting the latest state every 15 minutes.

So this report should be refreshing continuously.

Every 15 minutes and our customer wants to see the result every 15 minutes.

So it should be happening like this, right?

So frequency is the first problem that helps us to decide whether we need a batch processing or a stream

processing.

So let's talk a little bit about Gray.

So the first problem that we need to address is the frequency problem.

So let's try to understand what is your job frequency.

Your customer may say we need to generate this report once in a month, so your job frequency is monthly.

It could be weekly, daily or it could be an hourly requirement.

So in these kind of frequencies, we have enough time between two iterations of the workflow, right?

What do we want?

We want our workflow to start from the job zero and in a sequence, execute until job.

For once we are reached.

The job for everything worked perfectly fine.

Then stop.

And maybe if your frequency is early, next iteration will be after an hour.

So we collect more data.

We collect new data for one hour and after one hour we want to again trigger the job again.

Start the job.

This workflow is starting from Job zero and Job four so that our reports are refreshed with the new

results.

Right.

So if your frequencies are.

Maybe in hours or days, weeks, months.

We have enough time between two iterations of the job or between two iterations of the workflow.

And that simplifies a lot of problems.

But when your frequency starts coming down.

Two minutes or maybe seconds or maybe as soon as possible.

We may have to start facing some new kind of problems.

So let's try to understand that.

So let's assume you have a 15 minute frequency requirement, as in our current example.

So what does it mean within 15 minutes, you have to make sure that this entire pipeline should be finished

so that after 15 minutes you can start the next iteration, Right?

So if every 15 minutes we need to run this entire pipeline from start to end, so each iteration must

finish within the 15 minutes and that sometimes is a big challenge.

Think about it.

Think you are building a solution in cloud platform, right?

And you opted for on demand clusters.

You are launching a cluster on demand, running your job on that cluster.

And once your job is done, you are shutting down your cluster, right?

In that scenario, there is some cluster startup time.

If you are using a spark on demand clusters in cloud, maybe five seven minutes goes in starting a cluster,

five seven minutes are gone and then only your job will start.

So you are left with eight more minutes.

So you need to make sure that eight minutes, within eight minutes, all the processing should be done

right.

So try to understand the problem.

You have only 15 minutes to do everything and that everything includes your startup time job, startup

time.

It includes processing time.

Whatever logic you have written, all that should be executed within the same 15 minutes.

And you also have to handle the volume of the data.

If you have maybe 100,000 records, it is easy to handle those 100,000 records within the 15 minutes

time.

But what if you are getting 1 million records or maybe 10 million records to process?

Right?

So as your volume increases, your processing time is going to take longer and then meeting that 15

minute goal is quite difficult.

You may also have some cleanup or housekeeping activities as part of your job, right?

For example, you processed some records, some some files you already processed in this iteration,

maybe for next iteration before you start the next iteration, you want to move those files which you

already processed to some an archive location.

That's a housekeeping activity, so you may have some housekeeping activity.

Also, all that should be finished within 15 minutes duration or within whatever your window is, whatever

your frequency is.

What if we cannot achieve that within 15 minutes window?

Let's say you have a 15 minute frequency requirement and your iteration takes 17 minutes.

So next iteration will be delayed by two minutes and the next one also takes 17 minutes or 18 minutes.

Then next one will be delayed by 5 minutes or 4 minutes.

Right.

And this delay, keep on aggregating.

Right.

Every next iteration will keep on delaying further and further.

And that delay by the end of the day will be maybe few hours.

And that is something what we call back pressure.

Back pressure is an idea where you have lot of work to do, but you cannot finish it within the expected

timeline.

Right.

And your your workload is keep on piling.

So you have lot of data to be processed, but your processing speed is getting slower, slower and slower,

and you start getting back pressure.

So that's the back pressure problem.

So as your frequencies starts coming down, right as you need to run your workflow more frequently,

your back pressure problem will start growing or you have to handle your back pressure problem.

You have to make sure you are not getting stuck into a back pressure problem.

So that's the first problem.

Second problem, think about it.

If your frequency is in seconds, you want to run it every second.

And what if your requirement is in milliseconds or customer says as soon as possible, give me results

ASAP?

Right.

How do you schedule for continuous listening?

Right.

How do you schedule your workflow which picks up data as soon as it comes?

Maybe in ten milliseconds, 20 milliseconds, 50 milliseconds, even seconds, even one second is quite

difficult, right to achieve.

So with the frequency.

We have two types of problems.

If your frequency keeps shrinking, right, you have two types of problems to address.

The first problem is back pressure problem.

You need to make sure that you are not getting stuck into the back pressure problem, and then you need

to understand how you are going to do the scheduling for continuous listening or milliseconds response

or for even a second response.

We will learn that how to handle that in the course.

But let's move on to the next problem.

The next problem is to handle the incremental data processing.

So we know that back pressure is a big problem for us.

How to handle that back pressure or how to handle the performance of your job, the entire workflow,

so that it finishes as quickly as possible.

The first solution that comes in your mind is do the incremental data processing.

Do not process the entire data every time.

Right?

Only process what is new.

Only process that.

So that will dramatically reduce the amount of data that you want to process.

And less data means you can process it faster.

But incremental data processing comes with a new challenge, right?

Think about it.

You have this workflow which starts from Job zero and goes up to Job four.

So we start from here.

Job zero will pull data from this system, right?

So as an incremental data processing, what do we want?

We want this job zero to pull some data from this system.

Whatever is available for the first time, whatever is available, pull that data from the system and

save it in the landing zone as a data file.

But next time, pull only those things which are new.

Do not pull what you already extracted in the previous iteration.

Right?

Extract only new records and save them here in the landing zone.

Right.

Similarly, all these jobs need to process only the new records.

Right?

So job one, this landing zone will have all the files.

You pulled some records at nine, then you pulled some more records at 915, and you have two files

now, file created at nine, file created at 915, and then so on.

You will have files created at 916, nine, 17, nine, 18.

Lot of files will be there.

But job one should not process all the files every time.

Right?

For the first time.

Process whatever is there in the landing zone and bring it into your raw data table.

Right.

But second time you pull only those things which are new since the previous time, right?

Similarly, all these jobs, every job should make sure that they are processing only new data.

How do you do that?

For each job, you have to remember you have to memorize or you have to implement some solution where

you know what I processed in the previous iteration and what is new this time for this iteration?

That kind of solution is known as checkpointing.

So when you start a job first time, it creates a checkpoint that what I'm going to process in this

iteration right.

And saves it somewhere.

And then next time, when it comes to process the next iteration, it will go and read what I processed

last time.

What we have now, what is the delta, what is the difference?

And then take only that difference and process that difference.

Save that information again in the checkpoint.

And then in the next iteration, you again go and read what is there in the checkpoint which already

processed and what is new and compare and take only new things.

Right.

And then keep on doing it in every iteration.

So checkpointing is another problem.

It's a solution.

We can do incremental data processing using checkpointing, but then we'll have to implement that.

So incremental data processing or implementing incremental data processing correctly is the next problem

in the stream processing.

Because in the stream processing we want to deliver results as quickly as possible.

And in that scenario we don't have time to process everything again and again, right?

So that's the second problem.

Now let's look at the third problem.

The third problem is how do you handle failures, right, for handling failure?

Checkpoints are instrumental checkpoints are the key for handling all kind of failures.

Right?

So for example, you are running this job, this workflow, which starts from job zero one, two, three,

four and all that.

And iteration by iteration.

Iteration by iteration.

Ten iterations worked perfectly fine in the 11th iteration.

Something went wrong and your job failed.

Right.

So 11th iteration was not successfully processed and your job failed.

It stopped.

You are notified.

You started investigating.

What is the problem?

You fixed the problem and you want to restart the job.

But when you restart the job, it must restart from the 11th iteration itself.

Right?

It should not start from the 10th iteration or should not start from the beginning.

So and that information is stored in the checkpoint, right?

So checkpoints are very critical to protect you from the failure or to allow you to stop your streaming

job and restart.

But implementing checkpoint correctly is another complex problem.

So think about it how checkpoint works.

Check point is used at the beginning of the workflow, right?

This entire workflow at the beginning, you want to read the checkpoint, understand what you processed

last time, and then start your job so that you read only new data and you are not going to read again

what you already processed in the previous iteration.

It starts from there and then during the job processing, you need to keep on recording what you processed.

And at the end, when everything is successful that time, you need to commit your checkpoint.

So this is done right.

But now think about it.

You have two transactions here.

This.

Your workflow is one transaction which is separate.

Your checkpoint operations are another kind of transactions which are separate.

If these two are separate, then you have a problem.

It is always possible that this is successful, but the Checkpointing operation is failed.

You processed everything successfully, but you failed to commit the checkpoint or your job is working

perfectly fine.

But writing to checkpoint somehow failed.

Right?

And you couldn't record it properly that what you are processing.

So implementing checkpoint correctly is a real, real complex activity.

To make sure that both either both are successful or none of these are successful.

They behave like a single transaction.

You make sure that everything is properly committed at the end, otherwise everything is marked as fail.

That is a complex implementation itself.

So the next challenge in implementing stream processing or low frequency jobs is to correctly implement

the Checkpointing solution.

That's the next problem.

Now, let's talk about one more new problem.

The next problem is how do you handle the late coming records?

What does it mean?

Let's try to understand.

We start job at nine and the entire workflow should finish before 915.

So we have a 15 minutes window to process all the records.

Right.

So whatever is whatever data has arrived in our system at 9:00, we take all that data and do the processing,

compute the results.

But is it guaranteed that all the data that should arrive by nine has already arrived into your system?

It is possible that some records are stuck somewhere in the network or they fail to arrive to your system.

They were dropped in between and whoever is sending that data, he is retrying it again.

And in all that, your data is coming late.

Your window, your processing window is 9 to 915 and whatever data you expect to be there at nine is

coming late.

It's called late data arrival.

So how do you handle that late data arrival?

But before that, before we try to handle that, let's try to understand what is the problem with the

late data.

So I have set up an example here to understand that for our example, let's say our iteration, one

should be executed at ten, right?

So our execution one or iteration one starts at ten.

And if you recall the report, we need to calculate what is the total sale at ten, what is the total

purchase at ten and what is the difference?

Right.

So let's say we have these one, two, three, four, five transactions.

This first transaction was performed at 932, 933, 935, 945 and 952.

So all these transactions were performed before ten between 930 and 10.

So all this should be used to calculate total sales at ten and total purchase at ten, right?

So we got all these records and then you calculated the sum of total sale is 3400.

Total purchase is 3000.

You calculated that aggregate and you posted one result record in your results table, which is at ten.

Total sales is 3400 and total purchase is 3000.

And this is correct, right?

So you've processed this entire thing at ten.

You started processing at ten, and before 1030 you finished it.

So your next iteration starts at 1030.

And for calculating total sales and total purchase at 1030, you need what are new transactions happened

between 10 to 1030.

All that transaction you need and you looked at the record and you got all these records.

The records listed here.

So one transaction that happened at 1030.

Another at ten, five, 10 to 10, 15, ten, 18, ten, 26.

All these records have arrived to you, but you also got one more record.

That transaction happened at 959 and you couldn't get that transaction in the previous lot.

That transaction should have come here right at 959.

And you should have calculated that total, including this record also.

But due to some reason, this record was late.

It came late to your system and now you have this record arrived to your system by 1030.

When you start your second iteration at 1030, you saw this record.

What do you want to do with this record?

That's the problem, right?

So obviously at 1030 you are calculating you will skip this record because this processing is supposed

to take all the records that were generated between 10 and 1030.

So you will take all these records and do the calculation and your total sum comes to 2700.

That's correct.

Right.

And your total purchase comes to 2900.

And you posted that data here.

So your total sales at 1030 was 2700.

And this is sales between 10 and 1030.

This one is sales between 9 and 930.

So you you got this result correct, But.

What do you do with this record?

You haven't taken care of this record yet.

Ideally, you should take care of this record in this lot.

Right.

So the answer for this lot.

The result that we calculated earlier for this lot becomes invalid at 1030.

At this point of time, it was correct.

As per the known information, Right.

We didn't knew that there is one more record which was hanging somewhere will arrive late.

So at that point of time, at ten this result was correct.

But when we know that record has arrived.

This record becomes invalid.

We cannot leave an incorrect record or incorrect result in the system.

Right.

So we'll have to correct it.

So how do you correct it?

What is the approach?

So the idea is that you put that record back in this lot and reprocess the calculation for this lot,

right?

If you reprocess the calculation for this lot, your numbers will change.

And then once you recalculate the previous lot, including the later records, you must come and update

your result, set, update your result set to fix this problem.

So all that becomes, again, very, very complicated.

Right?

And that's one of the biggest problem with the.

Extreme stream processing requirements or when your job frequency starts shrinking, right?

You want to process data every second or every minute or even five, ten minutes.

These kind of problems will start coming up.

So let's summarize what we learned.

In all this discussion, we learned there are five.

A specific problems that we need to address when our job frequency is short and it starts maybe for

15, ten minutes, right?

And as your frequency starts becoming shorter and shorter, handling these problems starts becoming

more and more difficult.

Implementing right solution for handling these problems becomes more and more difficult.

So what are the five problems that we introduced?

The first problem is your back pressure problem.

You need to make sure that your system, whatever you designed, is not suffering from the back pressure.

The second is how do you schedule your job for continuous listening or one second listening or millisecond

and microsecond kind of latencies?

How do you configure your workflow?

That scheduling also becomes very complex.

Third problem how do you.

Implement the checkpoint because you want to do an incremental data processing.

So you have to implement a checkpoint.

You have to record somewhere what I processed in the previous iteration and what is due for this new

iteration.

Right?

So implementing checkpoint is another requirement.

How do you handle fault tolerance?

Because for making sure that your system is fault, tolerant and restorable your jobs can be restarted

after stopping them.

You need to make sure that your checkpoint and your job are acting like a single transaction or either

they both fail or they both succeed.

Right?

So that is another challenge.

And then last problem is how do you handle late arriving records?

Because when you have late arriving records in your system, then you need to remember what result I

calculated earlier.

This new record, which is late, will belong to which of the previous iterations.

And then you need to go and update that iterations result to correct the result.

You need to make sure that if you calculated something incorrect due to some records was late, you

need to go back and update that result, correct it and all that requires maintaining the state of your

previous calculation.

Previously calculated results.

That's what the state management is another problem that we will have to handle.

So the point here is not to scare you with the stream processing.

The point here is if you are building batch processing systems and you want to do it faster, you want

to reduce your job frequency in minutes or in seconds or in milliseconds, you have to build solution

for doing all these things, for solving all these problems.

What if we have a system that gives you a ready made solution for all these?

Because these are common problems for every application.

Every application that wants to produce results in minutes or in seconds, all these problems are common.

So why keep on implementing all these for every project or for every requirement?

Why don't we choose a system which comes with inbuilt capabilities to handle all these problems in a

easy way?

Reusable solution you may write.

And that's what we are going to learn in this course.

A spark.

Structured streaming comes with the inbuilt solution for all these things.

But how to use those?

How to implement those in your stream processing applications.

That's what we will learn throughout the course with lots of.

And that's all for this lecture.

We'll connect again in the next lecture.

See you again.
