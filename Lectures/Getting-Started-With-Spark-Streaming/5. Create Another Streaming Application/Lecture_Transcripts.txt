Welcome back.

In this lecture we will try to create another spark streaming application.

So let's start with the requirement.

I'll give you a quick introduction to the requirement.

And then we will start coding it.

Let's assume you are working for a retail organization.

They are a retail organization selling kitchen utensils and home decor accessories.

ET.

It's a big organization.

They have multiple channels to sell their products.

Customers can buy from their mobile apps or through from online portal, or they can come to a store

and buy it from the store also.

So let's assume you are generating invoices from these different channels.

And those invoices are being collected at a central server or central system.

It's already there.

We are not going into the details of what this system is.

We start our requirement from here onwards.

So the requirement is that first thing we need to do is to ingest data into the landing zone and save

those invoices as a file in the landing zone.

And as soon as files are produced in the landing zone directory we start our processing.

So first step of our processing is to ingest data from the landing zone, create raw data set and then

apply processing.

And once your processing is done produce it into a table.

Right.

So very small requirement.

Read the data from the landing zone, create raw data set and then apply your processing logic on the

raw data set.

Produce results and save it into a target table.

That's all.

Let me quickly walk you through the input and the expected output.

So input file contains a bunch of Json records and each Json record represents one invoice.

You can see the input sample here.

So basically it's a Json record.

We have invoice number then created time which is timestamp in number format is store ID a point of

sale ID cashier id.

All these details are there payment method, delivery, address type, home delivery.

And then we have one complex data type here delivery address which contains address line, city state

pin code and contact number.

And at the end we have line items of that invoice.

So invoice may contain one line item or it may contain multiple.

This example contains four line items.

I'm showing only one here.

And each line item comes with item code, item description, price, quantity and total value.

So this is what is your input right.

We need to.

Read this input.

Do some kind of processing and the final result should be flattened version of this input.

So when I say flattened it means each line item.

Each invoice line item should be represented as a single record.

So if we take the example of this invoice number, this same invoice number, this invoice contains

three line items.

I'm showing only one here, but there are two more hidden here.

Right.

So this invoice contains three line items.

If we flatten this, we should generate three records in the target table.

And each record comes with some common information and then line item details.

So we need invoice number created date time and store id ID and customer type payment method, delivery

type.

All these details we need as a common item from the invoice itself.

You also need the delivery address, details and finally all the items are taken individually, right?

So item code.

This is the first item code which is sum product here price quantity and total value.

So the goal is to take one invoice and produce a flattened version of that invoice, where each line

item creates one record with all the necessary details.

So it's kind of denormalized version of the invoice.

That's the goal.

That's what we need to do, but we need to do it as a stream processing application.

So if we come back here on our requirement, we need to run this entire process ingesting from the landing

zone till producing the result as a continuous stream.

It should be continuously running right.

So that's that's the objective.

Take it as an assignment and start coding it.

You already learned how to code stream processing applications using Spark Streaming, so I'll recommend

you give it a shot.

Try doing it yourself.

If not, I'm going to do it so you can watch me coding.

Okay, so I'm in my work space.

Let me go to work space.

My home directory.

And here is my project directory.

Let me create a new notebook for this requirement.

Give a name to the notebook.

Let's call it Invoice Stream.

So we learned some best practices in the earlier example.

So this example we will code using all those best practices.

We will not write raw code and then refactor it to create best practices right.

We will start from the beginning following the best practice.

So it's a good idea to create a class and keep everything inside the class, right?

So let me define a class.

So my class name is Invoice Stream.

Let's define a constructor for the class.

That's my constructor.

What do I want to do in the constructor?

Set up some initial variables so I have only one variable which is my test data directory.

So let me define that.

I'll paste the directory location here just to save some time.

And that's all.

My constructor is done.

Now what do we want to do?

The first thing that we want to do is to read the data from the landing zone and create a raw data frame.

But my data is coming as a Json, right?

And it comes with the complex data type and complex data structure.

So it is recommended that we always define a schema for our data and then use that schema to read the

data set.

Depending on the automatic schema, inference is not a recommended approach.

So let me define a function which returns the schema for the data set.

So.

Let's call it get schema.

And this function will return the string schema string.

So it's going to be simple.

I'll just use multi-line.

A string in Python and start defining my schema.

So if you can recall the.

Sample Json dataset that I showed you will take help from that sample dataset and start creating a schema.

So let me quickly show you that.

So here is my sample data set.

Right.

What do I need to do?

Take invoice number, create a time store id, id all these fields I'll define in my schema.

So let me start typing it.

So invoice number.

I'm expecting it to be an string.

Then.

Create a time.

I'm expecting it to be a big integer.

Then.

Store ID is a string like that.

I'll go on, uh, typing the schema details.

So just to save time, let me paste a few things here.

The cashier ID and then in the next line.

Okay, so all these fields are part of my dataset.

Invoice number, created time, store ID, ID, cashier ID, customer type, customer card number,

total amount, number of items, payment method, taxable amount, cgst excess and delivery type.

After that I have delivery address which is a complex data type.

So how do we define a complex data type?

I hope you learned that in your spark course.

So delivery address and delivery address is a struct right.

So define a struct inside these angle brackets and rest is same.

All the fields are kept inside this.

Right.

So.

Let me put a line break here.

So that's my delivery address.

It's a struct.

What is next?

The last item is my invoice line items.

So invoice line items is also a complex data type.

So let me define invoice line items.

And it's an array array of multiple line items.

So we define it as an array in the angle bracket.

And each element each line item in this array is a struct.

So we take a struct.

And all the line item columns will become part of this struct, so let me paste it here.

That's all.

So.

Look, my invoice line item is an array which is made up of a struct, and that struct is made up of

these fields item code, item description, item price, item quantity, and total value.

And that completes my schema.

So let me.

Close it.

Think a little here.

That's all.

So get a schema method is defined.

If I call this get schema method what I will get I will get the schema string.

That's all.

Now we are ready to start writing the code for reading the data file from the landing zone.

So let's define a new function.

I'll call it read invoices, and reading a data file from a landing zone is super simple.

You already learned it.

So what do we want to do?

We want to return the data frame and write the spark dot read stream code inside this pair of parentheses

so we can read the data frame from the landing zone.

So let me type in.

Spark dot read stream and then format I know my data is coming as Json.

So format is Json.

Then we want to supply the schema explicitly.

So I'll use a schema method.

How to get the schema.

I can call my This.get schema function right.

Okay.

So defined the schema.

And what is next.

The last step is to load.

Load from the landing zone directory.

So what is my landing zone directory?

Let me paste this string here.

Okay, so my landing zone directory is inside my base directory.

Slash data slash invoices.

This is my landing zone but I'm using a variable here.

So let me use F here for string substitution.

That's all.

I'm done with reading voices.

So what is next?

The next thing I know, I want to explode my invoice.

Right.

So let me quickly show you the.

Input sample.

So I have this kind of invoice.

Right.

And this invoice comes with multiple line items inside the single invoice.

What do I want?

I want to flatten this invoice, take out all these line items as a single record.

So how to do it?

We have to explode the invoice line items, and so that all these line items from the invoice becomes

a single individual row.

Right.

That's what I want to do.

So let's create a function for exploding it.

The function name is Explode Invoices, and it takes a invoice data frame so that it can explode it.

That's all.

Then we will return the data frame after exploding it.

And my code goes inside this.

Right?

So what do we want to do?

How do we explode it before we explode it?

We need to take out the common fields from this.

Right from the invoice VFS.

So let me take the invoice DF.

And do.

Select expr and first thing I will take out some common fields like.

Invoice number.

And then.

Create a time.

So what?

Our common fields are there.

I'll take out those.

So let me paste some of the common fields that I need for saving some time.

Right.

So delivery address is a complex data type.

Correct.

So what I want I want to take out the fields from the delivery address.

Also I want to take out delivery address dot city.

And similarly I want to take out delivery address dot state pincode etcetera.

So.

Let me take out the delivery address state and pin code also.

Now I've got my common fields.

So what is next?

Next is to explode the invoice line items.

So let me write code for that.

I can use explode function, and inside the function I'll explode invoice line items and let's give

it a alias.

Each line item becomes a line item, and I guess that's all we wanted to do in this function.

So.

Do some formatting.

That's all.

I'm done with the explode invoice.

I'm almost done.

I've exploded my data frame, but we still have a few more things to do.

For example, when I exploded this data frame, I created a line item object.

Right in each row we will have invoice number, created, time, store ID, all these fields will be

there, but we will have one more field which is line item and line item itself is a struct, right?

We exploded the array and created one record for each line item.

But line item itself is a struct.

So we need to take out individual fields from the struct, rename them, and create a flattened proper

table structure.

Right?

So let's write one more function for doing that.

Let me call it.

Platen invoices.

And this guy will take exploded data frame as an input, and we will do a fine tuning or restructuring

of the exploded data frame and return it.

So.

That's all.

We will write our code inside this return statement.

So what do we want to do?

Let's start with the exploded data frame.

And fix some columns, so I'll use width column method.

What do I want to fix?

I want to create a new column called Item Code.

And this column should come from.

Line item.

Dot item.

Code.

Right.

But we know that this is not going to work, so I'll have to.

Convert this string into an expression.

And then only it is going to work.

So for making that work I have to import the expr function.

So from.

Good.

So what I've done, I've fixed one column here.

And same way we need to fix some other columns.

So let me paste the code here.

Good.

So what I'm doing creating item code, taking out line item dot item code.

Similarly, I'm creating item description, taking out line item dot item description.

And like that I'm creating all these fields.

So once I'm done creating these new fields we don't need this line item field.

It's already there in the data frame right.

So last thing is to.

Drop the line item.

Right.

Drop the.

Line item.

So from a line item which is a complex data type, which is a struct.

I've taken out all the fields, gave them new names, and after that the complex data type line item.

I want to drop that so it doesn't exist in my data frame.

And that's all we are done with this function also.

Um, if you want, you can do a little formatting.

And hope this all works.

I don't have typos, but nothing to worry.

We will test that.

What is next?

We are done all the processing.

My my flattened invoices dataframe will return a dataframe which is my final output.

Right.

And as the last step, what I want to do save it into a table.

So let's write one function for that.

So I call this function append invoices.

And this function takes flattened data frame and save it.

Save it using streaming.

Right.

Right.

So let's write code for that.

Flattened DF dot.

A stream, and this code will write the flatten to a target table.

I'll define all the remaining details, but we also want to take out the background query thread right.

So we can manage the background query thread.

We can we want to stop it or we want to monitor it.

For all that purposes we should always take out the background thread background query thread.

So let me add a return here.

And I'll finish my right stream inside this parenthesis.

So what do we want to do?

I want to write is write it as a delta format.

Then write a stream should always have a checkpoint detail.

So let me add that.

I'll add checkpoint detail.

So what is that checkpoint location?

Paste it here so the check point is inside my base directory.

We will have a home directory for all the checkpoints and inside the checkpoint I will create a separate

checkpoint directory for invoices and I want spark Write Stream to use this directory as the checkpoint

location.

What is next?

We must define the output mode.

So.

In the earlier example we used complete output mode.

The complete output mode is something like, which will produce the entire result every time.

And as a side effect of it, you will be overwriting the entire table.

But this one we want to use append mode.

So we don't want to overwrite the target table all the time.

We want to insert new records in the target table and avoid overwriting it.

So forgot the dot, right?

So what is the last step?

At the final action.

So flattened.

Right?

Stream all this up to here.

All of this code is not an action.

It's just a preparing your data frame for writing the stream.

To table is the final action.

So unless until we write to table, we are not going to produce any result.

So that's the action.

So so we want to produce it to invoice line items table.

And that should be all right.

I prepared my append invoices function.

What is next?

Let's write the final code.

Which?

Which is something like the main method which stitches all of this.

Right.

So let's define a function for that.

Let me call it process method and process method.

What it will do.

It will read the raw data.

Then it will explode.

Then it will flatten the data frame.

Exploded data frame.

And finally we will execute the write stream at the end.

So the code is simple.

Let me paste a few things.

So first thing you want to do is to print some message starting invoice processing stream, and then

we will go step by step.

So first step is to call the read invoices.

So we trigger the read stream and we get the raw data frame which is invoices DF.

What is next.

Next step is to export the invoice.

So I'll call explode invoices.

Pass the raw data frame here and get the exploded invoice.

Then I'll call the flatten invoice and pass the exploded invoice and get the final result.

What is next?

The next step is to.

Call the final append invoices method, which will call the spark dot read stream on the result data

frame, and we'll also give back the background query thread, the streaming query thread handle, and

we can use it in whatever way we want.

So finally I will.

Print the log message as done, and will also return the streaming query from the process method.

So whoever is calling this process method can manage the streaming query.

And that is all.

That is all we wanted to do.

You you might want to run it once and see if everything compiles.

So for executing it I need a cluster.

So let me go to.

My compute menu cluster already stopped.

So let me delete this and create a new cluster.

It will take five minutes.

So as soon as my cluster starts, I'll attach this notebook with the cluster and run it once to see

if there are any compilation errors.

And that will be all.

That will be all the next.

What is the next step?

The next step is to write a test suit for this code.

Write so that we can call these methods, test it and validate if they are performing good as expected.

So I'll leave test suit creation for yourself.

We already learned how to create test suit for streaming applications.

I'll leave it on you, but the solution is already included, so I'll quickly walk you through the test

suit and we'll take it from there.

So let me go to workspace and create a new notebook.

And give a name to a notebook.

In this notebook, we want to write the invoice stream test suit a code for testing the invoice stream

so I'll not type everything.

I'll simply paste things and walk you through the code.

I've already written it, so first step is to import the invoice stream notebook here.

So all the class definitions that we just created are available to this notebook.

And then I will paste the code for the test suit class.

Here it is.

So we already learned how to create this kind of test suits.

So I'll not repeat it.

Or I will not type all this once again.

But let me quickly walk you through it.

So I'm defining a class invoice stream test suit.

And I have a constructor which defines the base directory location.

This is the directory location where I have already uploaded my test data.

Your test data is included in your course material.

You can upload it in your dbfs directory.

So what is next?

The first method in the test suite is to clean the environment.

So as usual, I'm dropping the table and I'm also making sure the table directory is also deleted.

And then I'm deleting checkpoint, deleting my landing zone, and finally recreating the landing zone.

So.

We already have the landing zone directory in place and we don't get errors like a path not found.

Then I have a method for doing the data ingestion.

So data ingestion in our case is super simple.

We take a.

Invoices.

Json file from our test data directory and write it in the landing zone.

And for doing that we use db utils dot dot command.

So db utils is a Databricks utility and is the file system utility for working with the file system

Dbfs file system.

So that's what I'm doing.

And then I have a assert result method here which takes an expected count and calculates the expected

count.

So after each iteration we will take the count from the final output table.

That will be my actual count and expected count.

I know based on my test data, I'll pass the expected count and then we will assert it.

Expected count should be equal to the actual count.

If this matches, we'll pass the test case.

Otherwise we'll print the failure message and raise an exception for failure.

I've added one more method here.

Wait for Micro-batch so every micro-batch will take some time to process the data.

So when the once the Micro-batch starts, it will go read the data from the landing zone, create a

raw data frame, and then we'll apply all the processing logic and at the end produce the final result

and write that result using the data frame dot write stream method.

So all that processing, reading, processing and writing will take some time, right?

So between the iterations, between the micro-batch we want to wait for some time, let's say 30s.

So I define this function which will allow us to sleep for 30s and we will call it from the main method.

And then finally we have run tests.

Run tests will call the clean tests first, then create a instance of our application invoice stream

application and then start the Invoice stream application.

And we will get the background query thread handle right.

So stream is started.

Background query is is started.

And as soon as we start ingesting data into the landing zone, the background query will start triggering

the micro batches.

And each micro batch will process some data sets from the landing zone.

So we want to do three iterations.

So I have code for three iterations here.

Each iteration will ingest some data files.

So we have three four data files here.

The first ingestion we will ingest one data file.

Then wait for micro batch to start and complete the processing.

So we want to wait for 30s.

And 32nd is already defined here as a default value.

So I'm not setting any value here.

And after 30s we expect that data will be produced in the final table.

So I'm calling assert result.

And for the first iteration I expect 1249 records to be produced.

So that's my expected value.

And if this pass we print validation passed.

And same way we apply three iterations.

The second iteration will ingest second data file and wait for 30s for batch to finish, then assert.

And this time we expect 2506 records.

Same way third iteration will validate the third micro batch.

And finally, once all the testing is done, we want to use the streaming background query handle to

stop it.

We start the stream at the beginning here.

Right.

And then once we are done with all the testing, we want to stop it because we are done with the testing.

So that's all about it.

Now if my cluster is ready, I can run it.

So let me open.

The invoice stream in a separate tab.

Connect to my cluster.

Cluster is already running.

Hit the run all once.

And see if everything works.

Waiting to run.

So it worked.

Everything worked.

Now let's come to the invoice stream, test suit and test it.

So before I run all, I should create the instance of my.

Best suit class and.

Call the run tests.

Right.

So let me write code here.

I'll simply paste it.

So I'm creating invoices, stream test suit instance and then invoice stream test suit dot run test

so that this method is called let me run it.

I hope it works.

So starting cleanup.

Okay, so it looks like some typo or something is wrong.

Syntax error at or near invoice line items.

Where is it?

In this schema.

Right.

This is the schema I've defined.

So looks like I made some mistake in the schema.

So error is near invoice line item syntax error.

Let me go and look at my schema definition.

Okay so here is delivery address.

It starts from this angle bracket and here.

And we should have a comma here.

Right.

And I missed that.

Let's try it again.

Okay, one more error.

Data stream.

Writer object has no attribute.

Output mode okay.

So let me go to data stream writer.

This is so.

I have a typo here.

Oh.

You.

The output mode.

Let's try once again.

Okay, so it looks like first iteration is started.

Waiting for 30s.

So the first Micro-batch picks up the data, does all the processing, and finishes producing the result

so that we can start the validation.

So starting validation and done.

Validation passed.

So first iteration passed.

Hopefully all the three iterations will pass.

So let's wait.

Great.

It worked, and I hope you started getting the sense of how spark is streaming applications work and

how we can create a spark streaming application.

That's all for this lecture.

See you again in the next lecture, and we'll start talking about some more details about Spark Structured

Streaming.

See you again.
