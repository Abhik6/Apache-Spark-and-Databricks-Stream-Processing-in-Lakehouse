Welcome back.

In this lecture we will try to understand the internals of Spark Structured Streaming.

We will try to understand how is Spark Structured streaming works behind the scene.

So let's start.

Data processing is a three step approach.

Write.

Read data.

Process it.

Prepare the result, and finally write the result somewhere.

Spark is no different and spark streaming is also no different.

So in Spark streaming, what we do.

Read data using spark dot read stream.

Then process the data using various kind of transformations.

Prepare the result and finally write the result using result data frame dot write stream.

That's all we do.

But how is spark will execute all this?

I hope you already learned that spark will take up all this code.

Submit it to spark SQL engine.

SQL engine will prepare a logical plan and then go through various optimizations.

Prepare a physical plan and choose the best physical plan to execute this code.

And that's what we call spark execution plan or Dag.

Right.

So spark will prepare an execution plan to execute it.

Let's assume the execution plan looks like this.

So what do we want to do here?

We want to use read stream to read data from a landing zone or from a streaming source.

Then apply some transformations, do some processing, prepare the result and use right stream to write

it to a table or a streaming sink.

So that's the high level execution plan.

But this is not a standard spark execution plan.

It is a spark streaming execution plan.

And a spark engine knows that how we are using Read stream and we are using right stream.

That's how spark knows this is an streaming execution plan.

Now, the next thing that we want to learn is how a spark engine will execute this streaming execution

plan for every streaming execution plan.

A spark will start a background thread to manage and coordinate the execution of this streaming execution

plan.

This background thread is known as streaming query of this execution plan.

Each execution plan.

Each streaming execution plan will have its own streaming query, right?

So what will happen once the streaming execution plan is ready?

A spark driver will start a streaming query for that execution plan, and as soon as the streaming query

starts, it will prepare the checkpoint location.

Checkpoint location is nothing but a directory location where streaming query will keep some housekeeping

information.

We will learn what information goes here.

So the first thing that is Streaming Query will do is to initialize the checkpoint location.

And then it goes and looks into the streaming data source.

In our case, the streaming data source is a landing zone directory.

It's a directory location.

So streaming query will go and look into the directory location and check if there are data files available

for processing.

If there are no data files available for processing, streaming Query will keep a watch on the landing

zone directory and wait.

Wait until some data arrives in the landing zone directory because there is no data at the moment,

right?

And since there is no data, there is no reason to execute this plan.

For this execution plan, there is no data, so it's not going to do anything.

So Streaming Query will wait until some data arrives into the landing zone.

Let's assume we ingested one data file.

The file name is file one.

We ingested a new data file here in the landing zone is streaming.

Query will immediately know that because it is watching the landing zone directory.

So we ingested file one and streaming query will know that file one is now ready.

It is available for processing.

So what is streaming query will do.

It will note down the file name and update that information in the checkpoint location.

We are not copying data file.

We are not copying file one from the landing zone to checkpoint location.

We are just noting down the details that file one is now available for processing and streaming.

Query will note it down in the checkpoint location that now we are going to start processing the file

one.

Right.

And then it will trigger the Micro-batch.

It will trigger this execution plan.

And each trigger or each execution of this execution plan is known as batch right.

Once data file is available, streaming query will trigger the micro batch.

So your execution plan will start.

It will start with the read stream.

So what read stream will do.

Read stream will read file one from the landing zone.

Why file one.

Because Streaming Query will configure this micro batch to read only file one from the landing zone.

When streaming query triggers a micro batch, it configures what data you are going to read from the

streaming source.

In this case, streaming query configured file one as the micro batch input data.

So what read stream will do?

Read stream will go and read file one from the landing zone.

Then we have code for doing the processing.

We will do all the transformation, all the processing prepare the result.

And that result goes to write stream and write.

Stream will write the result into a table or a streaming sink.

Right.

So that's how it first micro batch will execute.

And streaming query will keep a watch on the execution of the micro batch.

It will keep on monitoring.

If Micro-batch is successfully complete is streaming query will know that okay, this micro batch is

finished and then streaming query will commit the checkpoint.

So before the micro batch starts, a streaming query will update the information in checkpoint that

we are starting one micro batch with file one as the input data for the micro batch, and at the end

when micro batch is complete, execution is complete.

Streaming query will commit that that everything is successfully processed.

Everything is complete now, but then your application will not stop here, right?

A streaming query will not stop here.

It will keep on running.

And in the next step, what is streaming query will do.

It will again start looking for the landing zone or the streaming source for new data.

So initially file one was there.

We processed it.

Now Streaming Query is looking into the landing zone waiting for new data.

Let's assume this time one more file is ingested.

This time we ingested file two.

So streaming query will know okay.

File two is now ready for processing.

So what streaming query will do?

It will repeat the same process.

We'll update the checkpoint with the file two.

It will update information in the checkpoint that we are going to start Microbatch two.

And the input data for Microbatch two is file two.

It will update that information in the checkpoint and then trigger the second microbatch.

So it will again trigger the second Microbatch and configure the input for the second Microbatch.

What is the input for the second Microbatch file two right.

So what will happen?

The read stream?

That's your first statement of the microbatch, right?

So read stream will read only file two from the landing zone.

Why only file two?

Because Streaming Query has already configured this Microbatch to read only file two.

File one is also there.

It's sitting in the landing zone, but read stream will not read file one.

It will read only file two.

And as a result what we are doing, the streaming query or this execution plan is reading only new data,

right?

Every micro-batch is reading only new data.

And then once you read the file two, you will do the processing whatever code you have written that

will do the transformation and processing prepare the result and that result.

You are using write stream to save into the sink so it will save into the table or sink whatever you

have configured.

Second iteration is done.

Streaming query is monitoring the execution of the second Micro-batch.

Once that is done, Streaming Query will know that, right?

And then it will again go back and commit the details into the checkpoint.

So checkpoint is now updated that Micro-batch two is finished and we process the file two successfully.

All that details are committed in the checkpoint, but it's an infinite process, right?

The streaming query will not stop.

It will never stop until we purposefully stop it.

So once we started streaming, query will keep on running and.

Keep on running microbatches in an infinite loop.

So what is the next second?

Micro-batch is completed.

The information is committed.

So what will happen is screaming query will again go back and start monitoring the streaming source

your landing zone directory.

Do we have any new file for data processing?

No.

Keep on waiting there.

Let's assume we ingested two data files this time.

So let's assume ingested file three and file four ingested two data file into the landing zone.

Streaming query will know that okay two new files are now available for processing.

So what it will do file three and file four.

It will put up that detail into the into the checkpoint location.

So it will update in the checkpoint location that we are going to start Micro-batch three with file

three and file four two data files.

Right.

And once that detail is in incorporated into the checkpoint, it will trigger the third Micro-batch.

Now third Micro-batch is configured with the two input data files file three and file four.

And as a result, your read stream statement will read only file three and file four from the landing

zone.

Remember file one, file two are also there in the landing zone.

We have not done any cleanup there, but Read Stream knows that it am supposed to read only file three

and file four.

Streaming query knows that file one and file two is already processed.

So it configured the third micro-batch to process only file three and file four and streaming query

will read only file three and file four.

Then we'll again apply all the transformations.

And finally use the write stream to write the results into the streaming sink.

And that's how third Micro-batch will run.

Streaming query will keep a watch on this execution.

As soon as this is done, Streaming Query will commit the results back into the checkpoint location.

And then you'll go back and start monitoring the landing zone for new data source.

So what does it mean?

So Streaming Query will take this execution plan which driver has prepared and.

Since the data, watch the data and as soon as new data is available for processing, a streaming query

will trigger the execution plan.

And that trigger is known as one micro batch trigger.

Each Micro-batch trigger is configured with some input data by the streaming query, and your application

will keep on running as a continuous loop of micro batches.

That's how streaming application works in spark.

That's how spark structured streaming works.

You want to stop it, you can stop the streaming query and this continuous loop will stop.

That's all.

I hope it made sense.

So let me quickly summarize a few things.

So we'll learn three things.

In this lecture we learned about streaming Query.

We learned about micro-batch and we learned about checkpoint.

You should be very clear about these three terms and their meaning.

So what is a streaming query.

A streaming query is a background thread.

The responsibility of the streaming query or the background thread is to configure, execute, monitor

the microbatches and it keeps on running Microbatches in an infinite loop.

And it also makes sure that what data Micro-batch is processing that data is committed into the checkpoint.

So we know that which Micro-batch processed, which data set.

And what is the status of the Micro-batch?

Is it successfully complete or it failed in between or whatever it is, right.

What is a Micro-batch?

Micro-batch is one iteration of your code, right?

Whatever code you have written, you will write code starting from ready stream and your code will end

at the write stream.

And then finally either two table or some other kind of action, right?

So that entire piece will be transformed into an execution plan by the spark driver itself.

Right?

That's usual.

That's how spark works.

Start from the data.

Read till an action, take up everything, prepare an execution plan and then execute that plan.

But in case of streaming application, driver will not immediately start execution of the execution

plan.

That plan is taken up by the streaming query, the background thread, and based on the availability

of data, the streaming query will trigger the execution plan, and each iteration of that execution

is known as Microbatch.

Right?

That's the micro batch.

And what is the checkpoint?

Checkpoint is a directory location where streaming query or your spark APIs.

Together they will keep some information updated in the checkpoint.

And basically that information is about the status of the micro batch, what micro batch we have successfully

executed, and what was the input data for this micro batch?

What data is already processed from the streaming source so that streaming query can figure out what

is already processed and what is new, right.

So the summary of all this is that spark structured streaming is implicitly incremental.

It is incremental in nature, so it keeps on running.

Microbatches.

With new data.

Every micro-batch runs with only new data, right?

So you are implicitly doing incremental processing.

You don't have to write any code or apply any logic for implementing the incremental data.

Read write.

Spark structured streaming gives his out of the box right.

So hope it makes sense.

And that's all for this lecture.

We will keep revisiting this concept again and again throughout the various examples that we are going

to develop.

And that's all for this lecture.

See you again.
