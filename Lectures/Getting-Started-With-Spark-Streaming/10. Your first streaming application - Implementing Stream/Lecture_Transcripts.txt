Welcome back.

In the previous lecture, we created a batch processing solution.

Let me show it to you.

So here is the solution.

That's what we developed in the previous lecture.

It was a word count problem, but it is a typical data engineering process, right?

We are doing a typical thing, which we do in most of the data engineering projects.

We are ingesting data from the landing zone and doing the processing and finally saving the results

into a final target table.

That's what we do in a typical project.

Our customers wanted to run this entire pipeline every 15 minutes.

That looks an achievable goal, right?

But let's think about it.

Let's think if our customer reduce the frequency to 30s.

Now they want us to run this entire pipeline and finish everything in 30s.

Can we do it?

Can this same solution sustain this requirement?

Think about it.

We have one big problem here.

What is that?

Let me talk about it.

So we have a full load process.

This entire solution performs a full load process.

What does it mean?

Simple.

We are reading data from the landing zone.

And then we are processing it in the first iteration.

Landing zone may have 1 or 2 files ingested.

We will read 1 or 2 files and process it.

In the second iteration.

We may have 1 or 2 more files.

So the second iteration will process all the new files plus the previous files.

Right.

In the 10th iteration, we may have ten, 15 or 20 files ingested and the 10th iteration will process

all the files that are present in this landing zone.

So what we are doing here, we are loading the full data set from the landing zone, processing the

full data set and finally overwriting the result into the target table.

So everything from top to bottom, from beginning to end is a full load process with the time your data

volume keeps on increasing and then achieving 30s or even 15 minutes may be impossible over a period

of time.

Your job will start taking more and more and more time and you won't be able to finish it in 15 minutes.

Forget about the 30s requirement.

So we have a back pressure problem here, right?

We have a back pressure problem because we are doing a full load, full process and complete override.

How do we solve this if we want to really solve it?

The one and the only approach is to implement a incremental data processing.

So what we want to do first iteration, whatever files are there in the landing zone, take those files

and process it and write the result here in the second iteration, whatever new files have arrived,

take only those files and do the processing only for those new data set and update the result into the

final target table instead of overwriting the final target table.

That's the incremental approach, right?

We want to do that, but how do we do that?

It's not easy thing to implement that kind of solution, and that's where Spark structured streaming

comes into play.

The spark structured streaming makes all these things super simple.

So let me implement.

Spark structured streaming for the same solution.

Translate our batch processing solution into a streaming solution and then we will talk about it.

What we are doing here.

So let's start.

So here is my batch word count application.

You are already familiar with this code, right?

What I want to do, I want to transform this code into a streaming word count.

But I'm not going to change the same code.

Let's keep it as it is.

I'll copy it.

And then.

Paste it into a new cell.

Change the class name.

And we will change this class to make it a streaming word count class.

Right.

So let's start making some changes.

Let me make some code changes here and then I will explain what did I change and what does it mean?

Right.

So first thing is to change the spark read into.

A spark dot read stream.

We don't want to read batch data.

We want to read streaming data and that's why we are changing it to spark read stream.

That's the first change.

Then we come down.

This is the place where we are writing the result to the table.

So I want to change this to.

Right stream.

So instead of writing batch data, we want to write streaming data.

And that's the second change at very high level.

These are the two most critical changes.

Instead of reading batch data, we want to use Spark dot Read stream.

Instead of writing batch data, we want to use data frame dot write stream.

These two changes are the most critical changes.

Other than that is some logistics.

So let me add something here.

That's all.

So I'm adding one extra option to tell checkpoint location.

And that's the kind of mandatory requirement for a streaming application.

A streaming application cannot exist without a checkpoint location because a streaming application implements

incremental data processing and for doing incremental data processing.

Spark streaming gives us a pre implemented solution of checkpointing.

Checkpointing is the location where a spark stream will store information about what is processed in

this iteration and in the next iteration.

What are the new files that we want to process?

All that information is maintained in the checkpoint location.

Spark structure is streaming.

Does that automatically?

All we have to do is to tell the checkpoint location.

Where do we want to create a spark, a checkpoint for us?

So that's I'm doing that's what I'm doing here.

We are almost done converting our batch application into a streaming application.

A few small changes are there.

The API is a little different, so.

I have moved here and I have save as table here.

These two names are changed in the spark streaming API.

So instead of mode override we have output mode.

Complete meaning is same mode overwrite means overwrite the result table every time.

Output mode complete means give the entire output every time.

Kind of overwrite the result every time and finally save as table changes to.

To table.

Both are same.

Save as table is going to save your result as a table and to table is going to save your result as a

table or send your output to a table.

Names are different, but meaning is same.

That's all.

Nothing else.

But there is a small change here.

The data frame dot write does not return anything but data frame dot write stream returns a streaming

query.

So we want to return that streaming query from this method.

That's all.

So what we want to do, right?

Stream returns a streaming query and we want to return that back from this.

Overwrite word count method.

That's all we are done.

Few changes we may have to make in the final.

Word count application.

Let me make those changes.

So instead of executing word count.

Let me change the print method.

The starting word count is stream.

That looks nice, right?

So we get the raw DF.

Then we create the quality dataframe and finally we calculate the result and then use the override word

count method to write the result.

Here is the override word count method, but this method returns the streaming query.

So let me hold this streaming query in a variable.

That's all.

I have this streaming query now, but last line is to write the result.

After that we are done.

But for some logistic reasons, I want to return the streaming query from here, so let me return.

The streaming query.

And that's all.

That is all we need to change in a batch processing word count to convert it to a stream processing

word count.

But let's try to understand what did.

Data processing is a three step process, right?

The first step is to read the data, then process it.

Process means implement your business logic.

Do all the transformation, whatever you want to do.

And then final step is to write the result in some table or in some sink so that your consumers or your

customers can get the results.

Read the results from there.

That's what we do.

Now think about batch processing and stream processing, how they are different.

Batch processing will also read data, process it and write.

Stream processing will also read data, process it and write.

The difference in batch and stream is going to be only in how do you read and how do you write the middle

part?

The processing part remains as it is without any change, and that's the magic of Uniform API or Unified

API of Apache Spark.

Spark gives you unified API for batch and stream.

And what does it mean?

All the processing part, at least all the processing part, remains as it is without any change.

So if you learn batch processing, you already learned stream processing.

The only difference is in how do you read the stream?

How do you write the stream, the how do you read a batch of data and how do you write a batch of data?

You already learned.

But in the stream processing, what we need to learn is how do we read the stream and how do we write

the stream?

Because changes are at only at those two places and that's what I have done in this code.

Let's quickly review what I have done.

So here is my streaming word count code.

The first change I did was change the read API to read stream API because we don't want to read a batch

of data.

We want to read a stream of data, and for that we have a different called read stream.

And the read stream will read the data as a stream.

It won't read it as a batch.

It will read it as a stream.

And the result will be a streaming data frame.

That is all.

So the first difference is in how do we read?

Instead of reading as a batch, we read it as a stream.

The second difference is in how do we write?

And that's what I did here in the override word count method.

So instead of writing a batch data frame what we are doing, write a stream.

Since we are reading a stream we are processing.

So result will be a stream.

So that's why we need to write the stream and that's the difference.

Rest all is just logistics.

So since we are writing a stream and a stream needs a checkpoint location, we cannot write a stream

without a checkpoint location.

So I'm telling what is my checkpoint location, where to create a checkpoint and then there are little

differences in the names of the API or methods.

So instead of mode, we are saying output mode instead of save as table.

What I'm saying is to table.

That's all the data frame dot write doesn't return anything but data frame dot write stream returns

a handle to a streaming query which runs in the background.

I'll explain that in a later video, but we want to return that streaming query handle from this method

that is also logistics.

And finally, in the main method or our word count method, we are not changing anything here.

Everything remains as it is.

Only thing is, I take the streaming query and return that streaming query from the word count method.

Why?

You will learn that in a minute when we start doing testing and that is all my batch word count application

is now transformed into a streaming word count application.

What is next?

How do we test it?

How do we write test case for a streaming job?

That's the next step.

And we will start doing that now.

So let me go to my workspace and open word count test suit.

We have already created batch word count test suit, so I'll copy the same.

And then.

We will modify it to make it a stream word count test.

So.

So.

Change the name.

So what will change?

What is going to change from batch test suit to streaming test suit?

Almost nothing.

Almost nothing will change.

The cleaning process remains the same.

Data ingestion process remains the same.

Assertion also remains the same.

The only difference is in how we run the test.

Right.

So the first change is instead of instantiating batch wordcount, we will instantiate our streaming

wordcount.

Let me open it once and so we can see it.

So this is batch word count.

This is streaming word count.

So this is the class that we want to instantiate now, right?

Because we want to test the streaming word count.

So instantiated the streaming word count.

Then what we are doing in the first iteration, we are ingesting first iteration of data.

We are ingesting some data and then we are calling word count batch word count.

Then we are ingesting second set of data and again, calling the batch word count and then ingesting

third set of data and again calling the batch word count.

So batch is designed in a way that for every iteration you have to execute the batch process.

But streaming process, you start it once and it will keep on running, right?

So what we need to do, we need to move it from here.

And.

Keep it here.

Start it only once.

Right.

That's all.

It's starting the process again and again is not required because it is streaming.

Once it started, stream process keeps on running.

So we start the word count stream process.

Initially, there won't be any data in the landing zone, so it will go look for data.

No data is there, so it will keep on waiting for data to arrive.

We ingest the first set of data here and immediately the word count is streaming.

Word count will pick up that data and start processing.

Then again, wait for the next set of data.

As soon as we ingest the second set of data, the streaming word count will pick up it and start processing.

So execution is automatic.

We start it once and the stream process keeps on listening it forever until we stop it.

So what is next?

What I want to do after ingesting data, I don't want to immediately go and assert the result from the

target directory.

What I want to do once data is ingested, wait for 30s and give 30s at least 30s for my streaming word

count to pick up the data, process it and produce the result in the target table.

So we want to sleep for 30s after every ingestion and then we do the assertion.

So let me prepare for that.

At the top I'll import time package.

I define a variable sleep time.

30s.

And then after ingesting data, I want to.

Sleep for the sleep time and before sleeping.

I'm just putting a message here.

So waiting for 30s and then sleep for 30s and then assert the result.

Same thing we want to do after every iteration.

Right?

That's all.

I'm done.

But one more last thing.

I started my.

Streaming job here.

Streaming word count process here.

It will keep on running.

It will keep on listening.

The landing zone for new data arrival.

As soon as new data arrives, it will start the processing and we can assert the result.

Then it will again keep on waiting for the next set of data we ingest the second set of data sleep for

30s and then assert the results.

Similarly, we ingest the third set of data, sleep for 30s and then assert the result at the end.

What I want to do, I want to stop my streaming query, which I started in the beginning, right?

So when I executed word count, it started the streaming query.

Let me quickly show you the word count method in the streaming class.

So streaming word count returns me a streaming query handle, and that's why we keep on returning this

streaming query handle from a couple of places.

So what I want to do here is.

When I start my streaming query, take the handle back and.

Keep it there.

And at the end, when I'm done with my testing, I can.

To stop my stream inquiry.

That's how I'm done.

So we are done defining our test suit.

What is next?

We want to run it right?

So let's create one cell and write code to execute it.

Streaming word count test suit equal to.

A stream to suit, instantiate the test suit and then streaming word count.

That's all we are done.

Do you want to run the streaming test suit now?

Let's do it.

So I already have it.

A cluster attached.

In the beginning.

We are doing batch word count test also.

So let me comment that.

So it doesn't run.

I don't want to run it and keep on waiting.

And once this is commented, let's run all.

Third.

Let me run it again.

Okay, so my streaming word count is running.

Starting cleanup is done.

The starting word count is stream.

So word count is stream is started.

Now we are testing first iteration of the batch.

Ingestion is done and I'm waiting for 30s so that the first batch of data is processed.

We want to give 30s for our application to pick up the data, do the processing and produce the results.

First iteration passed.

Now the second iteration.

Second one is also past.

We are running the third iteration.

Great.

So everything passed.

So we finished creating our first stream processing application.

It was a word count, simple thing, but it gives us a lot for learning or starting to learn stream

processing.

So we stop here and start analyzing what we did, how spark streaming works from the next lecture.

See you again.
