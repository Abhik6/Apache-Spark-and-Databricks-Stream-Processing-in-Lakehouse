Welcome back.

In this lecture we want to learn two things.

The first thing is how to use a delta table source connector, or how to use delta table as a streaming

source.

The second thing that we want to learn is how to chain streaming jobs.

Right?

So far we have created individual streaming jobs.

One single streaming job.

So behind the scene, a spark will create one streaming query which will keep on executing Microbatches.

But how do we create multiple streaming queries or multiple streaming jobs, and how do we chain them?

Right?

That's what we want to learn.

So let's start with a new example and we will learn how to change the streaming jobs.

Let's get started.

In an earlier lecture, we created a simple invoice processing application.

Right.

So we were ingesting invoices in this format.

We were applying some transformation and producing the result in this format.

I hope you already remember that.

In this example, what we want to do, we want to break the processing into two parts.

And first part should read data from the landing zone, ingest the data and create a raw data table.

That's all.

That should be my job.

Zero or the first part of the application.

One single independent stream processing application which only cares about ingesting the raw data and

creating a table, a target table, which is my raw data table.

And then I want to create a separate application which starts from reading the raw data table.

So it reads raw data table from the this table and applies the rest of the processing, which means

exploding this invoice and then flattening and doing all the processing and produces the result into

a target table.

Right.

So first job, first streaming job takes care of this part.

The second streaming job takes care of this part.

And this is a typical scenario when you are implementing a medallion architecture in your Data Lake

projects or Lakehouse projects, you will create multiple jobs, maybe one job for your bronze layer

where you will ingest all the raw data.

Deep, creating raw data tables.

And then you will have one separate job or set of jobs, which will be reading raw data and preparing

your silver layer tables, or implementing silver layer processing.

And then you may have some other set of jobs which will be reading silver layer tables and producing

the gold layer tables, right, producing the gold layer data.

So how all that can be done using a stream processing API.

We want to learn a smaller or miniature version of the same problem with this example, where we want

to create one job which ingests data and creates the raw table, and another job which reads or starts

from the raw table and create a silver layer table.

So let's start coding it.

Let me go to my workspace and create a new notebook.

Let's give a new name to the notebook.

Let call it medallion approach.

So we want to recreate the same application that we created earlier using a medallion approach, which

means we want to break the processing into two parts and create two streaming jobs.

Right.

So let me go to my old notebook.

And let's go to Invoice Stream and copy the same code.

So.

I can make changes because most of the processing is going to remain the same.

So we don't want to type in everything again from scratch.

Let's close this and we will start making changes into the same code.

So let's rename it as bronze.

So basically I want to create two classes one class for bronze layer, one more class for silver layer.

So I'll come down and.

Create a new class there.

And call it silver.

Right.

And then.

All these methods, right?

I want to move these methods from bronze.

To silver because these are useful there.

Now let's start modifying bronze layer class and finish our coding.

So my constructor remains the same.

Get schema also remains the same.

There is no change in the schema, so we use the same.

The read invoices also remains the same.

We want to use the read stream.

The data format is Json.

We get a schema using this get schema method and read data from the invoices directory.

Right?

So my read stream also remains the same, but I want to take it as an opportunity to introduce you with

two additional.

Features of the file source or the directory source.

So for this example, we are using read stream to read data from a landing zone directory.

The Spark Structured Streaming directory source gives you an additional option known as clean source

option.

So what does it mean if you set that clean source option?

A spark structured streaming allows you to automatically clean the processed files, right?

So each Micro-batch will start reading some data from the landing zone.

It will do the processing finish that when the next microbatch starts, it will read new files and also

clean the old files which were processed by the previous microbatch, so that auto cleaning is also

there as a feature for read Stream.

So how do you implement the auto cleaning?

Give an option there, right?

That option.

And the option name is Clean Source and the.

There are two values.

One approach is to delete.

Right.

So what do we want.

Delete the files that are processed by the previous Micro-batch.

So every micro-batch starts it will pick up new files and pick up them for processing.

But old files that were already processed by the previous Micro-batch it will delete those files.

So that's an option available to you for file data sources or when you are reading from a directory.

It doesn't work with all other sources, but for a file or a directory data source, you can have that.

But cleaning or deleting the processed file is a little harsh, right?

It it is not recommended to keep on deleting files.

Those are your raw data files.

You ingested them.

What if something goes wrong?

You want to reprocess that data file.

So it's not always recommended to delete the file immediately just in the next micro batch right.

So we don't want to do that.

So let me comment it.

It's an option available to you.

You can use it instead.

There is another better option which is known as archive the file.

Right.

So we can ask Spark structured streaming to move the processed file from the current directory to some

archived location so that it is moved away from the main directory from the landing zone.

But we also have a copy somewhere else, just in case if you need it to reprocess that data, or for

whatever purpose you can restore from the archive location.

So archival is a better process or a better approach.

So let's do the archival.

What is the option.

Same option.

Clean source, but the value will be archive.

But where will it archive?

Which location?

What is your archive location?

Where will spark structured streaming?

Move your process to data file from the current location to your archive location.

You can give that location also using different option known as source archive dir.

So let me.

Paste that option here.

It's a little long.

So what I'm giving as next option.

Source archive and source archive dir is inside my base directory.

And then data and then invoices archive.

So what Spark Structured Streaming will do.

It will take all the processed files which are processed by previous Micro-batch right.

And move them from the landing zone which is this directory, to the archive location, which is this

directory.

Right.

So I wanted to introduce you to these two new.

Options for structured streaming file data source.

That's all the read invoices method remains the same.

The rest all remains the same.

So what is next?

We need to define a process.

Method, right?

The kind of main method which will do all the processing.

So let's create a process method.

What do we want to do in the process?

Method.

Look at the earlier process method, what we were doing, similar things we want to do in the bronze

layer.

So let me copy paste a few things.

So I want to print a message starting bronze stream.

And then we would need.

We will start a query and hold the query.

But before that I need to call the read invoices method, right?

So create invoices.

DataFrame.

Calling the read invoices method, and that method doesn't take any parameter.

So invoices we will create it.

And then we want to write the invoices.

So we will start with the invoices df dot write stream and then.

I want to give an option which is checkpoint location.

Right.

So checkpoint location is inside my base directory.

Then a checkpoint directory.

And the checkpoint directory for this process is invoices dot z.

Right.

So write stream will automatically create this directory and use it for checkpointing.

What is next.

We need output mode.

Append.

Because this is my raw data file, I want to keep on appending all the new data as it comes, right?

And then where do we want to save it?

We want to save it to a table.

And the table name is let's call it invoices Z.

I give the same checkpoint directory name also just to follow a standard and that's all.

And then finally I want to print a.

Print a final message, and at the end, return the query.

That's all.

Again, I want to take this as another opportunity to introduce you an approach for naming your structured

streaming queries.

We already learned when we run our structured streaming application, it starts a streaming query behind

the scene.

Right.

And that is streaming query is responsible for executing the microbatches for your execution plan for

each execution plan.

Spark will create one structured streaming query or one background thread, which is known as streaming

query thread.

In a real application, you may see ten 2000s of streaming queries running.

So how do you identify which query is for doing what?

Right.

So for that you can give a streaming query a unique name.

So let's give it query name.

Simple as that.

And let's call it bronze ingestion.

Right.

So what I'm doing.

This right stream will create a background thread which is known as a streaming query, and I want to

give it a name so that I can see spark UI and identify which query is executing and what it is doing,

for what purpose it is running.

And that can be done with a proper naming.

Right.

So I'm giving a proper name to this query.

And that's all we are done with our bronze layer.

So bronze layer class will have a constructor.

We have a Get schema.

These are two main methods.

Read invoices will read data from a landing zone.

Which is our raw data file.

Once data is ingested, it will start reading it from here.

And we are also configuring archival.

So every micro batch will archive the data files that are already processed by the previous micro batch.

So that's done.

And then we have a final process method which we can call from our test case or from outside while scheduling

the job or from our workflow.

And this process method will use the right stream to write data into invoices table, which is my bronze

layer table and it will use checkpoint directory.

And the query name will be Bronze injection.

That's all.

We are done with the bronze layer.

Now come to silver layer.

The silver layer class is a little more easier.

We still have the constructor which remains as it is explode function.

We are going to need that right and flatten invoices function.

We are also going to need that also which will remain same and append invoices.

We will have to.

Change a little bit this and we will have to change this.

But we are not reading any data for silver layer.

Right.

So I need to add a method here for reading data.

Right.

So read invoices will read data.

But from where do we want to read it?

We don't want to read it from the landing zone.

That responsibility is given to the bronze layer.

Bronze layer will read it from the landing zone and produce the outcome into a table.

Right.

And that table name is invoices.

So we want to read from this table from the bronze layer table.

The silver layer will read from the bronze layer table.

So.

We will return a data frame after reading it, and we write code for reading here.

So we use spark dot read stream and then.

Read from where?

Read from a table.

So this one is a little more simple.

We simply say read from the table and that's all we are done.

Right?

So use read stream to read from a table, create a streaming data frame and return that data frame.

That's all.

Compare it with this guy.

Read invoices.

Read invoices.

Is reading from a landing zone.

So we'll have to specify some additional things like what is the Json format?

What is the schema.

Right.

But when you are reading from a table here it's a delta table, right.

The table method supports reading from a delta table.

And since we are reading from a date delta table so we don't have to specify format, we don't have

to specify a schema because all those things are already there inside the delta table metadata.

Right.

So spark read stream can figure out all those details.

So all we have to do is to tell read from this table.

That's all we are done.

So we are reading from this table.

And then we will be passing that data frame to explode invoices to explode it.

And then result, we will be passing here in the flattened invoices to flatten it.

And then finally we will be doing append invoices.

So append invoices will write the output into the invoice line item.

So everything looks fine to me here right.

We want to use the format as Delta.

And we are already giving a checkpoint location.

Maybe let me change the checkpoint location a little bit.

It is a good idea to use the checkpoint directory name, same as the table name or your target name

so you know okay, this checkpoint is created for writing data into this table, right?

So that's a simple naming convention.

Output mode is append and to table.

We want to write into this table.

Let's give a query name for this one.

Also because we will be running two queries at a time right in the same cluster.

So we need to give a proper name to each query so that we can identify which query is doing what.

So.

So this query name gave bronze ingestion.

Let's give it a name.

And.

Let's call it silver processing.

Whatever.

Right.

All we need to do is to give a proper query name.

So that's all.

My append invoices is also done.

Now let's come to the process and see what do we want to do here.

So let me change the message string here.

Let's call it starting Silver Stream.

And then we create invoices, DF calling, read invoices, which we redefined here.

Right.

And then call explode invoices.

And get the exploded data frame.

Then pass this exploded data frame to flatten invoices and get the result data frame, and use the result

data frame to call the append invoices and pass the result data frame.

And done and return the query so it looks like fine, right?

I don't see any change required here, so this one works fine.

So that's all.

We are done with the silver layer.

And what we did, we broke the entire processing which we are doing earlier in a single process.

Now we broke it into two processes one bronze layer process, another silver layer process.

And we created two classes for that one bronze layer class, one silver layer class.

The bronze layer is responsible for reading data from the landing zone using read invoices method,

and then writing it into the invoices table which is bronze layer table and silver layer is going to.

Read data from the invoices, which is produced by the bronze layer process, and then do all the processing

and finally write it to the final table, which is our silver layer table.

Done, and return the query.

That's all.

We are done with this.

What is next?

We need to write a test suite for this.

So let's do that.

So let me go back to my workspace and create a new notebook.

Let's give a name to this notebook.

So we call it medallion approach test suit.

What is the first step we need to import our medallion approach notebook.

Right.

So let me copy the name.

Come back here and the current directory import the zero seven medallion approach.

And then we need to write the test suit for this medallion approach class for this guy.

So in the interest of time I'll paste the code.

I've already written it.

And then I'll quickly walk you through through the code.

You have been writing test suits already, so it should not be difficult and I hope you can understand

that.

So let me paste the code.

Okay, so here is my class medallion approach test suit.

I'll explain it, but before that, let me paste code for executing it.

That's all.

Now let's look at what we are doing in the test suit.

As usual, like all other earlier test suits, we have a constructor where we are defining our base

directory.

Then we have clean tests which will clean up the directory tables and other things to make sure our

testing starts on a clean environment.

So what I'm doing here.

Earlier I was dropping only one table, but now we have two tables.

So I'm dropping the invoice table also.

That's an addition to the code.

And I'm cleaning both the directories for both the tables.

And then I'm cleaning both the checkpoints because we are writing it two times.

Right.

One write stream is writing to bronze layer table.

Another write stream is writing to silver layer table.

So we have to write operations to checkpoints.

So I'm cleaning both the checkpoints.

Then I'm deleting archive directory and the landing zone directory.

Also both deletion is required to create a clean environment.

And then I'm creating the landing zone directory so we don't get errors like path not found.

And that's all done.

That's the clean test.

All the code is same.

You are already familiar with this code.

And then I have ingest data which is same like earlier.

There is no change here.

We are ingesting data into the landing zone.

Assert result is also same.

There is no change in the assert result because we are recreating the same example same data set right?

Now wait for Micro-batch is also same.

There are changes in the run tests.

So let's see what we are doing here.

So run test.

We start with cleaning the environment.

So we call clean tests first.

And then we create the bronze layer instance right.

Instantiate our bronze layer class and then start the bronze layer query.

The bronze layer query will start immediately.

There is no data in the landing zone, right?

So nothing to worry.

The background query for the bronze layer will keep on waiting for new data to arrive, and as soon

as the data starts arriving, it will start creating the micro batch.

So nothing to worry then will also start the instantiate the silver layer class and start the silver

layer process.

The bronze layer table is also blank or it doesn't exist, right?

So nothing to worry.

The streaming query for the silver layer will wait.

Wait for table to be created and data to be ingested in the in the bronze layer table.

And then we start processing iteration by iteration.

So we ingest data for the first iteration and then wait for both the micro batches.

To start and finish.

So first the bronze layer Microbatch will start because we ingested data into the landing zone.

And once the bronze layer microbatch finished producing data into the bronze layer table, then the

silver layer Microbatch will start because data is already arrived into the bronze layer table.

So silver layer process was waiting for bronze layer table to receive some data.

So silver layer process will also start.

And then we wait for both these processes to finish their first microbatches.

And then we validate.

Right?

Same thing, same approach we take for the second iteration, we ingest one more file into the landing

zone, wait for 30s for both the microbatches to trigger and finish the processing.

We finish assertion after that, and then we do the same thing with the third iteration.

And finally stop the bronze layer query and also stop the silver layer query.

That's what we want to do.

One more additional test case I have written here, which is basically to validate if we have a successfully

archived the files.

Right.

So I'm testing it three times.

Right.

I'm ingesting.

First data file here.

Then second data file here.

And then third data file here.

So as soon as I ingest the first data file, first Micro-batch will start.

It will process the first data file.

Produce the result into the bronze layer table.

Then when I start ingest the second data file, second Micro-batch will start.

It will process the second data file and produce the result into the bronze layer table.

But it will also clean up the first data file, which was already processed by the first Micro-batch.

So it will archive the first data file right?

And third Micro-batch will archive the second data file, which was already processed by the second

Micro-batch.

Right.

But the third data file will not be archived because we are stopping after the third Micro-batch.

So if I want to validate whether my archival was successful, I can go look into the archive directory

and see if there are two data files there.

Right.

First data file and second data file which which were processed by the first and second Micro-batch

third file will not be there.

Remember that because we stopped after third Micro-batch.

So how to validate it?

I'm using Databricks utilities.

Right.

DB utils file system utility and using RLS command to list the archive directory location.

But remember note this carefully.

What is the archive directory location?

Archive directory location is my base directory.

Slash data slash invoices archive.

This is the place which I defined in my archive location.

Let's go back and see it.

So source archive dir is my base directory slash data slash invoices archive.

That's the directory.

But while archiving a spark structured streaming will not dump files directly into this directory.

It will copy the entire directory structure.

Inside this directory.

Right.

So entire directory structure of my landing zone.

So what is the directory structure of my landing zone.

My base directory slash data slash invoices.

Right.

So the archival will.

Produce this entire directory structure inside this archive directory.

And that's important because while archiving, we also want to preserve the entire directory structure

from where we archived it.

It should not be that five processes are archiving into the same archive directory, and they all keep

on dumping all their files into the same archive directory.

No spark takes a better approach, will use the archive directory as the base location, and then preserve

the entire directory structure of the archive files inside the archive directory base location.

So if I want to do and see my two files are there or not, I have to go inside my archive directory

location and then go inside the entire directory structure of my landing zone.

So which is my base directory slash data slash invoices.

And inside this.

Directory.

I should see two files.

So I do ls and then run a for loop and assert if the file name in this directory is in expected list

of files and expected.

Okay, so this should be archives expected.

And and the archives expected is defined here.

I expect two files invoices, one dot Json and invoices two dot Json.

If these two things matches, great, pass the test case.

Else.

Print the error message and done.

That's all.

Finally, I have these two lines to instantiate my test suit.

This is my test suit here.

And then call the run tests.

Let's try running it hoping that everything is fine.

My cluster is down, so let me create a new cluster.

Okay.

It will take five minutes, so I'll pause the video and once it is up, I'll attach my cluster and run

my test suit.

Great.

So my cluster is up.

Now let me try running it and see if we.

Get any errors.

So looks like there is an error in indentation of the process method.

Okay.

That process method is defined here.

Let me quickly look.

Line 26 must be in the first one.

Okay, so it looks like little.

Out.

Okay, let's try it again.

Running now.

So it is starting the bronze layer background query thread.

And let me take it as an opportunity to take you to the spark UI and show you the.

The streaming queries.

So.

It's.

As of now, it's starting bronze stream.

Let it start.

Bronze and silver.

Both these streams.

A starting Silver Stream.

So if you come to the spark UI, you will see this structured streaming tab.

So you come to a structured streaming tab and you will see all your background queries running here.

So we have a silver processing query.

We have a bronze ingestion query.

And that's why we give names to these queries.

They are in running status.

And let me zoom it a little bit.

And as of now there is no detail.

So.

Let it run and we will see some more details on the spark.

So first iteration passed.

That should be enough to see some details here.

So refresh this page.

Come back to structured streaming.

And you can see both these latest batch is batch one which is running.

It also shows how much data it is processing.

Number of records processed per second and all that.

But these two queries are running.

So let's wait it to finish.

We are at the last iteration.

At the end of our test, we are stopping both the queries, right?

Remember that.

And we will see that how it impacts the spark at the end.

So third iteration validation done.

And then we are validating.

Archive are done.

Everything is done.

Now you come to the spark.

Both the queries should be stopped.

So let's refresh this and you will see status finished because we stopped both the queries at the end

and they executed three micro batches.

Right.

Because we ingested three data files.

And that's all for this lecture.

See you again.
