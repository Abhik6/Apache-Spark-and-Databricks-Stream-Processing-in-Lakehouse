Welcome Back.

This lecture will discuss a common problem scenario in silver layer processing.

So let's start looking at the data flow diagram.

Here it is.

Look at this gray line.

We are supposed to read user profile update events from the Kafka multiplex table.

And merge the changes in the user profile table.

Correct?

Let me explain a scenario.

Let's assume you have one user record in your destination table.

It looks like this.

You are running merge new updates to this table in batch mode.

And you got some new updates for your next batch.

They look like the one shown below.

You have three updated records for the same user.

He updates his first name at 10 AM.

Then he updates his date of birth at 10:02 AM.

Finally, he updated his address at 10:06 AM.

You are running a 10-minute batch to merge these updates into the target table.

How will you handle it?

Well!

Do you understand the problem here?

We have multiple updates for the same record.

And the order of updates is supercritical.

If you apply the first one, the second, and finally, the third record to the target.

And you do it in the order of records shown here.

You will miss updating some information.

The final record looks like this.

We applied the first record, and hence we modified the first name.

Then we applied the second record and modified the street address.

But we lost the street address update when we applied the third record.

And that's the problem here.

So if you are getting multiple updates for the same record in the same batch,

you must apply the changes in the order of arrival.

And this is not only the problem in batch updates.

This is a problem in streaming updates, also.

Why?

Because Spark streaming is a micro-batch.

You may get multiple updates for the same record in a micro-batch.

Correct?

How do we handle this problem?

Simple!

Rank the record by Updated timestamp in descending order.

The source table will look like this.

Now you can remove the ranks 2 and 3 from the input and apply only the rank 1 record.

The Rank 1 record is the last update and comes with all the changes.

So leaving 2, 3, and applying only 1 is perfectly fine for our example.

Correct?

And that's what we are going to do in the silver layer.

So now go back to the silver layer code and look at the upsert_user_profile method.

So now go back to the silver layer code and look at the upsert_user_profile method.

Here it is.

The code looks the same as other functions.

I am reading the source using the readStream.

Applying the necessary transformation using a set of select() expressions.

Removing duplicates using the dropDuplicates.

But this dropDuplicates will not remove the multiple updates.

Why?

Because we are removing duplicates for id and updated timestamp.

Finally, I am using the writeStream to merge the data into the target.

So the code is similar to the other functions.

But where are we ranking and eliminating ranks 2 and 3 from the input?

Well! That is done in the data_upserter.upsert function.

Let me show you.

I am creating the data_upserter using the CDCUpserter.

You can see it here.

Now jump to the top of the notebook and look at the CDCUpserter class.

The approach of the CDCUpserter is the same.

But we have some extra code to create a windowing aggregate and compute the rank for each record.

Then we filter it to keep only the first rank.

And that is all.

We createOrReplaceTempView after the filter and execute the merge statement.

That's all.

Great!

I hope you learned a new technique.

The point is straight.

The foreachBatch is a powerful tool.

You can bring micro-batch data to a user-defined function,

apply some transformations, create a temp view, and run some SQL expressions.

You can handle a lot of requirements and complexities using this technique.

Great!

That's all about our silver layer coding.

See you in the following lecture, and we will talk about the gold layer.

Keep Learning and Keep Growing.
