Welcome back.

We completed coding for all the medalion architecture layers.

And we also defined a run notebook to trigger all the jobs.

The next step is to work on our test strategy.

So, let's talk about our testing approach.

Here is the data flow diagram.

We have written the code using the Stream processing approach.

And we can run the code in batch or streaming mode.

So, we want to create two test cases.

One for batch mode and another for streaming mode.

However, testing requires preparing test data.

We have five inputs for our system.

So, we will need five input data sets for each input type.

We need some user registrations, some set of user profile updates, and a bunch of BPM stream records.

Similarly, we need some workout sessions and login/logout events.

Preparing test data is a challenging task.

But I have done that already.

Here are the test data files.

I have two sets of test data.

Payload 1

and Payload 2

Each payload comes with five input data files.

So how are we going to use them?

We will copy the first payload into the landing zone and trigger our workflow jobs.

Our job will process all the records from the first payload and validate the results.

I have also prepared two sets of expected gold layer reports to compare the output.

After the first payload and job run,

we will compare the gold layer table with the expected outcome in the gym_summary_1.parquet file.

We will pass the first iteration test if the results are as expected.

Then, we will copy the second payload into the landing zone and rerun the job with the second set of inputs.

We will again verify the results with the second set of expected reports.

And that's how we can do complete end-to-end integration testing.

The point is straight.

We should test at least two iterations of input.

We can have more than two iterations,

but at least two payloads are required to see how the system behaves with an incremental load.

Preparing test data files is one of the most challenging activities.

Once you have the test data, the next step is to automate the integration testing.

How do we do that?

In our example, I must create three notebooks to automate my test.

Producer notebook

Batch test notebook

and Streaming test notebook.

The producer notebook will copy the first payload to the landing zone.

The batch test will call the run notebook to execute the jobs in batch mode.

Once the batch mode jobs are complete, we will run our validation function to verify the result.

Then, we can again call the producer to copy the second payload.

And finally, call the batch test for running the second iteration.

The same thing is done with the streaming test.

We can use the producer notebook to copy the first and second payloads.

The streaming test notebook will trigger the job in streaming mode.

And that should be all.

So, the next set of activities is to develop three notebooks.

Producer notebook

Batch test notebook

and Streaming test notebook

And we will do that in the following lectures.

See you again

Keep learning and Keep growing.
