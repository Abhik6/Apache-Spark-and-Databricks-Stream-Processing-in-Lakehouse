Welcome Back.

We finished coding for data ingestion into the bronze layer and processing for the silver and gold layers.

So, the core of our project development is complete.

Now, it's time to plan how we want to run our jobs.

Let's look at the complete data flow diagram.

Here it is.

How many jobs do we have?

I can see three bronze layer targets, so we have three bronze layer jobs.

Correct?

However, I have created one single consumption method in the bronze layer.

The single consume method triggers all three pipelines and ingests data into all three bronze layer tables.

So, for the bronze layer, I must call a single consumption method.

Similarly, I have created a single upsert method for the silver layer and a single upsert method for the gold layer.

The point is straight.

I will call three methods to execute all the jobs shown here.

One for the bronze layer, another for silver, and the last for the gold layer.

And everything will start running.

Correct?

Let's create a standard run notebook and code what we just discussed.

Go to the development environment.

Here I am.

Go to Workspace, Repos and choose your SBIT repository.

Now, you can add a new notebook here.

Rename the notebook.

We are now ready to code.

We have three notebooks for bronze, silver, and gold layers.

Correct?

These notebooks define bronze, silver, and gold classes.

And all the class method takes three parameters.

Environment

RunType

and ProcessingTime

We will be calling those class methods.

So, the first thing in the run notebook is to define the notebook's three parameters.

Let me paste the parameter definitions in the first cell.

Great!

So, we defined three parameters.

The first parameter tells the environment and unity catalog name.

This parameter allows us to run our notebook in different environments.

The second parameter is the run type.

If we want to run our workflow in batch mode, we can pass the run type as 'once'.

Any other value will run the code in streaming mode.

The last parameter is ProcessingTime.

This parameter is only valid for streaming mode, and it defines the trigger interval.

Now that we have the parameters, the next step is to extract the parameter values into Python variables.

Let me do that in the next cell.

Simple!

I am extracting three parameter values and printing a console message about the runtype.

Great!

What is next?

Before I start the jobs, let me paste some performance-tuning configurations in the next cell.

Good.

What are we doing?

I am setting four configurations.

I am setting the default shuffle, Partitions to the number of executor cores.

We know the default shuffle partition is 200, which is unsuitable in most cases.

The best default should be equal to the number of cores.

And hence, I am setting it to defaultParallelism, which is nothing but the CPU cores.

We expect to create small files due to streaming ingestion into the bronze layer.

So, it is also recommended to enable optimizeWrite and autoCompact for the delta tables.

And that's what I am doing in the following two lines.

We are using streaming API all over the place.

So, it is also recommended to use RocksDBStateStoreProvider,

to optimize the streaming state store read/write performance.

And that's the last optimization config.

These four are the bare minimum for our use case.

So, I am setting them before I start the Spark jobs.

We are now ready to run our bronze, silver, and gold jobs.

But before that, we should also run the setup module and history loader.

Correct?

So, let me import the setup module.

Then, I will import the history loader module.

The next step is to create setup and history loader objects.

Now, we are ready to run the setup and history loader.

However, these two must run only once.

Correct?

So, let me paste some code in the next cell.

Good.

So I am checking if the database is already created, then setup is not required.

What does it mean?

If the database is already created, we executed the setup script earlier.

So we do not need to run setup and history loader.

If the setup is not already executed and required,

we run the setup method, validate it, then run the history loader and validate it.

I hope this part is clear.

We have the setup now.

What is next?

The next step is to run the bronze, silver, and gold methods.

So, let me import the bronze layer.

Then, we import the silver layer.

Finally, we import the gold layer.

The next step is to create objects for all the three layers.

That's all.

Now, we can trigger the bronze layer method.

Once the bronze layer is complete, we can run the silver layer.

Finally, we can run the gold layer.

And that's all.

We are done.

We created the run notebook to start all our jobs.

But how do we use this notebook?

Well, it depends on our requirements.

Let's assume we want to run all the jobs in batch mode every hour.

So I can create a Databricks workflow job to schedule this notebook every hour.

And that is all.

Let's assume we wanted to run it as a streaming pipeline.

So we can create a Databricks workflow job to trigger this notebook only once.

That's all.

Once triggered, the streaming job will keep on running.

The point is straight.

We have a run notebook.

And we are now ready to create a workflow job using this notebook as per our requirement.

Hope that made sense to you.

That's all for planning and scripting our jobs.

See you again.

Keep Learning and Keep Growing.
