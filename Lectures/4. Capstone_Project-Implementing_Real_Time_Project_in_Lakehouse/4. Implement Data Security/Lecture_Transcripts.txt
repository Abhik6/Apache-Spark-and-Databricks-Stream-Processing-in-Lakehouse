Welcome Back.

We completed the storage layer design for our lakehouse in the earlier lecture.

Here is an updated version of the Lakehouse platform architecture diagram.

Now let me add a compute over the storage.

Here it is.

So we can create a Databricks cluster and run our Spark applications on the cluster.

And these Spark applications can access data directly from the storage layer.

We have two data access points in this architecture.

The first data access point is directly accessing the ADLS Containers and directories.

You Spark applications or the users can directly access the directories.

I have a simple scenario explained in the diagram.

In this scenario, a Spark application reads raw data from the data zone,

applies some transformation, and saves the output directly in the bronze_db directory.

The Spark job is also creating state information in the checkpoint directory.

We assume that we mounted these directories in the DBFS so the users can access them conveniently.

This approach comes with two problems.

Directory mounting does not support user-level permissions.

Once a directory is mounted, it is available for all the users in the Databricks workspace.

So we do not have fine-grained security control over the mounted directories.

The second problem is the risk of corrupting data.

If applications directly write data to the bronze_db directory, they may corrupt the bronze layer table structure.

A minor bug in the application comes with a massive risk of corrupting all the tables in the directory.

We also have another data access point.

Here is the diagram to represent it.

In this scenario, your Spark application is accessing database tables and does not access table directories directly.

This approach does not bring the risk of corrupting table directories.

However, we still lack fine-grained user permissions.

And we do not have any restrictions in place to stop users from directly accessing the table directories.

We can make a best practice rule and tell our teams not to directly access table directories.

But we do not have anything to block it.

The point is straight.

How do we implement fine-grained data access control?

We can design a data governance layer using the Databricks Unity Catalog.

So let me add a new layer between the storage and compute layers.

And this layer is the data governance layer using the Unity Catalog.

So all the user access will pass through the Unity catalog.

Let me show you the first scenario.

Here it is.

I have a simple scenario explained in the diagram.

In this scenario, a Spark application reads raw data from the data zone,

applies some transformation, and saves the output to the bronze_db table.

The Spark job is also creating state information in the checkpoint directory.

How do we support this use case?

We can grant read-only permission to the Spark application for the data zone directory.

Then we grant the read/write permission on the bronze_db tables.

We can also grant the read/write permission on the checkpoint directory.

In this case, the application does not have read/write permission on the bronze_db directory,

so we stop the direct access to the table directories.

Now let's see how we can implement the second use case.

Here it is.

In this case, we grant read/write permissions to the bronze, silver, and gold databases.

However, we do not allow users or applications to directly access the database directories.

And that solves our fine-grained access control problem.

Unity catalog also allows us to create user groups and grant appropriate permissions to the groups.

We can add users to an appropriate group and simplify access management.

And that's all about the data security design.

Let me summarize.

We do not grant direct access to the data directories for everyone.

Then we implement the Unity catalog and grant access permissions via the Unity catalog.

So only authorized users can access the data.

That's all about the data security design.

See you again in the following lecture.

Keep Learning and Keep Growing.
