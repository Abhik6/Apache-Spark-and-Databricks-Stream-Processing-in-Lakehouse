Welcome Back.

We designed the lakehouse platform.

Let me show you the current state of our design.

Here it is.

Now the next step is to design the data ingestion.

We have five data sets.

Device and user registration data is available in a cloud database.

We want to bring that into the lakehouse platform.

The user profile CDC, BPM Stream, and the workout session start/stop events are coming to the Kafka topic.

We also want to bring those events into our lakehouse platform.

Finally, the login/logout events are combined and updated into a cloud database table.

So we can directly bring them from the database to the Lakehouse platform.

So we have two types of source systems.

Cloud database

and Kafka topic

We assume the cloud database is a SQL server database in Azure cloud.

It could be Azure SQL database, a managed SQL server of Azure cloud.

Ingesting data from Azure SQL is super simple using the Azure Data Factory.

You can create a CDC factory resource or use Native CDC in a data flow.

And you can ingest data into a subdirectory inside your ADLS data zone.

You complete the data ingestion work once your data files are saved into the ADLS data zone.

And this approach decouples the data ingestion from the Lakehouse platform.

You are directly ingesting data into the ADLS directory, and the ingestion has nothing to do with the lakehouse.

Your ingestion pipeline may run in batch mode or continuous mode.

The rest of the data processing in your lakehouse will have no process dependency on the ingestion.

Good.

Now let's talk about the other data source.

You are collecting data into three Kafka topics.

We also want to ingest these topics into our lakehouse.

The ingestion target will be the same as the Azure SQL target.

What does it mean?

We will ingest Kafka topic data into another ADLS directory inside the data zone.

The point is straight.

We will ingest all the required data into the data zone.

The data zone is our temporary data landing zone for all types of ingestion.

So our ingestion pipeline and the lakehouse pipelines are not directly connected.

They are connected via the data landing zone.

But they remain two separate pipelines,

and we will have total flexibility to develop them using the most suitable tool.

And we can also manage them separately.

Make sense?

Great!

How do you propose to ingest data from the Kafka topic to the ADLS data zone?

We can do it using multiple methods and tools.

However, the Kafka ADLS Gen2 Sink connector is the most suitable and reliable option.

So you can spin up a Kafka connect pipeline using the ADLS Gen2 Sink connector.

The connector will consume data from the Kafka topic and create a JSON file in the data zone.

And that is all.

Your data ingestion design is done.

We will move to the implementation later, but we finished the design.

That's all for this lecture.

See you again in the following lecture.

Keep Learning and Keep Growing.

