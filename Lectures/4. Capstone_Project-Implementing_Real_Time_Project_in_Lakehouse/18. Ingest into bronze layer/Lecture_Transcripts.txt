Welcome Back.

We already finished our discussion on loading historical data.

Now it's time to start working on the three layers of the medallion architecture.

The first layer of the medallion architecture is the bronze layer.

So let's write code to ingest raw data into the bronze layer.

Let me show you the data flow diagram before we start coding.

Here it is.

We have three landing zone directories, as shown here:

reg_user

gym_logs

and kfk_mx.

We are collecting raw data in these landing zone directories.

As part of the bronze layer data ingestion,

we must write code to load the data from these landing zone directories into the bronze layer table.

So you are looking at the three green lines.

And we will be writing code to implement the three green lines.

Correct?

How do you want to do it?

We have two approaches to doing it.

Batch processing approach

Stream processing approach

Which approach do you want to implement?

I recommend using a stream processing approach.

Why?

Because Spark stream processing comes with many advantages.

Let me highlight.

You can run your stream processing workload like a batch at regular intervals.

You can also run your stream processing workload as a continuous stream.

Stream processing automatically performs a commit and checkpoint to implement the transaction.

Stream processing jobs automatically track the progress, stop and restart from the same point.

The point is straight.

Spark stream processing is more flexible, easy to implement, and reliable.

So we will implement the bronze layer ingestion using the Spark streaming code.

Great!

Now let's go to the Databricks environment and create a new notebook for the bronze layer.

We have three green lines here that we must implement.

So will be writing at lest three functions to move data from landing zone to the bronze layer.

Go to Workspace

Repos and choose your SBIT repo.

Add a new notebook.

Change the name of the notebook.

Great!

Now we will import the config notebook in the topmost cell.

The next step is to create a bronze layer code.

I already have the code, so let me paste it.

Here it is.

What are we doing here?

I am creating a Bronze class to wrap all the bronze layer ingestion functions inside the class.

The init method is the class constructor, and we define some variables in the constructor.

Then I have three functions to ingest data.

The consume_user_registration function ingests the user registration data from the landing zone.

Scroll down

and you can see the consume_gym_logins function.

This guy ingests gym login data.

Scroll down to see the third function

consume_kafka_multiplex

And this function reads Kafka topic data into the bronze layer table.

I also created three more functions:

consume,

assert_count,

and validate.

The consume function calls the three bronze layer functions.

So the consume is a single function to call all the bronze layer functions.

The assert_count is a generic utility function to validate the record count with the expected count.

And the validation method is our unit testing function.

That's all about the bronze layer code.

Now let me go back to the consume_user_registration function and explain it.

Great!

So the consume_user_registration function ingests data from the landing zone

and loaded it into the bronze layer table.

The code is super simple Spark streaming code using Databricks autoloader.

So what am I doing?

I am calling Spark.readStream to read the data in the streaming mode and create a streaming data frame.

The readStream uses cloud file format.

That means we are reading data using the Spark autoloader.

The rest of the code is simple for creating a read stream.

The next step is to writeStream to load the streaming data into a table.

I am defining the write stream in the append mode.

You should also notice that we are not transforming the input stream.

We simply read the stream and write it without any modification.

And that's obvious because the bronze layer simply loads the raw data.

We do not perform any transformation in the bronze layer.

And that's what I am doing here.

Reading the data from the landing zone and writing it to a table.

You should also notice that I am not triggering the write stream.

The trigger is defined later based on the input parameter.

The consume_user_registration takes an input parameter named once.

If once is true, we want to trigger the streaming write in batch mode.

And that's what I am doing at the end of the function.

If once == True, then we are writing toTable with the availableNow trigger.

Otherwise, we are writing toTable using a default 5-second processingTime trigger.

And that's the trick for creating a streaming code

and customizing it to run in batch or streaming based on the argument.

So we can call this function to trigger the ingestion job only once.

OR, we can call it to run in a 5-second loop.

The choice is yours.

Great!

I hope you learned to ingest data from the landing zone to the bronze layer.

I explained you only use the consume_user_registration function.

However, the rest of the functions are also written similarly.

Scroll down, and you can see the consume_gym_logins function.

Scroll down, and you can see the consume_gym_logins function.

The code is the same, but we read from the gym_logins landing directory

and write to the gym_logins_bz table.

Scroll down, and you can see the consume_kafka_multiplex.

This one is also taking the same approach.

We are reading from the kafka_multiplex_bz directory

and writing to the kafka_multiplex_bz table.

In this function, we also apply a left join on the raw data with the date lookup.

We are not applying any transformation or data validation.

But we are simply joining the date lookup to enrich the raw data with the year, month, week, and date.

And we are doing that so we can implement a partitioning strategy on the bronze layer table.

This table will ingest massive data, and we wanted to partition it by week number.

The raw data doesn't come with the week number; hence,

we are enriching the raw data by joining it with date lookup.

I hope that makes some sense.

Great!

And that's all about our bronze layer implementation.

As of now, we are writing code to do the things.

Once we have the functions,

we will stitch them together to create a workflow and schedule jobs according to our requirements.

I will explain that part at a later stage.

For now, let's write code for achieving our bronze, silver, and gold layer requirements.

Great!

See you again.

Keep learning and Keep growing.
