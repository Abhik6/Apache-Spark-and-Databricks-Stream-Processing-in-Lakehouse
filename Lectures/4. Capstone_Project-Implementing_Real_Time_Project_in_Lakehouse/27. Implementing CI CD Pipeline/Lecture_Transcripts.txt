Welcome Back.

We finished developing our project.

The last thing is implementing an automated CI/CD pipeline for your project.

The CI/CD implementation and automation is a complex skill in itself.

It is usually done by DevOps experts for your project.

However, as data engineers, we need to collaborate with the DevOps team to implement the same.

By implementing this project, let's try to understand some essentials of the CI/CD pipeline.

Let's start with an overview.

Here is the diagram that represents a typical CI/CD pipeline.

As we know, the CI/CD has got two terms.

CI

and CD.

The CI stands for continuous integration.

In the simplest terms, we call it the build pipeline.

Similarly, the CD stands for continuous deployment.

In simple terms, we call it a release pipeline.

So, a typical project may have two pipelines.

One build pipeline and a separate release pipeline.

We will also implement the same for our project

and learn the step-by-step process for implementing these two pipelines using Azure DevOps.

But before that, let me explain how it works.

We are the developers.

For Databricks Data Engineering projects,

we connect to the Databricks workspace and write our code in the workspace environment.

You can connect your Databricks Workspace with the Code repository of your choice.

For our project, we connected our Databricks workspace with the Azure Git Repos.

Azure git repos is part of the Azure DevOps platform.

It is nothing but a git repository.

You can connect your Databricks workspace with other Git repositories

such as Git Enterprise, Bit Bucket, or AWS Code commit.

You have total freedom to choose your git repository.

And we already learned how to connect your workspace environment with the git repository.

Good.

We also discussed earlier

that a typical project will have main, release, dev, and feature branches in your code repository.

Let's assume you write code in the Databricks workspace and create notebooks and Python files.

Once you are done, you will commit these notebooks and Python files in your feature branch.

The commit goes to your git repository.

The Git Repository will automatically notify the Azure DevOps build pipeline.

As shown in the diagram, let's assume we have designed a build pipeline using Aure DevOps.

I will show you a step-by-step process for creating this pipeline.

But for now, let's try to understand the process.

So, a developer will commit his code in the feature branch,

and the git repository sends an automatic notification to the Azure DevOps pipeline.

The Aure DevOps will read the notification and start taking action.

The first action is to launch an Ubuntu virtual machine.

We call it an agent.

The Agent is necessary to run your pipeline code.

So the DevOps will start a new Agent machine.

And then, on the Ubuntu Agent, your build pipeline code starts running.

The pipeline code does many things,

such as installing Python, PyTest, and Databricks Connector

and preparing the machine to successfully run your pipeline.

However, the main objective of the build pipeline is to check out the latest code from the Git repository.

And that's what the diagram shows here.

So, the build pipeline code will check the latest code from the git repository.

The next step is to connect with the Databricks environment and execute the unit test cases.

So, your build pipeline will execute unit tests to ensure everything is passed.

If a test case fails, your pipeline will stop there and send back a failure notification.

We have two approaches to running Unit test cases.

Run PyTest locally on the Agent.

and Run PyTest remotely on a Databricks environment using Databricks Connector.

The first approach is to run your PyTest test cases locally on the agent machine.

However, this approach may not work for data engineering projects.

Why?

Because data engineering project use cases may require access to data files, databases, tables, etc.

And we cannot create databases and tables on an agent machine.

We need a Databricks cluster environment for doing that.

Hence, we have a second approach.

In the second approach, you can connect to your Databricks test cluster using the Databricks connector.

And execute your test cases in your test cluster.

This approach allows you to write advanced unit and integration test cases and run them from your CI/CD pipeline.

Great!

So, let's assume your test cases passed.

The pipeline will take all your code, prepare a deployable artifact, and save it to an artifact repository.

What does this mean?

What do we mean by preparing an artifact?

Well! It is as simple as creating a zip file for all your notebooks.

And also creating Python wheels and keeping it ready for installation on the target cluster.

Great!

So that is all about the CI pipeline or the build pipeline.

Let me summarize it.

The build pipeline runs on every branch.

So, the build pipeline will run on the feature, dev, release, and main branches

as soon as we have a new code commit in the branch.

The build pipeline's main objective is to integrate everyone's code into a single code base.

Then, test the code to see if it passed the unit test cases.

Reject the code if it does not pass the unit test case and accept it if the test cases are passed.

And finally, prepare the artifact and keep it ready for deployment.

That is all about the CI part of the CI/CD pipeline.

Now let's talk about the CD part of it.

The CD or the release pipeline is separate.

And here are two essential facts about the release pipeline.

The release pipeline does not run on every branch.

You may not want to run a release pipeline on feature and dev branches.

Why?

Because the end goal of the release pipeline is to deploy code in a target environment.

And we usually have QA and Production environments.

A typical project deploys code to the QA environment from the release branch.

The code in production goes from the main branch.

Hence, a release pipeline is often limited to running only on the release and main branches.

The release pipeline is often triggered manually.

We can make it automated, but many projects prefer to keep it manual.

Now, let's try to understand the workflow.

A project team finishes all the development and merges all the feature branches into a common dev branch.

Once your dev branch is ready for deployment.

Someone from your team will raise a pull request to merge the dev branch into a release branch.

The pull request will send an automated notification to the build pipeline.

The build pipeline will run and prepare the artifact for the release branch.

Now, someone from your team will manually trigger the release pipeline.

You can automate the trigger so your release pipeline starts as soon as the artifact is ready.

But the dotted lines shown here are optional.

Many projects want to avoid this automation and trigger the release pipeline manually.

Why?

Just because they want to implement an approval process

and avoid automated deployments every time we raise a pull request.

We keep it manual to have better control.

That's all.

Let's assume you started your release pipeline manually.

The pipeline will create an agent machine.

This is again launching an Ubuntu VM to run your release pipeline code.

The next step is to extract the latest artifact from the repository and bring it to the agent machine.

Finally, the pipeline will run the code to deploy your artifact to the target environment.

What does it mean?

Simple!

Copy the notebooks to the target environment workspace.

Install the Python wheels and required dependencies in your target cluster.

That's all.

Your deployment is done.

You can also implement integration testing in the release pipeline.

This one is optional.

Many projects implement it as part of the QA deployment only.

Some projects do not implement an automated approach to integration testing.

That's all.

And I hope you now understand the CI/CD approach for your Databricks and Spark Projects.

The next step is to create both these pipelines step by step.

So you know what is happening and how it happens.

And that's the topic for the following lecture.

See you again.

Keep Learning and Keep growing.
