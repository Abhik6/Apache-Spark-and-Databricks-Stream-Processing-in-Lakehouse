Welcome Back.

I gave you a project requirement in the earlier lecture.

Let me show it to you again.

Here it is.

Now it's time to talk about the solution design for this requirement.

Let's start with data sets one and two.

The first data set creates a user account without much information about the user profile.

And this information goes into a cloud database.

This database is used by the mobile App.

However, we do not have user profile information in this database.

The user creates and updates his profile, but that information goes to the Kafka topic.

However, the mobile App uses the cloud database.

So the user profile information must also go to the cloud database.

So the user profile information must also go to the cloud database.

That's an operational requirement for the mobile application to work correctly.

The mobile application cannot query the Kafka topic for user information.

Kafka is not a database, so it does not support queries.

We send user profile updates to the Kafka topic, but we must also sync the same information to the database.

So your first requirement is to create an application that listens to the Kafka topic for profile CDC.

When a new profile or a profile update comes, you must flow it to the database and update the user profile.

Correct?

Great!

That's our first step toward the solution.

Now let's look at the login/logout data set.

We capture a login event in a Kafka topic when the user logs in to the fitness center.

Once logged in, the user might spend an hour in the gym.

And we again capture the logout event when the user leaves the fitness center.

The login and logout are two separate events.

We must match the login and logout events and create a single login/logout record.

The combined record should look like this.

This one is also an operational requirement for gym application.

The fitness center may have an application and a database to see who logged in and when they logged out.

And who is still inside?

So let's assume a database here, and we must update the login and logout events in the database table.

That's our second operational requirement.

Great!

So I talked about two requirements here.

However, both of these are operational requirements.

I mean, these things must work for the system to function on day to day basis.

These two requirements have nothing to do with the analytical or lakehouse implementation.

So we remove them from the lakehouse requirement and assume they are already done.

These two are not part of your project.

And a separate team will take care of it.

It is a good design practice to decouple operational workloads from the lakehouse.

Lakehouse is mainly designed to support analytical use cases;

operational use cases should be taken up separately.

Great!

So here we are.

This is the current state of our system,

and we will design a lakehouse platform and process these five input datasets for our analytical use cases.

I will see you in the following lecture and discuss the solution design.

Keep Learning and Keep Growing.
