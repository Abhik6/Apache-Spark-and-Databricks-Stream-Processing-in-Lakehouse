Welcome back.

In this lecture, we will write code to automate stream mode integration testing.

However, the stream mode testing approach differs from the batch mode testing.

Let me try to explain it.

Let me recall the step-by-step approach for testing your application in batch mode.

Here is the list of steps that we performed.

We start with a three-step initial setup.

In the initial setup, we cleaned the environment and then executed the run notebook to complete the setup.

Finally, we validated the setup.

Once the setup was done, we executed two iterations of payload testing.

Batch 1 produces the first payload, and then we run the notebook to process it and validate the results.

Then, we repeat the same with the second payload for the next batch.

You can keep repeating batches 3, 4, and so on.

However, executing two batches should be good enough to test your workflow.

But how do we do it in Streaming mode?

In the streaming mode, we must create a job and start it.

Once the job is started, we can produce two sets of payloads.

The steps for the streaming test should look like this.

The initial setup is different for streaming mode.

We must start with the cleanup.

That's obvious.

However, the next step is to create a workflow job to run the notebook.

And trigger this job.

Why?

Why do we need to create a job and trigger it?

Because streaming mode will trigger streaming jobs for bronze, silver, and gold layer processes.

Once started, they will keep on running.

That's how the real-time system will work.

We must start bronze, silver, and gold layer streaming jobs.

Once they are started, they will keep running.

And they will process data as it arrives.

So, a streaming test must be done simulating the real-life approach.

So, we will run the notebook only once in streaming mode for this scenario.

The notebook will trigger streaming queries for the bronze, silver, and gold layers.

And they will start waiting for new data and process them in microbatches.

So, we will start our testing iteration after the initial setup.

We will produce payload 1, wait for the streaming job to process the data, and we will validate the results.

That's all.

The first iteration of testing is complete.

Then, we will repeat the same in the next iteration with a second payload.

And that's how we can implement our streaming mode testing.

Let's write the code for this.

Go to the development environment.

Go to Workspace, Repos and choose your SBIT repository.

Now, you can add a new notebook here.

Rename the notebook.

We are now ready to code.

Let's start with defining our environment variable.

Good.

This notebook takes an environment name so we can run it in any environment.

The default value is the dev environment.

We also need to know the workspace URL and access token.

These two variables are required to programmatically create and trigger a workflow job.

We will be using Databricks Rest API to do the same.

And I will show you in a minute.

Let me extract these notebook parameters to Python variables.

Great!

What is next?

We will import the setup module and run the cleanup method.

Now run the cleanup.

Good.

What is next?

The next step is to programmatically create a notebook job.

Let me do it.

I will paste some code in the next cell.

This one is my Job Payload definition.

It is a JSON message to define a workflow job on a job cluster.

You must have learned to create workflow jobs using the Databricks Rest API.

Your Databricks course should have covered it.

And that's what I am doing here.

I am defining a job to start a job cluster and run my notebook.

The run notebook will trigger all three layers in streaming mode.

Once started, the streaming job will keep on running.

So, let's go to the next cell and write the Rest API code for creating a workflow job.

Great!

So I am using /api/2.1/jobs/create rest api to create a job.

Then, I am taking back the create_response.

Then, I am extracting the job_id from the response.

You must have already learned all of this in your Databricks course.

At this stage, I created the job.

But the job is not running.

I will write the Rest API code to run the job in the next cell.

Let's do it.

Good.

So this is the jobs/run-now rest API to start the job.

What is next?

A new job starts a new job cluster, and then it starts running.

Starting a job cluster may take up to five minutes.

We cannot run our test cases until the job is started.

What does it mean?

We must wait for the job to start.

Let me write some code to wait for the job till it is pending to start.

Good.

So, in this code, we are making calls to the jobs/runs/get API to get the job run status.

And we are waiting in a while loop until the job status is PENDING.

The while loop will exit when the job status changes to running from the pending.

After this stage, we are sure that my job is running.

So I can write code for testing my workflow.

How to do it?

Simple!

Produce the first payload and validate the results.

Then, we produce the second payload and validate the results again.

And that is all.

So, let me write code for producing and validating the first payload.

I have to import a bunch of notebooks.

Each import must be done in a separate cell.

So, let me import the history loader.

Then, import the producer.

Import the bronze layer.

Then, impost the silver layer package.

Then, import the gold layer.

That's all.

I am done importing.

Let me write code to load the data and run the validation.

So, I am waiting 2 minutes to ensure the setup and history load are complete.

The next step is to validate the setup and history load.

Finally, I am producing the first payload.

What is next?

Wait for another two minutes and validate the results for the first payload.

Correct?

Let me write code for the same.

So, I validated the first payload and also produced the second payload.

What is the next step?

Wait for another two minutes and validate the second payload.

Let's do that.

That's all.

What is next?

Cleanup.

Right?

Once we are done with validation, let's do some cleanup.

Let me write some cleanup code.

I started a job earlier.

Now, I am making another rest API call to cancel the job.

Why cancel?

Because I started this job for validation purposes.

Now that validation is over, so we cancel the job.

One last step.

Let me code that one, also.

This code deletes the canceled job.

It's not mandatory, but I do not want the job to be sitting there.

So, I am deleting the job.

And that is all.

That's how you can write a script to run an automated test for a streaming job.

One last thing. 

This is a learning example.

And keep it simple:

I haven't considered the exception handling in this notebook.

You may run everything in a try/catch block.

And make sure the cleanup is done even if your validation fails.

In the current code, if your validation fails, your job might keep running forever.

And we do not want that to happen.

So, ensure you implement exception handling in your test script and clean up everything.

Great!

You can run this notebook to test it locally.

Or you can call this notebook from your CI/CD pipeline to perform an automated test.

And that's what we are going to do in the following lecture.

In the following lecture, we will create our CI/CD pipeline to automate the build, deployment, and testing.

See you again in the following lecture.

Keep learning and Keep growing.
