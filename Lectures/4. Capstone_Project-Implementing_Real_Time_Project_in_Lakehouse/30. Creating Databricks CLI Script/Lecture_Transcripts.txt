Welcome Back.

In the earlier lecture, we created our deployment pipeline.

The deployment pipeline used a bash script for executing the integration test.

I promised to explain the bash script in a separate lecture.

Let me explain the bash script now.

I saved the bash scrip as a .sh file.

Let me open it in vs. code and explain it.

Here it is.

We created a bash script to run the following.

Create a job to run the batch-test notebook

Trigger the job and get the run id.

Wait for the job to complete while it is running

Delete the job after completion

Publish the job result

How do we do it?

The first four steps are done using the Databricks cli commands.

Now, let me show you the bash script and explain these steps.

The first line of the bash script is executing the databricks jobs create command.

The databricks is the cli.

The jobs are the module name, and the create is the cli command.

I assume you already learned the Databricks class in your Databricks course.

So we are executing Databricks jobs create command to create a workflow job.

The workflow job definition is given as a JSON input.

You can see the JSON here.

We want to create a notebook_task that runs the 08-batch-test notebook from the deployment folder.

For running this task, we need a job cluster.

So, I am also defining the job cluster in the same JSON.

Scroll down,

and you can see the cluster configuration.

We are creating a single node cluster using the Standard_DS3_v2 type of virtual machine.

And that's all about the first step of creating a workflow job.

In the end, I am using the jq command to extract the job ID from the response.

We will use this job ID later.

The next step is to run the workflow job that we just created.

Scroll down,

and you can see Databricks jobs run now.

So, the data bricks jobs run-now is the CLI command for running a workflow job.

I am passing on the job-id that we just created.

And also extracting the run id using the jq command.

The run ID will help me to query the job status.

And that's our third step.

I am running a while loop.

In the loop, I am executing Databricks runs get cli command.

This command returns the job status.

I will keep waiting in the while loop if the job status is RUNNING or PENDING.

Once the job is completed, I will come out of the whole loop.

My fourth step is to get the job run status and delete the job.

For deleting the job, I am executing the Databricks jobs delete command.

The rest of the code is Azure DevOps-specific code for publishing the result.

If the job fails, we want to log the error message and mark the task as failed.

And echo the complete response at the end.

That is all.

I hope it made sense to you.

If you are comfortable writing shell scripts, you can automate such tasks using the Databricks CLI.

However, you can also do this using Databricks Rest API.

You can write a Python script instead of this bash script and run the Python task from your pipeline.

The choice is yours.

Great!

That's all for this lecture.

See you again.

Keep Learning and Keep Growing.

