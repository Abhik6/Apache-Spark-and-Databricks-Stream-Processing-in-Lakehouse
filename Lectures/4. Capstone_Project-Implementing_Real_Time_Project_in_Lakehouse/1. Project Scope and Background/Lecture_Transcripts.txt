Welcome Back.

You learned a lot about the Databricks Platform and its capabilities.

Now it's time to implement all those things in a project.

So let's start with the project requirement.

Let's assume you are working with a wearable device manufacturing company.

Your company manufactures a wristwatch kind of device.

The end users can wear this device,

which is used to continuously send health parameters to your company servers.

You are assigned to design and develop a data engineering system to collect and process this data.

That's a high-level requirement and doesn't give us enough input to design a system.

Let's elaborate on it further and try understanding what data we capture.

A retailer sells the device to the customer and registers the device at the time of sale.

The device registration is nothing but creating a user id,

device id and linking the device's hardware address with the user.

Here is an example of the device registration data set.

So we create a user id

and capture the device id,

mac address,

and registration timestamp.

This data is stored in a database, and our database sits in the cloud platform.

Great!

So we learned about the first dataset.

We have user and device registration data captured in a cloud database.

Now let's talk about the second data set.

Your company also created a mobile app for the end users.

So a user can purchase the device, complete the device registration, and downloads the companion mobile App.

The mobile app is used to create a user profile, provide health reports and notify him with alerts.

So your second data set is the user profile data.

When a user creates his profile, we capture the profile information in a Kafka Topic.

And that's your second data set.

However, the user can also update his profile information.

So the second data set is a change data set.

Let's see some examples to understand it better.

Here are the three possibilities.

So let's assume the user is creating his profile for the first time.

The mobile app will create a new JSON record similar to the one shown here.

And send it to a Kafka topic.

Later, when the user updates his profile, we will create an update record and send it again to the same Kafka topic.

The updated record looks like the one shown here.

Similarly, when a user deletes his profile, we send a delete record to the same Kafka topic.

Here is an example of the profile delete event.

The point is straight.

The mobile app generates three types of CDC records and sends them to a single Kafka topic.

And that is our second data set.

Great!

So we learned about the two data sets.

Now let me talk about the third type of data.

The end user is wearing the device.

Correct?

The device keeps track of his pulse and sends a heart rate event to a Kafka topic.

So the device continuously sends a stream of heart rate events to a Kafka topic.

We call it a BPM event like the one shown here.

This one is our third data set.

It is simple but getting generated as a high-volume data stream.

Great!

We have two more data sets.

Let me explain them.

Your company has a partnership with various healthcare and fitness centers.

So the user can join one of the health or fitness centers.

You have a device identification scanner installed in these partner facilities.

So when a user enters the facility, you can detect his device and create a login event.

Similarly, you can also detect a logout event when a user leaves the facility.

So your fourth data set is a login and logout event sent to a Kafka topic.

Here is an example of the login and logout data set.

Make sense?

Great!

Now let's talk about the fifth and the last data set.

Your end user is entering the fitness center to do some workouts.

Right?

Your device comes with a push button to record the start and stop of a workout session.

What does it mean?

A user will press the start workout button before starting the exercise.

Once done exercising, he can press the stop button.

That's how we know he is doing a workout.

And we capture the workout session start and stop events in a separate Kafka topic.

Great!

So we learned about the five data sets, and I also talked about how and where we are capturing these data sets.

Let me quickly summarize it.

Device registration is the first data set that directly goes to a cloud database.

User profile and profile change events are the second data set.

We collect them in a Kafka topic.

We have a BPM stream which we collect in a Kafka topic.

The login and logout events also go to a separate Kafka topic.

Finally, we have a workout session start and stop events going to another Kafka topic.

I already showed you the samples of the five data sets.

Correct?

Now let's talk about the requirements.

We have the following requirements.

Design and implement a Lakehouse platform using a medallion architecture pattern.

Collect and ingest data from the source system to your platform.

Then, Prepare the following analysis data sets for the data consumers

Workout BPM Summary

and Gym Summary

It looks like a small three-point requirement, but we have much to do here.

You are asked to design a lakehouse platform and follow a medallion architecture.

Designing a lakehouse platform also includes implementing data governance,

such as audit control, security, data lineage, etc.

The following requirement is to ingest data from source systems

and design a complete data flow through medallion architecture's bronze, silver, and gold layers.

This requirement also requires you to design your data model and tables and implement necessary best practices.

The last step is to prepare two gold layers tables for your consumers.

This step requires you to design intermediate tables and data transformations and build your workflow pipeline.

Let me quickly show you the expected outcome for these two gold-layer tables.

Here is the workout BPM summary.

So, you will summarize workout sessions for each user daily

and calculate minimum, average, and maximum heart rates while he is working out.

The next one is the gym summary.

And this one is a summary data set for generating reports for the fitness centers.

This table summarizes the user movement in the fitness center and actual time spent exercising.

That's all we have for now.

But with these requirements, we also have some design goals and best practices.

Here they are.

Design a secure lakehouse platform with dev, test, and production environments

Decouple data ingestion from data processing

Support batch and streaming workflows

Automated integration testing

Automated deployment pipeline for test and production environments

Great!

That's all about project background and requirements.

The next step is to start talking about the design.

I leave you here to think through the design and see you in the following lecture.

Keep Learning and Keep Growing.

