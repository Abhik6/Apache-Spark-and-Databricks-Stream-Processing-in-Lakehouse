Welcome Back.

We coded our bronze layer process.

Now it's time to code our silver layer.

So before we start, let me show you the data flow diagram.

Here it is.

Just look at it

you know you have three bronze layer tables.

These three are your input tables for the silver layer.

Correct?

In the silver layer, we need to write functions that read data from the bronze layer and fill the silver layer tables.

So we need to code all the red lines in this diagram.

Correct?

Let me go to my dev environment and create a new notebook for the silver layer.

Go to Workspace

Repos and choose your SBIT repository.

Now you can add a new notebook here.

Rename the notebook.

Great!

We are now ready to code.

Let me import the config notebook.

I will paste some code for an Upserter class in the second cell.

Just leave this code here, and I will explain it later.

The next cell is to create a CDCUpserter class.

Let me paste it.

You can also leave this code here, I will explain it later when we use it.

Finally, let me paste the code for silver class.

This one is our silver layer code.

Let me quickly walk you through the code, and we will come back to learn it step by step later.

The __init__ method is a constructor for the class.

I am setting up some variables in this method.

After that, I have one function for each data flow line.

The upsert_users method will merge the data into the user's table.

Scroll down

and you will see upsert_gym_logs to merge data into gym_logs.

Let me jump back to the data flow diagram.

Look at this red line.

I am creating one silver layer function for each of the destinations.

I showed you the upsert_users method to merge data into the user's table.

Similarly, the upsert_gym_logs method represents the second red line and merges data into the gym logs table.

So the idea is simple.

We have eight silver layer destinations.

And we will be writing one function for each destination.

Correct?

Now let's go back to the notebook.

You already saw the upsert_users and upsert_gym_logs function.

Scroll down

and you will see upsert_user_profile,

upsert_workouts

upsert_heart_rate

upsert_user_bins

upsert_completed_workouts

and upsert_workout_bpm

So we have eight destinations and eight functions.

Apart from those eight, I also have an upsert method that calls all eight functions.

Finally, I have assert_count

and validate methods to unit test our code.

And that's all about the high-level code structure.

Now we can visit each function and see what we are doing in those functions.

So let's start with the upsert_users function.

Let me show you the diagram and try to understand the requirement.

So this first red line is for the upsert_users.

And the code brings the initial device/user registration data to the silver table.

We know the user cannot register again with the same device.

So we are not going to get duplicate records.

We also do not have a requirement to update the silver layer table.

It is an insert-only requirement.

Correct?

So the requirement is super simple.

Read all new records from the bronze layer table,

eliminate duplicates if a record is received twice,

then insert the data into the silver table.

In the next iteration, we should read only new records from the bronze table,

ignoring the records already processed in the previous iteration, and merge them to the users' tables.

How do you want to implement it: Batch or Streaming?

We already discussed the advantages of implementing a streaming approach.

This one also looks like incremental processing, where we should ignore the previously processed data

and read-only new records.

And tracking processed data in each iteration is super simple using stream processing checkpoints.

And we can anyway run a streaming code in batch or streaming modes.

So we have nothing to lose but only gain some benefits from implementing Spark streaming.

Correct?

Let's make a decision to implement it using Spark streaming.

Great!

But what are the steps?

It is as simple as a four-step process.

Read new data from the bronze table.

Remove duplicates, if any.

Transform the raw data according to the requirement and target table structure.

Merge the transformed data set to the silver table.

And that's standard for any bronze-to-silver layer table.

Step one and two will be the same for almost all the silver layer destination tables.

The change is required in steps two and three.

We have different logic for duplicate removal for different tables.

And we will have different transformation and data validation requirements.

However, the steps at the high level are standard.

We must implement a four-step approach in almost all silver layer functions.

Correct?

Now let's go to the upsert_users function and see how we implemented these four steps.

Here is the code.

The first step is to read only new data from the bronze layer table.

And I am doing that using the spark.readStream.

I am reading the registered_users_bz table.

Steps 2 and three are to remove duplicates and apply transformations.

We have a simple transformation requirement in this case.

And the selectExpr() is doing the required transformation.

Finally, I call the dropDuplicates using the "user_id" and "device_id" as unique keys.

That's all.

Steps one, two, and three are all taken care of.

I have df_delta as my target dataframe.

And the last step is to merge the df_delta to the target table.

But how do you merge a streaming data frame into a target table?

I hope you have already learned the approach in your Spark Streaming course.

We can do the writeStream and for each batch to pass on the dataframe to a user-defined function.

And that's what I am doing here.

I am doing writeStream and foreachBatch here.

And we pass the dataframe to the upsert method.

That's all.

Now the upsert method can do whatever it wants to do.

We want to run a MERGE statement and use this data frame as a source to merge into the target.

How do we do it?

Simple.

We defined the Upserter class at the top of the cell.

Let me show you.

The Upserter class takes two arguments: merge_query and temp_view_name.

The merge_query is the SQL statement for the MERGE,

and the temp_view_name is the name of the temp view we will create for the dataframe.

The upsert method receives our dataframe as df_micro_batch.

We can createOrReplaceTempView for the data frame.

Once we have the temp view, we can run a SQL expression on that view.

And that's what we are doing in the following line.

So let's return to the upsert_users method and see how we use it.

Here is the merge statement.

So I defined a merge query string.

We are merging into the user's table using the users_delta.

What is users_delta?

It is a temp view from the source data frame we received for each batch.

I am initializing the Upserter class with the merge query string and the temp view name in the following line.

So the class is initialized, and the MERGE query string and the temp view name are recorded.

Come down to the df_delta.writeStream.

Look at the for each batch.

Each batch will automatically pass the df_delta to the upsert method.

Now go back to the upsert method.

The df_micro_batch is your df_delta dataframe.

All we do in this method is convert the dataframe into a temp view and execute the merge statement.

And that is an approach for running your SQL expressions on a streaming data frame.

I hope you understand it.

Now let me quickly recap the upsert_users method.

Look at the code.

We are reading registered_users_bz using the readStream.

Removing duplicates and applying the transformation.

Then we want to merge this streaming data frame into the user's table,

so we use writeStream for each batch.

And the data_upserter.upsert method takes care of running the merge statement.

Simple! And this approach is almost standard.

We always use this design for upserting the streaming data from a source to the target.

The rest of the code is straightforward.

I am checking the function parameter.

If it is once, we trigger the code in batch mode.

Else, we trigger it in streaming mode.

That is all about loading data from source to target.

Now let's come back to the data flow diagram.

We have eight destinations of the silver layer.

And I am implementing data flow for each destination using a function.

I already explained the function of the first red line.

And I am using the same approach to develop all of these.

So I will not explain all of these lines and their functions.

Because most of these use the same approach, and you can read to understand the code.

The User Profile target is a little different.

We have some extra complications for this table.

But the rest of the data flows are straightforward and follow the same approach I explained in the user's table.

So I will leave all other data flows and explain the User Profile in the following lecture.

You should go through the code and try to understand every function.

You can come back to me and clarify your doubts.

But all of this is easy to understand with the knowledge you already gained in the course.

Great!

See you again.

Keep learning and Keep growing.
