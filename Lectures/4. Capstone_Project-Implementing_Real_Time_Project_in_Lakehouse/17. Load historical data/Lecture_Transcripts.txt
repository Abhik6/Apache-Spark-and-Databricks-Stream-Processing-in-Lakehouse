Welcome Back.

We created an initial setup of our project tables.

Now it's time to move to the next step.

What is our next step?

In a typical project, you may need a one-time data load.

For example, you may have some historical data.

You may also have some fixed reporting lookup data.

You want to load these data sets in your platform before you go live.

And such data sets are loaded only once.

The point is straight.

We need to create one module for loading historical data.

So let's code it.

Go to your Databricks development environment.

Here we are.

Go to Workspace

Repos

and select your SBIT Repository.

Let me add a new notebook.

Change the name of the notebook.

Now we are ready to write code for the history loader.

Go to the first cell and import the config.

Now, I can write the code for loading historical and lookup data in the second cell.

I already have the code, so let me paste it here.

Great!

It is a small code.

Let me explain it.

I am creating HistoryLoader class to wrap all my historical data load functions.

The _init_ method is the constructor, and we are setting up some variables in this function.

Then I have written a load_date_lookup function.

This function fills the date_lookup table in the silver layer.

The date_lookup is a calendar dimension for our company.

It is pre-prepared 20-year dates dimension according to our company's financial calendar.

And that's typical.

Most companies would have their customized date dimension.

So I am loading data from a JSON file into a dimension table.

The data is already present in the cloud directory.

I load it from the file and bring it to a dimension table.

That is all.

We have only one lookup dimension.

Now comes the historical data.

I do not have any historical data.

So I am not writing any function for loading historical data set.

However, most of the projects will have some historical data, which you need to migrate and load to your project.

Let's assume your historical data is available on your company servers.

How do you migrate them to your project?

The approach is simple, and it is done in 2 steps.

The first step is to ingest the data from your on-premise storage servers to the cloud storage.

For example, you can create an Azure Data Factory pipeline to ingest data from your servers and bring it to ADLS.

That's all.

Your data came to the cloud.

The second step is to write the history loader function in this notebook.

I can write one function for each historical data table.

The code for the function will be as simple as reading from the ADLS directory

and inserting/overwriting into a delta table.

That's all.

We are doing the same for date lookup.

I am reading from ADLS and inserting it into the delta table.

The point is straight.

Plan and design an approach for your one-time data loads.

Historical and lookup data are the most commonly used one-time load candidates.

Great!

Now let's come back to the code.

The code here is super simple.

I have only one data set to load, so I created a load_date_lookup function.

The load_history function is simply calling the load_date_lookup.

I have assert_count and validate functions to unit test the history loader.

The assert_count is a generic function to validate the record count in a table.

It takes two arguments: table name and expected count.

And the function will count the number of records in the table and match it with the expected count.

And raise an exception if the table count differs from the expected count.

The validate function is to validate all historical data sets that we loaded.

And that is all.

Great!

So you learned how to handle historical data for your project.

You can go back to your scratchpad and test the history loader class.

I'll leave it to you for now.

Go ahead and test it.

And I will show you the complete testing approach in later lectures.

See you again.

Keep Learning and Keep Growing.
