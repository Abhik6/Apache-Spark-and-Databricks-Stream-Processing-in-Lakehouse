Welcome back.

In this lecture we will learn how to connect with the Kafka cluster using Spark Structured Streaming

Kafka Connector.

And we will learn some best practices and how to write code using Kafka Connector and interact with

the Kafka.

So let's start.

Let me quickly walk you through the requirement.

So you might have seen this requirement earlier.

Also we have source systems.

They are sending data to a central place.

And this time we assume that central place is none other than Kafka.

So we are collecting data from various sources in a Kafka topic.

And what we want to do, we want to ingest that data into our data engineering platform.

The first layer in our data engineering platform is our raw table or bronze layer table.

So what do we want to do in this example is write a small job which connects to the Kafka cluster,

right.

And starts pulling data from the Kafka cluster and save it into a target table, which is our bronze

layer table.

So simple right?

So let's try to code it.

Let's try to write an application for this.

So let me go to my workspace and create a new notebook.

Let's call it Kafka to Bronze.

And in this notebook, we want to create a application which ingests data from Kafka and saves the result

into the bronze layer table.

So let's define a class for doing that.

And one constructor.

Okay, so what do we want to do inside the constructor.

We want to define a few things.

So first thing like every other application we are defining a base directory location.

But what else do we need for connecting to Kafka cluster?

We already learned that we need some kind of configurations, right?

We need connection details.

Your Kafka host name and we need username, credential, all that we need.

Right.

So have defined that in the earlier example while writing the batch code.

Here it is.

So let me copy all this and define it inside my.

Constructor.

And once this is ready, we are ready to writing methods for it.

So what is the first method?

Do we want to write?

Go straight.

The objective is to connect to Kafka and start reading data from the Kafka topic.

Right?

So let's write one method for reading data from Kafka.

Let's call it ingest from Kafka.

Right.

So what do we want to do in this method?

Simple.

We want to connect to Kafka using and read data from Kafka using spark dot read stream.

Right.

So I already have a code here which we created in the previous example.

We'll copy it.

Change it to read stream from read to read stream.

That's all your batch processing code becomes your stream processing code.

And we want to return this data frame from here.

Let's do a little formatting.

And that is all we are done with the ingest from Kafka.

But that's not all.

Let me take it as an opportunity to introduce you to best practices here.

So we know that we want to connect to Kafka cluster and read data from Kafka cluster using read Stream.

Read stream will continuously read data from Kafka cluster from the topic as Microbatches, right?

So it will go connect to the topic.

Try to pull some records, but in next micro batch it will again go and try to pull some more records

from the Kafka topic.

Right?

But how many records?

We don't know that, right?

So sometimes it is possible that one micro batch is pulling 10,000 records.

Another micro batch, next micro batch you have very little data.

So the other guy is consuming only maybe 500 records.

Right.

That creates an imbalance between consecutive micro batches.

Right?

One micro batch is processing too much data.

It's taking a lot of time.

Another next micro batch is receiving a small amount of data and it's finishing quickly.

All that is like not in our control, right.

So what do we want to do?

Most of the time we want to be in control.

We want to make sure that each micro batch receives a uniform amount of data.

So how do we control that?

We already learned that for a landing zone directory we can configure the max file per trigger.

Similarly for Kafka topic you can configure max offsets per trigger.

So let me do that.

And that's all Max offsets per trigger.

What value do we want to do?

Let's give ten for now.

It's just an example.

So ten is a very small number but that's fine.

So that's one best practice.

You should try to make sure each iteration or each micro batch.

Is uniform, or at least has a maximum limit of how many records you want to pull into one micro-batch

so that micro-batch doesn't run very long.

Right.

We want to avoid that.

So that's one best practice.

Second best practice is we need to be able to specify a start timestamp.

We need a capability in our application that we start from 1st January.

10 a.m. in the morning or say, start from 11th January, 10 a.m. in the morning.

So whatever time we want to restart our job, go back in history and restart from some point in time,

we should be able to do that.

How to do that?

So that's one more best practice or a capability that you should write in your all spark structured

streaming applications that should be there almost always.

Right.

So what is that configuration.

That configuration is starting.

Timestamp.

Simple, right?

Just pass one configuration as a starting time is ten.

And when you start again, you can make use of this starting timestamp to let your application know

from where in the past we want to restart our job.

So what value do we want to pass?

We want to parameterize that value, right?

We cannot hard code these kind of values.

So let me paste something here.

The starting time equal to one.

That's a default value.

I'm giving one.

This timestamp is starting.

Timestamp must be a long number.

When we pass one or the default value is one, it will start from the very beginning.

So we set that default value and that should be enough for ingest from Kafka method.

We are done with the ingest from Kafka method right.

What is next.

So let's write one small transformation method here which will take this data frame which we are collecting

from Kafka and convert key and value into a meaningful string.

So let's let's write a method for that.

And then we will write it to table.

So let's call this get.

Right.

So what do we want to do in the get invoices?

Type in, so return.

And we will write our code here.

And we will start from Kafka.

And it and maybe select select and then which field we want to transform.

Kafka dot key.

The first is key which we want to transform into a meaningful value.

Simply cast it.

We know.

Kafka is giving us an array of bytes, but we sent a string there, right?

So we cast it to a string.

That's all.

Cast it as a string and maybe give an alias.

Also, let's call it key again.

So that's all for the key.

We take Kafka dot key, cast it as a string and call it key.

What is next?

We want to take Kafka df dot value.

Write a cast that also is string.

Right?

And that should be enough.

It will be converted into a string and we are sending invoice in a Json format.

So we will get a Json string out of it, right?

But what do we want to do.

We want to parse the Json properly and create all the fields right.

Invoice ID or invoice number.

Date line items.

All those we want to transform back to a proper schema.

So for that spark gives you a function for parsing a Json, a Json string.

You can parse a Json string and create a proper dataframe out of it.

The function name is from Json.

So okay, so from Json takes a Json string right.

So we have Kafka df dot value.

We cast it as a string.

So we will get Json string.

And we want to give a schema for that Json string so that we can read it as a proper columns.

Right.

So how do we give the schema.

For now let's call it schema variable.

We will try to get some schema later.

So what else?

Let's call it.

Well, we keep it still.

Keep it as value.

What else do we want?

Kafka data frame, which we read from Kafka, comes with a lot of fields like key value.

And then we get the topic name, we get offset, we get timestamp, we get timestamp type.

We don't want to keep everything in my raw table, but I want to keep the topic name.

And also time stamped.

Right?

Kind of.

That's all we wanted to do in this, uh, get invoices function.

Right.

But we still need to fix this schema.

And we are using from, uh, Json.

So let me import it.

I'll paste code here from PySpark dot SQL dot functions import import what from Json function.

Right.

Let's import it.

And that's all we are left with some way to specify the schema that we can copy from an earlier example.

We used the same here and we defined a get schema method there, right.

This is the same invoice schema that we are using here.

So instead of typing it again and again I'll just copy it in this class.

So what I have get is schema.

So instead of giving a schema here I'll call get schema.

We will get the schema and it should be self dot right.

In the get invoices method, we will pass the raw Kafka data frame that we are ingesting using the ingest

from Kafka.

Right.

And that raw data frame will start that raw data frame and will take three columns from there.

Key value for topic and timestamp will take only four columns.

We'll leave all other columns.

What is next?

The next step is to create your final process method, right, which we can call from outside and process

or execute all of this.

So let me define a process method.

So do we need any additional parameter in the process method?

Yes, we do need.

Basically we are passing a start time here.

The starting time.

And that should come from outside.

So let's keep that variable here also as a parameter right.

So we can get the start time from somewhere else.

And then.

Okay, so what I'm doing is starting bronze stream, just printing a message.

And what do we want to do?

First step in the process.

Right.

Call this ingest from Kafka.

Correct.

And get.

Kafka df equal to self dot ingest from Kafka and pass this starting time as time whatever we received

right?

And that's all we got our raw data taken out from a Kafka topic.

What is next?

We want to create.

Invoices PDF.

And this one we will get calling this guy right?

And pass the Kafka to this guy so it can give us the invoices.

And this is now ready to go into our target table.

So how do you write it to a target table?

We will take a streaming query as an output.

And then we will write our.

NY says DF dot.

Write stream code here, right?

So what all do we want to do in the write stream?

Here it is.

So we want to give a query name.

Let's call it branch ingestion.

We want to set up a checkpoint location so that we can restart from the same point.

If somehow our job stops or for any reason, we have to stop our job and restart it.

Output mode.

We want to append only whatever records are coming.

We want to insert it in the target table.

Next micro will take new records, insert it into target table.

So our mode is append and to table.

Where do we want to set and send it.

Invoices underscore z.

So that's our target table.

And that is all.

After that maybe we print the complete statement.

And return the query right?

So print done and return the query.

And that is all.

So it's a simple application.

We defined a bronze layer class called it bronze.

In the constructor we defined all the necessary parameters that are required to connect to your Kafka

cluster.

And then we have one method to ingest from Kafka, right.

Which will go connect to the Kafka and ingest data from Kafka.

Then we have a get schema which will give us the schema for the invoice.

And then we have get invoices which will take the rock after data frame.

And.

Process it to make it a meaningful.

Invoices.

DataFrame.

And finally we have a process method which will stitch all things together.

And that's all.

That's all we wanted to do in this example.

Now what is next?

The next step is to write a test suit for this and start running it and testing it.

Right.

So let's do that.

Great.

So.

Let me create a new notebook.

Call it Kafka to Brown's test suit.

Let me close these guys.

We don't need these.

And open our Kafka to.

Bronze.

In a separate.

First thing first, let's import this into.

Import it into our test suite notebook.

And then we want to write test suite.

So.

I'm assuming that you have been writing test suit for quite some time now, and if I copy paste test

suit here and then explain, it should be fine to you.

Right.

So.

Let me copy paste.

And a few things and explain.

There is nothing new here.

We have been writing this code earlier also.

So what I'm doing here, I'm defining a class called it Kafka to Bronze Test Suit.

And in the constructor I'm defining our base directory and in the clean tests, which is usually required

in all test suit to create a clean environment before we start running our tests.

I'm dropping our invoices table, which is my target table here.

Right.

So I'm dropping that and also deleting the invoices table directory.

And then I'm also deleting the checkpoint directory for this job.

Right.

This is our checkpoint directory that we defined all that I'm cleaning.

And that's all assert result is also same.

We take expected count in the assert result and we take a select count star from our target table.

Invoices.

Collect it first field sorry first row first field and bring it into the actual count.

So we get total number of records found in the invoices table.

We assert it comparing expected equal to actual count.

And that's all.

If test pass, great.

If not, we print the error message.

And we also have one method called wait for microbatch so that we can wait whatever time we want to

wait to make sure that each microbatch, after reading data from Kafka, is able to process it successfully

and save the results into the target table before we start validating our results from the target table.

So we want to wait for few seconds.

Default is 30s, right?

So that's all.

Now what is next?

The next step is to write the run tests method.

So let me paste some starting code for the run tests and rest will try to.

Type, so usual before starting to run our test.

We call the clean tests so that environment is clean.

And then we create an instance of our bronze class.

Right.

This is our application bronze class.

So we define the instance of the bronze class.

And then what do we want to do.

I want to implement three scenarios right.

So let me paste some print message for the first scenario.

And then I will explain what is the first scenario.

So the first test scenario is let's assume we want to start our streaming job for the first time.

Right.

You have built the application.

You are deploying it in the production environment and you want to start it right.

And when you start it for the first time, it should start from the beginning.

Whatever data is already there in the Kafka cluster in the topic, it should take all from the beginning.

Nothing should be left behind, right?

Read it from the very beginning, from the start.

So that's the first scenario.

So how do we code that scenario?

So what I'm doing, I'm taking this bronze stream object that I created, which is my application and

calling the process method.

Here is my process method.

So this process method will go and start processing everything from scratch right.

Just call the process method.

Take the query right and wait for maybe 30s.

So that first set of data, first Micro-batch or first few micro batches are able to pull data from

the Kafka topic and save it into our bronze layer, and then we stop the query.

You want to stop the query immediately after waiting for 30s, and then we go and look into our bronze

layer table.

How many records are ingested?

I have already prepared my Kafka topic in a way that there are 30 records already there in the Kafka

topic, so I expect that.

When we start the job, the streaming ingestion, wait for 30 minutes and then stop.

In all that duration, we should have ingested 30 records because only 30 records are there in the Kafka

topic.

I'm not sending any new records, so whatever is there will be ingested.

So that's my first test case if that validation passed.

Great.

We are done with that.

One more thing I want you to notice here that while calling the process method, I'm not passing any

start time, right.

The process method takes a start time as an input, and that start time is used to configure this read

stream tells from where from what time stamp we want to ingest.

Default value is one, so I'm not passing anything.

So the default value will be one right.

So it will basically start from the beginning.

That's what I wanted.

And that's what we are doing.

So that's my first scenario.

We have a second scenario.

Let me paste code for testing the second scenario.

And then I'll explain that also.

So here is the second scenario.

Scenario restart from where it stopped on the same checkpoint.

So we started our query consumed 30 records because there were 30 records we stopped that query.

Now what do we want to do.

We want to start it again and then see if it consumes the same records once again.

Will it consume same record once again?

No, because we have a checkpoint there where this streaming query would have noted down that how many

offsets I have already consumed from a Kafka topic.

So all those offsets that are already consumed will not be consumed again.

Right?

Because all that details is are saved in the checkpoint location.

And that's how Spark Streaming implements iterative processing or incremental processing.

Right?

We cannot process same data again and again, even if we stop the job and restart it.

So that's what we want to validate here in case of Kafka.

Right.

So job was stopped earlier.

I want to restart it and then wait for 30s to see if it consumed something more.

What do you expect?

Will it consume any new records?

No, because we are not sending any new records to the Kafka topic.

And all those 30 records were already consumed or processed in the previous microbatches, so I still

expect only 30 records in my target table.

So if that is the case, we pass the validation, right?

That's all.

So that's my second use case or scenario.

I have a third scenario also.

What is that third scenario.

Let me paste here and then I will explain.

The third scenario is to restart from a known point in the past.

Right.

So what do I want to do out of those 30 records, 20 records were sent on 20th October 2023, and ten

records were sent on 21st October 2023.

I have purposefully created that scenario where out of those 30 records which are sitting in my Kafka

topic, 20 records were sent on 20th October and ten records were sent on the 21st October.

So what I'm doing, I'm taking a timestamp of 21st October, converting it into milliseconds.

That's the number here.

And this number I want to pass in the process method so that it starts from the 21st October.

Think of it as a 21st October date time stamp converted into a long number which is milliseconds elapsed

time or epoch time.

So we pass that and we want to start it from here.

But.

No matter whatever time we pass, it will always start from where it is stopped.

And if I want my spark streaming job to start from the given time, then I have to clean my checkpoint

because checkpoint is the place where spark is streaming has stored what I have already processed.

As long as the same checkpoint is there, right?

As long as your process is starting with the same checkpoint, it will not even consider your timestamp

or start time or whatever you give, right?

So if we want to restart from a given point of time, for example, in our case I want to restart from

21st October so I can pass 21st October date here, but I have to clean my checkpoint and give a clean,

empty blank checkpoint for spark streaming.

So what is Spark Streaming will do?

It will look into the checkpoint.

Nothing is there, right?

Checkpoint is blank.

Nothing is there.

So it doesn't know where to start.

Then it will look okay.

This is the starting time stamp given to me.

21st October is 6 a.m. in the morning, right?

So let us let's start from the 21st October, 6 a.m. in the morning, and then it will start from 21st

October.

Trigger a micro batch.

Go try to pull records from Kafka Topic, which were sent on 21st October.

And in my scenario I have ten records there, so we expect ten more records to be captured and then

in the target count will be 40, right?

30 were consumed earlier, ten more should be 40.

So that's the scenario I've set up.

So as a result 40 if it validates we are good right.

So that's all we are done with that.

Uh do you want to run it.

Uh, let me go to a compute cluster and okay, let's start a new cluster.

Let's delete it.

Let's create a new compute cluster.

Call it my cluster.

And this time I want to make sure I'm choosing spark 3.5.0 and create a cluster.

So it may take a minute, so I'll pause the video and come back when my cluster is ready.

Great.

So my cluster is up now.

And we already learned that for executing Kafka and Spark interaction, uh, you need to install a Kafka

connector for Spark to Kafka connector in your cluster.

So let me go there.

And paste the coordinates for the connector.

Hit the install button and it will get installed and once it is installed, we are ready to our.

Run our test case.

So let me paste a few lines of code at the bottom so we can execute our test case.

Right.

So what I'm doing is creating an instance of my Kafka to bronze test suit and calling the run test.

Okay, so library is installed here.

Now let's hit the run test.

So looks like first scenario passed and we are running the second scenario.

And second scenario is to start from where we stopped, right?

So in that scenario, we are not consuming any new data because everything was already consumed.

So it looks like second scenario also passed.

Let's wait for the third scenario.

And in third scenario, we want to start consuming again from 21st October because this is sometime

in the 21st October.

And.

Great.

So that scenario also passed.

And that's all for this lecture.

I hope you learned something.

See you again.

Keep learning and keep growing.
