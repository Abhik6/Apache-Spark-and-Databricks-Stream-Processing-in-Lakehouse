Welcome back.

In this lecture, we will learn to create a Kafka sink using Spark Structured Streaming.

So let's start.

Before we start coding, let me give you a quick introduction to the requirement.

So what do we want to do?

We want to create a Kafka producer which reads data from a data file.

And sends each record or each line in this data file as a Kafka message.

So we convert that line into a key value pair and send it to the Kafka cluster to a Kafka topic.

We already created similar kind of Kafka producer earlier, but that was a pure Python Kafka producer,

right?

But we are learning spark structured streaming.

So how do we use Spark Structured Streaming to send data to Kafka cluster or to a Kafka topic?

How do we write our results or data to a Kafka topic?

That's what we want to learn in this lecture.

So let's start coding.

Sir, let me create a new notebook.

And.

We will start coding.

So let's give a name to this node.

Let's call it Kafka Producer.

So let's define a class for this.

What is the first step?

Define a constructor and set some variables to hold configurations like your.

What is your base directory and what is the Kafka connection details and all that.

We have already done that in the earlier lecture, so I will keep on copying some part of the code which

is already done in the earlier lectures.

Hope you don't mind that.

So here is my.

Constructor.

I'm copying some configuration from the earlier lecture.

We have a base directory.

We have a Kafka bootstrap server module, cluster API key and cluster API secret.

What is the first step?

We want to read data file from a landing zone.

So basically we want to read.

Invoice file.

It's a Json data file.

We want to read that invoice file from a landing zone directory and load it into a data frame.

And then we will use that data frame to send data to Kafka.

So in the earlier lectures I have already created read invoices method to read data from a landing zone

using spark structured streaming.

So let me copy that method here.

So here is my read invoices method.

It takes one condition.

I've modified it slightly.

It takes a condition as an input parameter.

And then we are using a spark dot read stream format is Json because my data file is Json and I'm using

get schema, which we will define at the top.

Actually, we will simply copy it from an earlier example.

So this is the get schema method.

And we will use get schema to load the Json data file with a proper explicitly defined schema.

So and then we will load it from the landing zone.

My landing zone is data slash invoices.

So we are reading data from there.

Plus we are also putting a Where clause.

After reading the data as a stream, we are putting a Where clause and where clause condition will be

passed as an input parameter.

So I don't want to read a file and send the entire file to Kafka.

Right?

All records.

We want to have a little flexibility that we read a data file from the landing zone.

We create a data frame, but we don't want to send everything there.

We want to have a flexibility in our application that tell me some where condition dynamically, and

we will filter out all the records, take out only some records based on the provided filter condition,

and send those records only.

That's an added flexibility we are adding.

So.

What is the next step?

We know for sending data to Kafka.

We must create a message.

Right?

And the message is made up of three things key, value and time stamp.

So we cannot send a data frame directly to Kafka.

We'll have to transform a data frame into at least two fields key field and value field.

Right.

Timestamp.

We can leave because at the time of sending, my connector will automatically add a time stamp, which

is good.

So we must have a key and a value.

So next step for us is to take the whatever data frame read invoices is creating after reading data

from the landing zone and transform it into a key value pair, each record in the data frame should

be transformed into a key value pair, so that all the records are in the Kafka message format, key

and value, and we can send all the records directly, right?

So let's define a function for that.

Let's call it get Kafka message.

And it takes one input data frame and I want to make it a little generic.

So let me.

Pass a key field name also.

I'll write code and you will understand it.

So we want to return a data frame from here.

And we want to transform this DF that we received and transform it into key value pair.

So let me use select expr and we want to create only two fields.

One field is key.

Another field is value.

So let's write a field for key.

So what do you want to do.

We want to take a.

Whatever key is passed.

For our example, we will be passing a store ID, so we want to take a store ID and make it as key.

So how do I code that?

A generic way.

Select a expr store id as key.

That's all right.

And store id will be the value passed to the key parameter.

So finally if I pass a store id here it will become select expr first field store id as key.

If I pass invoice number here it will become invoice number as key.

So that's all we are done with the key.

What about the value.

So value.

What do we want to take as a value.

So what I want everything.

Everything that we have in the data frame we have maybe 1015 fields there.

Some are complex fields.

Some are simple fields like invoice line item is a complex field right?

I want to wrap everything into a single object and call it a view.

So how do we do that for that?

Spark gives you a struct function.

So we take a struct put a star.

And what it will do.

It will take all the fields starting from the first field to last field in the data frame in this DF,

and wrap it inside a struct and make it a single column with a struct type.

So it will become a record.

Embedded record kind of thing.

Right.

And call it maybe as value.

Right.

That's all struct star as value.

So everything in the data frame, all the fields wrap it into a single struct, call it value and we

are ready.

We are ready with key and value two fields.

Right.

But I know my data set is a Json record.

So in my invoices file every line is a Json, right?

So it's better to cast it as a Json, right?

Because my data is also coming as a Json.

So better to cast it as a Json and then send send it to Kafka.

So what I can do I can use to Json.

Function here.

And so what I'm doing is struck a star.

Wrap everything into a struct, convert it into a proper Json.

Record and.

A given as value.

And that's all.

That's all.

So we are done.

Right.

So whatever data frame we are creating here will be returned from here.

So get Kafka message will return me a data frame of key and value pairs.

Right.

And now once that is there, we are ready to send it to Kafka.

So let's define a method called Send to Kafka.

And what do we want to pass there?

One data frame and same data frame.

We will send it to Kafka.

So let's call it Kafka DF right.

And we.

Let's start with the.

Kafka DF Dot.

Write a stream and I'll fill in the other details.

But we also want to return the streaming query, right?

Write a stream creates a streaming query.

So we want to return the streaming query handle.

So let's write a return.

And.

We will fill in the details of right stream inside this parenthesis.

Right.

So what details do we want to fill in?

The first thing we want to tell right.

Stream that.

What is the connector.

What is the sync connector.

We want to use.

And we want to send data to a Kafka topic.

So sync connector is Kafka.

So format Kafka.

That's all.

And then since we want to send to Kafka, we need to provide some details for the connection.

Right.

How?

Right.

A stream will connect to the Kafka cluster.

So let me paste those details.

These are the same details.

That we used while reading data from Kafka.

Because connection details are same.

You are connecting to Kafka cluster for reading or you are connecting to Kafka cluster for writing.

In both ways.

In both these scenarios, you have to provide details for connection, right?

Same things.

Where is your Kafka server?

What protocol and what mechanism you want to use and what is your credential using config.

Right.

So that's all we are done.

And then we have to tell which topic do we want to send this data to.

Right.

So next option will be.

Topic name for reading data from Kafka cluster.

You define subscribe, right?

You have to tell which topic do you want to subscribe.

So the option name is subscribe.

And while writing it data to the Kafka, the option name is topic.

There is a reason you can send data to only one topic.

At a time, right?

So one right stream can write to only one topic.

And that's why the option name is topic.

And you will tell topic name here.

But read stream can read data from multiple topics at a time.

So option name there is subscribe and in the subscribe option you can provide comma separated list of

topic names and you can start reading from multiple topics.

So that's why the names are different.

The option names are different for read stream and write stream.

So what is next?

We must provide a checkpoint location and output mode.

So let me paste that checkpoint location.

Is this Kafka producer inside our checkpoint directory output mode is append.

And then what action should we give all this that we are giving from here to here are configurations,

right?

We are using a spark write stream and we are configuring the connector.

And all this is option.

We are configuring the connector with various options.

And we are telling what should be your query output mode.

That's all.

There is no action yet.

So let's add an action.

But what action do we write.

Start right.

So there are only few actions defined with the right stream.

One of them is to table right and that is shortcut action is specifically designed to write data to

a delta table.

So you can use to table if you are writing to delta table.

But for any other sink or any other kind of target, you have to use action as start.

That's all.

So that's all we are done with this send to Kafka.

What is next?

The last step is to write our process method right, which will stitch everything together and self.

Do we need any condition or parameter here?

Yes, we are using one parameter.

So we should add that parameter to the process method and pass the same while calling the read invoices.

So what do we want to do in the process method?

Let me copy paste.

Print statement at least.

Right.

So starting Kafka producer stream.

Let's print that.

And what do we want to do first?

We want to first call the read invoices and create a data frame out of it.

Right.

So let's call invoices DF and.

Should put self here and then read invoices and read invoices takes the condition right?

Let's pass the condition there.

What is next?

We want to create Kafka out of the invoices DF.

And for that, we define a get Kafka message.

So self dot get Kafka message.

And in this we will pass the invoices DF.

That's all we are done.

We got the Kafka DF.

What is next?

We want to create the write stream query or the streaming query.

Create a variable to hold the query, and then we will call the send to Kafka method there.

Right.

And we will pass the Kafka DF.

Make sure your Kafka DF is made up of only two fields key and value.

Otherwise you will see an error.

You cannot send arbitrary data frame to a Kafka.

Kafka data frame must be made up of key and value, because Kafka expects only key and value two fields,

so you can wrap everything into the value whatever data you want to send, wrap it in the value key.

You can send simple keys, string keys.

You can also create a complex keys so you can wrap everything that you want to send in the key and send

it to Kafka.

We will see some complex key examples later.

So that's all.

Then we put the print statement and return it.

Let me connect to my cluster and run it once.

So if there are any indentation and all that kind of error, I get, it worked.

So we are done with the code.

We are done writing our Kafka producer.

Now it's time to test it.

So let's write a.

Test suit for that.

So let me go to my workspace and create a.

No notebook.

And side by side.

Let me open my main application notebook.

So let's call it Kafka producer test suit.

What do we want to do?

We want to.

Import our application.

That's the first thing we should be doing in the test.

So and then we write a test suit class.

So let me copy a few things because a lot of things we have already done in earlier examples.

So to save some time uh, I'll copy paste few things.

So Kafka producer test suit is the test suit class name.

In the constructor we are defining the base directory.

Nothing new here.

And let me copy the.

Clean tests.

Also, for every test suit we keep on writing clean tests and what all I'm doing as a cleaning.

So starting cleaning, I'm deleting Kafka producer checkpoint.

That's what we created here in the write stream.

And I'm deleting, uh, invoices, data, invoices, landing zone directory and data invoices.

Recreating data invoices, landing zone directory.

This one we are using while reading data at the beginning.

Right.

So this is our landing zone directory from where we will be running read stream to read our invoices

file.

So I'm cleaning those things and that's all done.

Next thing.

What is the next thing?

Next thing is to write a method to ingest data.

Right?

So we have already done these things in the beginning in the earlier lectures.

So let me copy paste that code also.

In just data method, which takes iteration number as an input and then we copy.

Our data file from our test directory to our data directory.

So.

We are done with the ingest data, we will call it three times because most of the scenarios we want

to test three iterations.

So we'll call it three times.

Every time we will pass the iteration number one, two and then three.

What is next?

Few more methods we keep on using like weight.

So let me paste that code.

So wait for Microbatches.

That's also standard.

We keep on using that I copied it.

What is next?

The next step is to write our assert function.

Right.

We want to assert.

And what do we do in the assert method.

We want to compare the counts.

We want to keep it simple.

We don't want to compare the entire result record.

We just want to compare the counts.

But.

What is my target?

My target is Kafka topic.

So I'm reading data from a source file and sending it to Kafka topic.

So we have to read Kafka topic and get the actual count from the Kafka topic, right?

And then we can compare it with our expected count.

So let's write the assert method.

I'll copy a few things.

So you have learned all those techniques.

So it should be easy.

So method name is assert Kafka.

And we want to pass two parameters here.

Usually in the assert method we pass only single parameter expected count.

But I'm passing a start time and expected count.

Why.

Start time.

Because we want to start reading Kafka from a given point of time.

Because it's a Kafka topic, right?

And we may have a lot of data in the Kafka topic.

We may want to run our test case multiple times.

Every time we don't want to read.

All data.

Right?

So we will put a timestamp when we are starting our test suit our test scenario.

Right.

We will put a timestamp there.

And every time we want to assert we will read data from Kafka topic starting from that time stamp.

Right.

So that we read only those data that we are sending as part of this test suit.

So let's write code for reading data from Kafka topic.

You already learned that how to read data from a Kafka topic, but we are not going to start a streaming

query for test case for assertion.

We just want to read it once, take all the data, and then count it to get our expected count.

So even batch code or batch query is good enough.

So I'll use spark dot read.

I will not use spark dot read stream.

So let's do that.

So let me copy paste some code here.

Um.

I am creating a Kafka producer instance here.

What is that?

Kafka producer.

This is the code that we have written here.

Kafka producer class.

Right.

So I'm creating an instance of the Kafka producer.

It is not required.

It's not mandatory to create an instance of the Kafka producer in assert method, but we will be connecting

to Kafka, reading data from Kafka, and for connecting.

We have already defined all these connection details here in the Kafka producer, so I just want to

use these from the Kafka producer.

So that's why I'm creating it.

Here.

Let me paste.

Code for reading data from Kafka.

That's it.

So I'm calculating actual count and how I'm getting the count.

Spark dot read format.

Kafka.

So I'm connecting to Kafka cluster and passing all these configurations.

And for that I'm using PD dot bootstrap server.

And that's why I created a Kafka producer instance here PD.

PD is an instance of this class, and from this class bootstrap server config and key and secret.

All that I'm reusing here.

Right.

So all that and then we want to subscribe to invoices topic because this is sending data to invoices

topic.

And we want to read it from a specific point in time.

Right.

We don't want to read all the data.

Maybe I'm running this test suit three times.

So three iterations each time, right?

So a lot of data will be there.

But each execution should read only data that is sent during this test.

So we are reading from a start time.

And then one more configuration I want you to learn here starting offset by time stamp strategy.

So whenever you are using starting time stamp.

In your read or read stream from Kafka.

It is possible that Kafka doesn't find any record.

The starting.

After the given time is done, it is possible and Kafka topic is partitioned, right?

So it is most likely it is possible that you don't get at least.

In sum, one partition.

Out of those 1020 topic partitions, it is very likely that at least one of those partitions doesn't

have any record which is starting after the given time stamp.

So in that case, by default, Kafka or your connector will throw an exception that no record found

is starting from the given time stamp.

Write no record found which is older than the given time stamp.

That will throw an error even if a single partition doesn't have a record which is received or created

after the given time stamp, it will throw an error.

That's the default behavior.

We don't want that, right?

We want to set it to latest.

And this is the configuration which tells starting offset by time stamp strategy.

What is the strategy.

Default is error.

And we want to change it to latest so that it simply doesn't throw an error.

There is no record.

That's fine.

Move the pointer to latest and give me the latest record if there are any.

But if there is no record.

Starting from an past time later than my past time, then there won't be any latest record also.

So that's fine.

Just to avoid that error.

We can set it to latest and it should be fine.

So that's all.

Finally we load and then after loading you will get data frame and we apply count method.

Count action on the data frame and get the actual count.

That's all.

Once we have the actual count, all we have to do is to call the assert method, right.

So let me paste that.

Simple.

We have used that earlier also.

So assert expected count equal to actual count.

If yes pass the test case.

If no fail it right.

And then that's all.

What is next?

The next step is to write our run test method.

So run tests method will be simple.

Let me copy paste that also and then I'll explain it.

You keep on writing this run test since quite some time, right?

So I think copy paste should not.

Harm.

So here is my run tests method.

So what I'm doing, I'm importing Python's time package and simply taking a time stamp.

Starting time stamp before I start running my test scenario and I get time dot.

Time method will give me current time epoch time in seconds.

But Kafka this is starting.

Time stamp should be in milliseconds, so I simply multiply it by 1000 to convert that seconds into

milliseconds.

Round it.

So we don't have any decimal part of it, which is microseconds.

This starting time is time doesn't expect a decimal or a floating value.

So round it and get the starting time is time.

So this is my time stamp when I start running this test.

And after that usual things we clean the environment, create the Kafka producer, which is my application

here, and then call the producer dot Kafka producer or dot process method which defined here.

Process method takes a condition.

So for this test I want to send only those records which are created by store ID this right.

So we want to filter store ID so each file may have 1000 1500 records.

I don't want to send all those 1500 records for test scenarios.

We should stick to a small data set right, which we know and we understand.

So I'm sending only records which are only for this store ID and which will be 53 records in the first

iteration from the first file.

Right.

So I start the producer, it will trigger the streaming query immediately.

Query will be running waiting for data into the landing zone.

And then we start our iterations.

So first iteration we ingest first data file.

Wait for 30s.

By default it should work for 10s also but to be on safer side.

And then after that we call the assert method and assert method we pass the timestamp right.

So what my assert method will do.

My assert method will go to Kafka.

Pull all the messages that are sent after the given time stamp, which is after starting this test run

and take only 53 records, which I expect in the Kafka topic and assert it.

If we get 53 test case passed, otherwise it will throw an error and then similarly second iteration.

In the second iteration I expect 11 more records, so assertion should be 53 plus 11.

In the third iteration, I expect 25 more records.

So assertion is 53 plus 11 plus 25, whatever that number comes.

If everything works, we passed all the three iteration validation.

And finally we stop the stream that we started here, which is my Kafka producer stream.

That's all.

Let's paste some code to trigger this test suit.

Right.

So that's all.

And then we are ready to run it.

Let me connect to the cluster.

I hope I already have installed the Spark Kafka connector.

So let me run all and oops.

So looks like indentation.

Python is.

Critical about indentation, so let's run it.

Get Kafka message missing one required positional argument key.

So let's go there and.

Let's fix this key.

Should be.

The store.

Let me hardcode it.

You can even make a parameter and pass it, but that's fine.

Let's try running it.

So passing first iteration passed.

Second also passed.

Third also passed, so it worked.

Great.

That's all for this lecture.

See you again.

Keep learning and keep growing.
