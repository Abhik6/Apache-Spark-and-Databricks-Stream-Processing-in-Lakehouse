Welcome back.

In this lecture we will learn to create a Kafka Producer application.

So let's start.

So here is an example that we are going to build right.

In an earlier lecture, we already created a Kafka cluster in the cloud, and we created a topic in

the cloud with four partitions.

Now we want to write a producer application which will send data to the Kafka topic.

So we will write a Python producer.

We will write code in Python which will read data from a file.

We will read data from a file and create a message out of it.

Out of each record will be converted into a message.

And then we will send that message to the Kafka topic.

That's what we want to learn in this lecture.

And once messages or data starts arriving in the Kafka cluster on the other side, we will build a consumer

application and learn how we can create a consumer application, collect data from the Kafka topic and

process it.

So this end to end cycle we will complete in two lectures.

So let's start.

I want to create a Kafka producer application on my local machine, run it on my local machine and send

data to the Kafka cluster.

That's what we want to do.

So I've just started my PyCharm.

I'll be using PyCharm to create this project.

So let's go and create a new project.

Uh, let me call it.

Kafka producer.

Create a new virtual environment for this project.

Make sure you are using Python 3.10 version and all these are unselected.

Hit the create button and it should create a.

New blank project.

Right.

So we will be writing a Kafka producer application, which reads data from a file and then produces

each line of the file as a message to the Kafka cluster or to a Kafka topic.

So we will need data.

I already have a data file prepared for this.

Let me paste it in the project itself.

So here is the data directory.

And I have one invoices Json file.

Each record in this file contains one invoice.

Right.

Let me show you quickly.

So this is how this file looks from left to right.

We have each line represents one invoice.

Right.

You have already used this file in earlier examples.

I've taken the same file added in my project under the data directory.

So what I want to do I want to create a Python application which reads this file, takes each line,

converts it into a Kafka message, and sends it to the invoices topic in my Kafka cluster.

How to do it?

Let's start.

For creating a Kafka producer, the first thing that you need is to install a Kafka client library.

That client library is created by confluent and made available to us.

So go to your Python packages.

In your project and search for confluent.

You will see confluent Kafka may be at the top.

Choose that and hit the install button and that's all.

Your ID will download the Confluent Kafka client library and install it on your project.

Once installed, you are ready.

So let me go to.

My project.

Right click and create a new Python file.

Let's call it.

Invoice producer.

That's all.

We are ready to code now.

So first thing first let's import some important packages.

So.

We will need Kafka producer so import Kafka producer from the confluent Kafka package.

We will need a couple of more packages, so let me import them.

We will be needing Json package and we will be needing time package imported it.

You will see when and why we need it.

Now.

Next thing is to start coding.

So instead of coding like a script, let's create a class.

And then we wrap everything inside the class as methods.

So.

className is in invoice producer.

Let's create a class constructor.

And in the class constructor we want to define two variables.

I have already defined it.

I've already written that code.

So let me paste that code here and then I will explain it.

In the interest of time, I'm not going to type in everything but paste it here and then I'll explain.

What do I have?

Great.

So what I'm doing, I'm defining a variable called topic, which is nothing but a topic name.

This is the topic that we created in our Kafka cluster.

So we will be sending our messages or our data to this topic.

For connecting to Kafka cluster, we will need some mandatory configurations.

So that's what I'm packaging here in the single configuration variable.

Variable name is conf.

And this is a Python dictionary object.

And I have key value pairs of all the configurations which are mandatory.

So first mandatory configuration is bootstrap servers.

This is nothing but location of your Kafka cluster.

So in the value here we need to supply the host name of your Kafka cluster or IP address of your Kafka

cluster, and the port at which Kafka cluster is listening for new messages.

So we will get that detail from our Kafka cluster that we created in an earlier lecture and paste it

here.

I'll show you how to get those details for making a connection to the Kafka cluster.

It should not be that anyone and everyone can come and start dumping data into my Kafka cluster, right?

So we need to have a secure mechanism to connect to the Kafka cluster, and only those applications

which has the credential to send data to a particular topic, should be allowed to send data to the

Kafka topic.

So for that we need for details what is the security protocol we want to use and what is the mechanism

for the given security protocol.

So security protocol that we want to use is SSL.

We want to use SSL layer for sending data.

Mechanism is plain.

And with that we also need username and password.

So we will have a user created and a password created with permissions to send data to the Kafka topic.

So these two details I'll fill in later.

We will go get that done in our Kafka cluster.

Get the user detail and password and paste it here.

And the Kafka Cluster host name.

Also we will get from the cluster and paste it here before run.

There is one more thing which is called client ID.

It's not mandatory, but it is a good practice to send some client identification.

So I'll be sending my messages from my laptop.

Right.

So I just typed in Prashanth laptop here.

Uh, this will help us later to identify which message was sent by whom.

Right.

So that's one thing.

So that's all with these two things, we are now kind of prepared to start sending code messages to

Kafka.

So what do we want to do?

I want to create a function or a method which reads this invoices file.

Right.

Which reads this invoices file.

Take each line of the invoice, convert into a Kafka message, proper message with key and value and

send it to Kafka.

That's all we want to do at the bare minimum, right?

So let me write a code for doing that.

So I'll paste out few things and hope you don't mind that in the interest of time.

So the method name will be produce invoices.

Right.

And what do we want to do in this produce invoices.

We want to open this file right and loop through it.

So let me paste code for opening the file.

So I hope you know Python a little bit.

So with this open method which is file open, we'll open the file from data slash invoices dot Json

which is my this file.

Open it as lines.

And then we can look for the line right.

For line in.

Lines.

Right.

So we open the file as lines and we loop through each line in the file.

Right.

Uh, in the lines.

And what do I want to do.

First thing I want to do is to read each line.

Uh, convert it into a Json object because we will be reading text file.

Right.

The each line is text.

So let me paste some code for that.

I think we have a typo here.

Uh, I was wondering why it is showing red.

And the should come inside.

Okay, so what I'm doing.

I'm reading line.

From the file which is here in the lines, and passing that line into json.loads.

To convert each line, which is a string, into a proper Json object, and that Json object becomes

my invoice, right?

Each single invoice.

And from that invoice, I want to take out the store ID and and take it in the store ID variable.

So where is the store ID?

Let me show you.

You come to the Json invoice.

Each line is an invoice.

And in the line we have a store ID element here.

Right.

So we want to take that store ID and bring it here.

So I convert that line into a Json invoice.

And then we can use Json syntax to extract a value of a tag.

Right.

For a of an element.

So take a store id element value and keep it in the store.

And that's all we are prepared.

What I want to do send the store ID as a key and invoice this invoice as a message as a value, right?

Pack these two things key value in the message and send it.

So let's write code for that.

How do we send the message.

So produce invoices takes a producer object.

We will create producer object later.

It's a producer object made out of this producer class right.

So with the producer we will take this producer.

And we want to call.

Produce method right with the producer dot produce and it will produce the method.

Message to the Kafka cluster.

What does it take?

It takes which topic do you want to send?

Right.

So here is our topic name.

We take that?

Here.

So this is the topic where we want to message.

What is the key?

So key.

We want to send a store ID as the key.

Right.

And what is the value?

So we want to send invoice.

As a value, right?

And what else?

Or maybe invoice is a Json object, it might throw an error.

So what I can do, I can convert this Json object back to a string.

Right?

I converted that line back to a invoice for extracting the store ID, right.

And now this invoice is a Json object which is equal to a Python dictionary object.

And this value might not accept a Python dictionary object.

So we can either convert it back into a string, or maybe send the line itself.

Right.

So that will be more convenient because that's what we want to send each line.

Send each line as a message to the Kafka.

That's all we are done.

That's the bare minimum we need to provide to the producer dot produce method.

And it can start sending data to the Kafka.

But there is one more point here to understand.

My producer, this is my producer.

It will be sending data to the Kafka cluster.

Kafka cluster will accept that data, store it, save it.

Right.

But how would I know?

That message is successfully received at the Kafka cluster and it was successfully stored.

Is there any acknowledgment mechanism?

Yes.

So cluster after the broker.

The Kafka broker, after storing the data successfully, will send an acknowledgment back to the producer

API.

This producer itself, right.

Our producer will know that acknowledgement has arrived.

So an acknowledgement may be of two types, right?

There was an error.

I failed to save your message successfully or it was successful.

I successfully saved your message.

So producer will receive that.

Every producer will receive whatever messages they are sending will receive an acknowledgment.

But we want to take some action on that acknowledgement.

What do we want to do?

Let's assume I want to simply print that this message was failed and this is an error, blah blah blah.

Print it in the log.

So how can I do that?

So what you can do you can define a callback here.

So.

So the producer dot produce method also takes a callback function, and for each acknowledgement the

producer API, the producer object will call your function.

So you can supply a Lambda function here.

Or you can supply you can define a function and uh, provide the details of the function here so that

for each acknowledgement, the producer API, the producer will keep on calling your function.

So let's call let me paste a name here.

We call it.

Self-delivery callback have not defined this function, but we will be defining that function.

So just to avoid some error, let me define it here.

Um, this is how that callback function looks.

For now, let's simply write pass here.

We will come back and code whatever we want to code here.

The basically the structure of the delivery callback function is fixed.

It takes two arguments.

The first argument is error.

The second argument is message itself.

So what will happen?

We will use producer dot produce method to send the data to the Kafka cluster.

And we are telling okay, data should go to this topic.

The key for the message is this value for the message is this.

And.

Time stamp is also a mandatory argument, right?

Every message should have a time stamp.

So we are not giving time stamp because we want the producer.

This produce method to automatically add a time stamp.

So whatever time we run it at that point of time, it will automatically add a time stamp and send this

message to the Kafka cluster.

Key value automatically added time stamp will send it to a topic, the cluster.

The broker will receive that message and send a acknowledgement back to the producer.

The producer object.

This object itself will collect that acknowledgement and for each acknowledgement means for each message,

some acknowledgement will come.

So for each message, whatever acknowledgement is received, this producer API will call our function

the delivery callback.

Passing these two things error or details of the message.

What message was delivered to the Kafka cluster?

What is the message that it has saved?

So if in case of error, you can print some error message or you can log the details and take action

later, or you can retry or whatever you want to do, right.

In case of success, you can log that something is successfully processed.

So let me paste some code here before I come back to the produce invoices.

So I'll remove this path and paste some code here.

That's all.

So let me quickly explain what I'm doing.

If error.

In case of error, what we want to do print error, message right error, colon message failed delivery.

And we also print the error description.

Right.

It will come here.

So that's all.

That's all we want to do in case of error.

Else if there is no error message was successfully delivered and the cluster has also successfully saved

it, then what do we want to do?

We take that message and from that message message comes this MSG is an object message object which

comes with a lot of details.

It comes with which topic the message was delivered, what was the key when it was delivered?

What was the value?

Which partition cluster has saved your message?

What is the offset number in that partition?

All those details are available in this message object, but we are not interested in all those details.

What I want to do is take only key from that message.

Message dot key function will give me the key.

Take that key.

But key and value are always binary.

For Kafka I'm sending string key from here, right?

Store ID is a string and this line is also a string I'm sending as a string.

But before it goes to the Kafka cluster, it is converted into a binary.

That is, string is converted into the binary representation of that string.

So when we receive the acknowledgment here back in the acknowledgment also key and value will be binary.

So what I need to do convert that binary, decode it using UTF eight character encoding, and convert

it into a string so that it is readable for the end user.

So that's what I'm doing.

Message dot key will get me the key.

I'll convert it in a UTF eight string, decode it using UTF eight character encoding, and I'll get

the key.

And then I also want to take the invoice ID.

So this first element in the invoice itself is the invoice number with some number here.

So what I want to do take message dot value, convert it into a string.

And once it is converted into a string, use Json dot loads method to convert that string into a Json

object so that I can extract an element from the Json and what element I want to extract invoice number.

So I'll take this invoice number just to make sure just to print proper log entry here, that this key

and this invoice ID was successfully delivered.

Right.

So then I'm putting a print message here produced event.

And maybe two is not required produced event with this key and with this value.

And that's that's all we want to do in case of successful delivery.

So that we can keep getting logs that what is delivered.

And if something fails we know what it failed.

Right.

So that's all about the callback.

Uh, I have the produce code here.

But it's not complete yet.

One more thing we want to do.

In the producer is after sending.

One message to the Kafka broker.

We should also call the produced poll method, and poll method takes one argument, which is time in

seconds.

So and it is timeout.

So what poll method will do?

After sending data to the Kafka broker poll method will look into the producer object for acknowledgement

and pull the acknowledgement.

Right?

So poll method is the one which will make sure that delivery callback is called for each message, right?

So calling poll method after producing data to Kafka cluster is mandatory.

You should always call a poll method.

Poll method may take some time to respond.

Maybe acknowledgement has not yet arrived, so it is waiting for acknowledgement.

So we give a timeout that one second maximum one second.

You should wait.

It is not mandatory to call poll method at after calling produce method every time.

It is also possible that you send 100 method messages.

You call producer dot produce 100 times and then poll it once.

So all 100 acknowledgments you poll immediately at once and then again send 100 messages and then poll

100 acknowledgments.

Right?

So that is much better.

So maybe you can put one more one counter kind of thing here.

So let me do that.

Right.

So let me define a counter here.

Counter.

Equal to zero in the beginning.

After sending it to Kafka, I can increment the counter.

Plus one, right?

And, uh.

Now I can put a condition here if.

Counter equal to n.

Let's.

Uh, define some and or, uh.

Uh, let's call it counts.

And I can pass.

This counts here right as a parameter.

So if count are equal to counts then what do we want to do.

We want to pull and.

Break also adds.

Right.

So what do we want to do here?

I want to define a counts counter here so that I can pass.

Let's say I passed 100 here.

So this file comes with the this file.

My file comes with maybe more than 2000 records.

Right.

1519 verses are there.

I don't want to send all.

So you know this produce method I can pass some number here.

Let's say ten.

Right.

So what it will do.

Open the file, start reading messages from this file and start sending it.

Increment the counter.

And when counter equal to ten which is equal to counts, we will pull all ten and then break.

Which means we want to exit this loop.

We just want to send ten messages and that's all you you can apply whatever logic you want.

I'm just putting up a logic here to make sure I don't send 1500 messages.

I send only what I want to send out of those 1500.

That's all.

That's my produce invoices method done.

What else?

We need one more method which will call.

These methods will actually start the production.

So let's call it start method.

So what do we want to do in this start method?

First thing is to create necessary objects right?

So let's create a Kafka producer object.

Actually, I want to create an object of this producer.

So create a producer.

And this producer takes connection details.

Right.

All the configurations.

So configuration we defined here.

There it is.

So what we are doing using the producer class, passing the configuration to create a Kafka producer.

And once this producer is done, it's ready.

What we can do, we can call our producer invoices.

Right?

So.

Produce invoices.

And we need to pass this producer here.

And it also takes some count.

Right?

So let's say ten though my file has 1500 plus records, I just want to send ten invoices to to the Kafka

cluster.

And that's all.

Kind of that's all.

But Kafka producer is an asynchronous producer, right?

By default it will keep on sending all the messages.

We want to send ten messages, so it will send ten messages, one after other and wait for the acknowledgement.

But my program doesn't stop.

The acknowledgement is received asynchronously, so my program doesn't stop and wait until the acknowledgements

are received.

So idea is just send it and wait.

Let acknowledgement come whenever it comes.

And do not stop the code for waiting because we are waiting for the acknowledgement.

So my code will quickly execute and without even receiving the acknowledgement successfully, it will

stop.

So to avoid that, we should use at the end when everything is done and we want to exit before exiting,

we want to call the flush method and maybe you can pass timeout.

So let's say 10s.

So what flush method will do?

It will make sure if there are some acknowledgements still not received, whether success or fail,

whatever.

If acknowledgements are not yet received, wait at least 10s before terminating the application.

If everything is received already, that's fine.

We can exit immediately.

So that's all we want to do, right?

And a small change I want to make here this poll I'm polling after sending ten messages, right after

sending whatever count I want to send.

So let me remove it from here.

Basically, after sending each message, let's poll it so that.

And we keep on getting the output displayed on our console.

Otherwise, at here, all ten outputs will suddenly come at the end.

We send ten messages and then we suddenly pull all ten at once.

So output doesn't look like it is coming like this.

Or maybe also I want to put a sleep here.

Right?

So maybe.

Time dot sleep and 0.5 seconds.

So don't want to send all ten messages or 100 messages at once so quickly.

Just just simulate.

I'm sending data slowly so we send one record, wait for half a second, send another record, wait

for half second, send another record.

So we have some time difference in the timestamp of the messages.

These are not necessarily mean in real project.

You will send data immediately.

If new data is there, you will send the next one immediately.

I'm just simulating few things for future examples for the consumer and other things that we want to

learn here.

Putting this sleep and putting this counter where we can break.

Otherwise your code is this.

This is your main code producer dot produce and you are sending data to Kafka.

So we are done.

If you want to run it, I'll have to write some main method here.

Right?

So let me paste the main method code.

That's all.

That's my main method.

So what I'm doing is using my class here in voice producer and creating an object here.

And then I'm calling the start method, which is this.

And the start method will take care of calling everything else.

Right.

So that's all we are ready to run.

But before executing it we need to make sure we have all these details filled my cluster IP, my username

and my password.

Without that, even if we try, it won't be able to send anything to anywhere.

So let's go back to our cluster, find out all these details and fill in those details here.

So let me go to my confluent Cloud environment.

I remembered my login, so I'm already logged in.

Uh, so here is my home page, right?

I have two environments, so let me go to environments.

I created cluster in the dev environment.

So let's go to dev environment.

And here is my cluster learning cluster.

Right.

This is the cluster we created in the earlier lecture.

And that's all.

So what do we want.

We want bootstrap server detail and we want username and password.

Three things we want to connect to this cluster.

So come to cluster settings and you will see bootstrap server detail.

Right.

So copy it and maybe keep it somewhere so we can use it.

Right.

So I've copied this and saved it in a text file.

And now you can come to API keys.

This is where you can create credentials right.

So create a key.

Let's go for a global access.

We will come about the details of limited access and granular access.

Global access is like will give you everything.

So go to next.

And here is your key.

So create this key.

Copy this key.

Uh, save it somewhere.

That becomes your username.

And here is your secret.

Copy that secret and paste this also somewhere, uh, that becomes your, uh.

Password.

So this is your username.

This is your password.

Download that maybe in some text file also and continue.

And that's all your key is created.

If you want to delete this key you can come to that key and delete API key.

Once you are done.

Or if you have shared it with someone else, you can come and delete it.

I'll delete it after recording this lecture.

So that's all we got the details we wanted.

And now let's come back to.

Topic.

We have the invoices topic.

Here.

Uh, come to messages.

And nothing is sent here so you won't see anything.

But when we start sending message, when we execute our application, you will see some messages here.

Right?

So let's go back and to let's go back to the ID and start executing it.

So here is my ID I'm back to my ID and let me.

Paste the details I have copied.

From the cluster.

So bootstrap server goes here.

And then my API key becomes my username and my.

Secret becomes my password.

Right?

So that's all.

I'm done with this and we are ready to execute it.

So let's come down and execute it.

Hope it works.

Oh, started working every half a second.

I'm sending one message.

Finished.

All ten are gone.

So if you go back to your cluster, you will see those messages received here, right?

Um, ten messages received here.

If you don't see it here, maybe you can come to jump to offset and jump to offset.

And.

Let's say if you are here right now and then you come to messages or after sending you come to messages,

it might not show you anything here.

So you can come to offset and type some partition and offset number and it will show you.

So on this partition zero we have these messages right.

So you will be able to see your messages.

Right?

So don't worry if you don't see messages here.

When we start consuming it, you will see those messages on your consumer.

And that's all.

Uh, what did we learn in this lecture?

We'll learn to write a Python producer, a simple one which reads data from a file and sends it to the

Kafka cluster.

Right.

And that's what we wanted to learn in this lecture.

Let me quickly show you what we intend to do.

Right.

So we intend to write a Python producer using Python language, which reads data from a file and sends

each line as a message to the Kafka topic, and we are done with that.

In the next lecture, we will write a consumer application which reads data from the topic and does

processing, and we'll take it from there.

See you again.

Keep learning and keep growing.
