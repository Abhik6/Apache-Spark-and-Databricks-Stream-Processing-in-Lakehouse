Welcome back.

In this lecture we will try to understand some core concepts associated with Apache Kafka.

So let's start.

So here are the things that we want to understand.

There are seven items listed here.

We will try to understand the meaning and the concept behind all these seven items one by one.

So let's start with the first one the broker or the cluster.

I hope you already understood.

What is a Kafka broker or what is a Kafka cluster?

So basically it's a Kafka server.

We usually have one Kafka cluster installed and it is nothing but a cluster of computers.

Right?

And each machine in the cluster is configured as a Kafka broker to execute the Kafka broker software,

and once it is configured and installed, Kafka is ready to accept messages from producers and send

messages to the consumers.

Right?

So Kafka broker or cluster is nothing but a cluster of computers configured with the Kafka software.

On the left side we have producer applications and these are nothing but applications.

We use Kafka producer APIs to send messages or data packets to the Kafka cluster, and most of the messages

are configured to have three mandatory fields.

One is key, other one is value, and the third one is timestamp.

So everything that we send to Kafka goes as a key value pair, which is time stamped.

Right.

So remember that because that's very important.

You can think of Kafka as a database, right, which collects data produced by the producers or sent

by the producers.

And each data record or each row can have a three mandatory fields key value and timestamp.

What is inside a key is on us.

We can decide what key we want to send.

What is the value that is also on us?

On the producer, a developer.

Whoever is developing the producer application they can send a value.

Could be a simple text.

It could be a Json, it could be a binary data.

Whatever it is, Kafka doesn't care.

All it cares is we have a key, we have a value, and we have a timestamp.

Then, for better understanding, let's assume I created a Kafka producer application which reads data

from a file, right?

And after reading data from a file, it can send messages to Kafka.

But what is a message?

Right.

So in this example, each line of text could be a message.

Right?

I can put some key here, maybe a number or line number.

Right.

And the entire line, the text of the line can go as a value.

And timestamp is just a timestamp.

At what time I'm sending that data from here.

Right.

What time the data or what time this record is generated or created.

That's what timestamp could be.

So I can prepare.

I can read data from a file and prepare a message with three fields key value and timestamp and send

in a simple case for sending a data from a file, I will read file and send each line as a message to

the Kafka broker, and Kafka broker will store all the lines in the Kafka cluster.

That's what the message means.

Take another scenario.

Maybe I want to write a producer or develop a producer application which reads data from a database,

or maybe read data from one table in the database and send all records to the Kafka cluster.

That is also a possibility.

So I can write a producer application, prepare the data, read data from the table, and take each

row to prepare my message and maybe primary key field of that row.

I can put in the key and rest entire row.

I can put it in the value and add a timestamp, prepare a message for Kafka and send it using the producer

API.

That's what the producer API means, or the producer side of the Kafka means.

So as we also learned that you can have multiple producers, you can have hundreds of thousands of millions

of producers sending data to Kafka Broker.

You can have different, different producers sending different, different types of data sets.

Right.

This producer is sending data from a file.

This producer is sending data from a table.

So both are sending two different data sets.

Right.

Or you can have producers which are sending same kind of data set, same schema, same type of data.

They are sending.

Like if you create a app, mobile app which is installed on the mobile, and that app is sending data

to Kafka, same kind of data, maybe user coordinates where the current mobile is lat long and sending

lat long to Kafka.

Right.

And all those mobiles where your app is installed will be sending same kind of data.

That is also possible.

Or you can have different types of producer, one producer or one set of producers are sending one type

of data, another set of producer sending another type of data to the broker.

Broker will keep on collecting all that data.

So that's what the producer means.

Now the next thing is on the other side of the Kafka, on the right side of the Kafka cluster.

So we have a Kafka cluster here.

You can have a application here which is called Kafka Consumer Application and Kafka Consumer application

will request Kafka Broker to pass the message right, the data that it has collected.

And then Kafka Broker will pass that message to the consumer.

You will get the consumer, you will get the data at the consumer end, and data remains the same.

Whatever Kafka is collecting, it means each record will be a key value and timestamp.

So each message is key value and timestamp.

That is what Kafka Broker collects.

And you can write a consumer application which receives all the key value and timestamps of the records

which are collected at the broker.

Right.

Some logic there, do some processing and maybe prepare some result and possibly save that result into

a database table.

Or whatever you want to do depends upon your logic.

Similar to producers, you can have multiple consumers, right?

So we have one consumer here which is reading messages, key value and timestamp, doing some processing.

Similarly, we can have another consumer connected to Kafka.

Right.

And this consumer application is also reading key value and timestamp messages and doing some processing.

Maybe it is looking at the message and based on some condition it is sending an email to the end user

or notification.

Right.

So it depends upon your requirement.

So the point is that for each broker you can have multiple consumers.

One consumer might be reading some data another.

You might be reading some more data and they are doing their own processing.

This might be doing something else, and this consumer might be doing something else.

Right.

So that's how you build consumer applications.

Now try to look at the complete picture.

So complete picture looks like this right.

We have two producer.

This producer is reading data from a database table and sending the records to the Kafka cluster and

sending it as a key value and timestamp.

So we read data from the table and do some formatting to prepare a message, make it key value and timestamp

and send it to the cluster.

Similarly, this producer is doing something else.

It is reading data from a text file and preparing it as a message and sending it to Kafka cluster.

And on the other side, on the right side, we have two consumers both want to process data, but this

consumer wants to process data sent by this producer.

Right.

These blue messages and this consumer want to read data from produced by this producer.

The green messages.

Right.

So how we separate out the messages.

Right.

This producer is sending this producer is also sending these two messages are different.

They have different value different meaning.

Right.

And these two guys are expecting different messages.

So this guy don't want to read data produced by this producer.

And this guy this consumer don't want to read data produced by this producer.

This green consumer want to read only green messages and this blue consumer want to read only blue messages.

So how do we separate this out for doing that?

Kafka comes with an idea of topic.

So topic you can think of a Kafka topic like a table in a database, right?

You can think of Kafka as a database with some additional capabilities a scalable, highly scalable

database which can collect data from sources, which can receive data from producers.

Right.

Which can send data to the consumers, but to separate out data sets, we can create topics in the Kafka

cluster.

For each topic is like a database table, right?

You can just try to correlate it with.

So if you think of Kafka as a database, this topic is a table.

So when we send data from the producer, when we develop our producer, when we write code using producer

to send a message to the Kafka, we tell that which topic do we want to send this data.

Right.

So we put topic information also along with the message.

So we send data to a specific topic in the Kafka cluster.

And then Kafka cluster will receive that data.

The broker will collect that data and save it in the given topic.

Right.

So when consumer wants to read it, it will also tell which topic do I want to read.

Right?

And then Kafka Broker will take data from the given topic or from the desired topic and send it to the

consumer.

Right.

So that's how we separate out.

Now in this situation, if we have two separate topics topic one for blue messages, topic two for green

messages, blue consumer can consume only blue messages.

It can tell okay, give me all the data that you have collected in topic one.

And the broker will send all the data that it is collecting in the topic one.

Only blue messages will go to this consumer.

Similarly, this consumer can ask only give me only green messages that you have collected in topic

two and only these messages are.

These records will come to the consumer.

And that's how we can separate out who sends data where and who receives data from where.

Right.

So that's another concept related to Kafka cluster topic.

Now the next concept is topic partition.

Right?

So we know Kafka is highly scalable, right.

And it is a distributed system.

And it can collect or receive data from these producers at massive scale.

It could collect data in GBS or even in terabytes or in petabytes also.

Right.

So think about the scale.

If your volume is too high, too large, fitting all that entire volume of data which is collected from

producers may not be possible to fit into a single computer on a single machine to avoid that problem.

To overcome that bottleneck, Kafka came up with an idea of partitioning the topic.

So if you think of topic as a table.

Table is partitioned also, right?

So in Kafka the topic is always partitioned.

You may have minimum one partition for the topic, or you may have 510, 100,000 whatever you want,

depending upon how much volume you want to collect in that same topic.

Right.

So you can define when you create a cluster, when you create a topic.

Actually, at the time of creating the topic, you can define how many partitions do you want?

Because we will have some sense of what amount of data we are going to collect in this topic, and based

on that volume, we can estimate that how many partitions do we need.

So you can define the number of partitions.

So now think about it.

Your data is coming from producers.

And it will come to the broker.

And broker want to keep that data in one of the partitions.

Right.

And each partition may be managed or is stored at a single machine in the cluster.

Right.

It partition cannot be broken down further, so it can only be stored on a single machine.

So each partition goes and sits on a single machine.

Now data is sent by the producer.

It is collected by the Kafka cluster or the broker.

Now broker wants to decide that which record goes in which partition.

And that's where the key, the message key comes into play.

Right.

So what broker will do.

Broker will look at the key, hash it, apply a hash algorithm to generate one unique number out of

it.

And that number will be between zero to number of partitions.

So if I configured my topic to have 01234 partitions.

So what Kafka Broker will do.

Kafka broker knows okay this topic has four partitions.

It will take the message key which is supposed to go into this topic right T1 topic.

It will take the message key for that record.

Hash it to generate a number between 0 to 3.

Right.

Because we have only four partitions in this topic.

And if the hash number is zero, it will send it to zero.

So that record goes and sits into the partition zero right.

If hash number is two then it will send it to P2.

Right.

So that record goes and sits into the P2.

So that's how Kafka Broker will distribute your records amongst these partitions.

What does it mean.

Very important concept.

It means each unique key a message.

All the messages with the same unique key will go into same partition.

So if you are sending a message with customer ID as a key, right you are sending.

Suppose this producer is sending a list of customers and customer ID is a key.

Each ID is unique, right?

So those keys will go and sit into any of the partition.

Doesn't matter.

But let's say you are sending orders right.

And store ID is the key, right?

So you let's say you are this producer is sending all the invoices or orders as a message.

And let's say a store ID is the key.

So all the invoices or orders generated at the same store ID will go and sit into any one of these partitions.

But all the messages for the same store ID will go into the same partition.

And that's very important concept, right.

Because all the key with the same store ID will generate same hash number.

Right?

And hence they will go and sit into the same partition.

And that's very, very important concept of how data is distributed amongst the partition in a single

topic.

So I hope it made sense.

Let me quickly summarize it.

Kafka topics are partitioned.

You.

If you think of topic as a table in database, you can have partitioned table, right?

A table can also be partitioned on some key.

Right.

So topic Kafka topic if you think of it as a table.

Topic can also be partitioned.

We decide the number of partitions.

And that partition is not based on key.

But data goes and sits into the partitions based on the message key.

Right.

So that's one approach.

Sometimes it is also possible that you send null keys.

Your messages doesn't have any key.

So in that case you can your Kafka cluster can send data to round robin method.

Right.

The first message goes in zeroth partition.

The second message goes in P1.

The third message goes in p3, p2, p3.

And again it starts from p zero.

But that's not very useful in long run.

So we always make sure that key is a valid key.

Whenever we send data through the producers we make sure there is a valid key there.

And so that we can aggregate data at the key level in different different partitions, which is beneficial

while we are processing the data and we'll learn about it.

So that's all about the partitions and topic.

The next concept is partition offset.

So what is that.

We call it partition offset.

So we know that each partition will store some messages or some records.

Right.

Record comes and sits into one partition.

And then next record for that partition will come and sit next to the previous record.

So each record when it goes into the partition is given an index which we call partition offset.

So first record in this first partition will have an index zero.

Second record in that partition will have index one, then index two, three, four five and so on.

And it's consecutive right.

It starts with zero and keeps on growing with the next coming message.

And each partition right.

Each partition will have a partition offset for their records which starts from zero goes on 123, four

like that.

And that's what the partition offset means.

So.

The idea of topic is very important.

The concepts associated with the topic is really, really critical.

So let me quickly summarize these three things.

Data comes and sits into the Kafka topic.

You can think of it as a table.

In database topics are partitioned.

You can configure any number of partitions, but topics are partitioned right.

And data goes into those partitions based on the message.

Key data is distributed based on the key.

And third thing, each record in the partition will have a partition offset, which starts from zero

for each partition and then keeps on increasing by one with each new record.

And that's all.

So I hope you learned all these concepts we talked about brokers or Kafka cluster.

It is nothing but a cluster of computers configured with the Kafka broker software.

Then we have producers.

These are the applications that send messages to Kafka broker consumers.

These are another type of applications which want to receive data or messages from the Kafka broker

and do some processing.

Each message when it is sent to Kafka.

Topic must be consists of three things key, value, and timestamp.

So each message goes as key value and timestamp.

Key could be null.

Value could be null.

It's up to you.

What do you want to send?

You want to send garbage?

Send garbage right?

But ideal case you should have a valid key, valid value and then a proper timestamp.

A topic is like a table in the Kafka cluster.

So you whenever you send your messages or the producers want to send data to Kafka, they send it to

a topic, right?

It could be something like a table.

And then these topics are partitioned.

They are broken down into partitions and the data is distributed.

Data in a topic is distributed across the cluster right for scalability.

And each partition will have a offset for each record which comes and sits into the partition.

And these offsets start from zero and keep on increasing.

So that's all.

And we will start using all these concepts as we progress with the course.

That's all for this lecture.

See you again.

Keep learning and keep growing.
