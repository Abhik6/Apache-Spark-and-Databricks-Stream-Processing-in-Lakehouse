Welcome back.

In this lecture we will try to learn two things.

The first thing would be to learn how to implement Idempotence.

We will also learn what is idempotence.

And the second thing that we will try to learn is how to use Kafka's for each batch sink.

That's a Swiss army knife.

Very powerful.

So we will learn how to use Kafka's for each batch sink.

So let's start.

So before we start, let me give you a quick recap of what we developed in the previous lecture and

what is the problem with that solution.

So in the earlier lecture, we were ingesting data from a Kafka topic, right?

We created a job streaming job which reads data from a Kafka topic.

Applies minimal possible transformations and writes the result into a bronze layer target table.

Right.

That's what we have developed.

But there is a problem with this solution.

Let's try to understand what is the problem with that solution.

So let's assume you build this solution or a similar kind of solution for your requirement, where you

are reading data from some source and writing it to some target.

Right.

So you created that job and you deployed that job on, let's say, 1st October.

You deployed a new job and then you started the job.

A job is running from 1st October till 14th October.

Everything worked perfectly fine.

You started the job and it kept on running continuously from 1st October to 14th October.

On 14th of October you came up with a new version of your job with some new additional features added

into the same job.

So how do you go and deploy that job?

So you will go and stop the previous job, right.

The job that you deployed on 1st October, which is already running, you want to deploy a newer version

of the same job.

So you stopped that job, deploy the new code, new features, right, and restart the job from the

point it was stopped.

Right.

So you stopped and then deployed and then restarted the same job.

The problem is how do you restart the job from the same point where it was stopped.

Right.

So that's easy.

Spark structured streaming gives you an out of the box capability for restarting the job from the point

where it was stopped, and that capability is offered to you through the checkpoints.

Right.

So checkpoint allows you to restart the job from the same point where it was stopped.

Simple.

Up to here everything is fine.

But on 17th October you realized there was a bug.

There was a bug in the previous deployment when you deployed a new feature, you missed one bug.

It went to production and from 14th October till 17th October, your new code is running with the defect

with the bug which is identified later in the production.

We somehow missed to identify and rectify that bug during the testing cycles.

So it went to production and it's running from 14th to 17th October.

What you will do, you will fix the bug, right?

You will fix the bug, prepare a new build new version and deploy your new version of your job in the

production again.

So you did that on 17th.

Same day you prepared the fix and you came up there to deploy the job.

So how do you deploy?

First you have to stop the previous job.

The job which are which is running from 14th October till 17th October.

You stopped that.

You stopped it deployed the fixed version.

So you deploy the fix and then restart your job.

But now you want to restart from the 14th October, not from the 17th October when you stopped.

You don't want to restart your job from the point it was stopped.

You want to start your job again from some back date like 14th October.

Why you want to do that?

Because you know, the bug was introduced on 14th October when you deployed the new feature.

And since 14th October, all the data that you are processing in this job is producing faulty results

or incorrect results, right?

So what you want to do, restart your job from the 14th October so that you can reprocess all the data

that you collected or that you received from 14th October onwards, everything you want to reprocess

and so that you can correct the results.

Right.

So how would you restart your job from a back date from a 14th October date?

How do you do that?

We already learned that also, right?

As part of the best practices, we learn that you can put a starting timestamp in your spark dot read

stream, right?

And with that help of that timestamp, start timestamp, and an empty checkpoint, you can restart your

job from whatever date you want, right?

So you can go back in time and restart reprocessing your data again from 14th October.

We already learned that.

So you did that.

You applied starting timestamp as some date time stamp of the 14th October, and then you deleted your

checkpoint or gave a new checkpoint directory location and restarted your job.

So everything works.

Find your job starts from 14th October.

It starts reprocessing these all the data that you have received since 14th October.

And.

Everything in the target table is supposed to be corrected.

But there is a problem here.

The problem is, the job that you have written is an append only job, which inserts new records in

the target table, and you are running it twice, right?

On 14th October, you started your job.

All the data was processed from 14th to 17th.

You inserted all those records in your target table.

On 17th October, you stopped the job, deployed the fix and restarted from 14th October once again.

So all the data is processed once again starting from 14th October, and your job is designed to insert

results into this table.

It's an append only job, so your job will insert records in this table and it will create duplicates.

Right?

It will create duplicate records.

The old incorrect results are already there in this table, and new correct results will also come and

sit into this table.

That's a problem.

How do we handle that problem?

The solution for these kind of problems is to implement idempotence in your sink.

So what do we mean by idempotence?

That means if you get same data from the source more than once right in the target, you are not reprocessing

or recreating the result.

You are going to implement a merge statement there so that the result table, if a new data is there,

it's inserted.

But if an existing record is received from the source instead of inserting, it is merged there.

And that's how we can implement Idempotence.

Right?

So let's try to modify our application and implement Idempotence so that this problem can be solved.

Let's start coding.

So let me go to my workspace and.

Create a new notebook.

Let's give a name to this notebook.

Idempotent Kafka to bronze.

And what do we want to do in this?

We already created Kafka to Bronze application in the earlier lecture.

So what I want to do, I want to copy that same and then modify it to make it idempotent.

So let's.

Open Kafka to Browns in a separate tab, and let's copy this code.

Paste it here.

I'll close this because that's not required.

And that's all.

Let's start modifying it.

So we will need all this.

We will need the constructor where we define all the base directory and these variables.

All that is required we will need ingest from Kafka because we still want to ingest from Kafka.

Starting time stamp is already implemented.

Max offsets per trigger is ten.

That's too small.

Let me make it 30 and all that we are ingesting from the invoices topic we will need get schema also

and we will need get invoices function.

And then we have a process here right.

So what do we want to do.

In the earlier.

We are directly writing data to a table and table name is invoices basic, right?

So we take the final invoices DF, which is our final result, and use right stream to write it to a

table.

Now we don't want to do directly write to a table.

Instead.

What do we want to do?

We want to take that data frame this data frame, and implement a merge statement to merge the result

into the invoices table.

So by default, if you are doing write stream to a table implements a insert statement behind the scene.

So whatever data you have got in the invoices, DF will be inserted into the invoices table.

And that's where we have a problem.

If we keep on inserting simply without considering what we are inserting, then if we are reprocessing

same data multiple times, we will create multiple inserts.

We will insert the result multiple times.

So what do we want to do.

We want to remove this part.

We don't want to write to a table.

Instead we want to catch the data frame in between and use that data frame to implement a merge statement.

How to do it.

So the simple idea is to implement for each batch for this write stream.

So let me do that.

So maybe after giving the query name I'll use dot and then implement for.

For each batch method.

And for each batch method takes a lambda function, or you can define a function and give function details

here.

So let's call it self dot.

Upset.

So what I want is park to do while writing the stream, right?

Stream will execute on this dataframe, right?

Invoices df.

So what I want is spark structured streaming to take this invoices df, invoke the write stream, implement

checkpoint and all other options that I'm giving and call this function.

For each batch, for each micro batch, whatever records are there in the invoices, pass that invoices

directly to this function.

Upsert function and will take care of whatever I want to do in the Upsert function.

And what I want to do in the Upsert function.

I want to implement a merge statement in the absurd function.

So we will define Upsert function somewhere here.

So let me define a Upsert function.

Let me copy the name so we don't make any mistake in this spelling.

So def upsert and self, and this upsert function should take two parameters.

The structure of the lambda function inside the for each batch is fixed.

It should always take two parameters.

The first function parameter is data frame.

So we expect this invoices to go there.

So let's give a name to this first parameter as invoices.

Because Spark Structured Streaming will simply pass the invoices for one micro batch into this function.

And then second parameter is the batch ID.

So.

Batch or the micro batch number sequence of the micro batch.

So for the first micro-batch it will be zero and second micro batch.

It will be one and so on.

We don't care about the batch ID, but the structure of the function is same.

So we must have this parameter also.

And what do we want to do in the Upsert method.

So we take this.

And we want to implement a merge statement for this invoices.

So what do we want to do?

Take all the records in the invoices and merge it into the target table.

And that's why we removed two table from here.

We don't want to simply insert into the two table.

We want to implement a merge statement.

So I want to implement a SQL expression for merge statement.

And SQL expression requires a either table or view.

So what I want to do convert this invoices into a view.

So you already learned that in spark.

So invoices dot create or.

Create or replace temp view.

Right?

So I will create a temp view using the invoices.

This let me copy the view name.

So.

What name do I want to give?

I want to call it invoices.

Temp view.

Simple.

Right.

So view is done.

And then what do I want to do.

I want to implement a merge statement.

So let me define a merge statement here.

I'll copy that also and then maybe explain it to you.

That's all.

So what I'm doing merge statement and statement is merge into invoices.

That's my target table.

Right.

So merge into invoices.

Is it.

Call it as maybe is using invoices temp invoices.

Temp view which defined here.

Let's call it T.

And then on s dot value equal to t dot value.

Compare it for the values and timestamp should be same.

So if timestamp is same, value is same.

That means it is same record.

It's a duplicate record right?

And when matched then updates it is star.

So if it is matched then update all the values.

The new values should replace the old values.

So if I'm reprocessing the record once again previous record is supposed to have created some incorrect

result.

So what do we want to do?

Take the new result and update all the values from the previous iteration, and get the latest values

or the correct values when not matched.

If it is a new record, then insert a star, insert all the columns.

And that's all.

That's so simple, isn't it?

So implementing Idempotence can be implemented using a merge statement.

And since spark does not give you a action for implementing merge in the target table, so we have to

implement it manually.

And for doing that, spark gives you a sink called for each batch.

For each batch is a sink, which is super simple.

It calls a function and will pass your batch data frame whatever records you have prepared.

As a result.

For the current micro-batch, it will pass that data frame itself to you in that function and in that

function, whatever you want to do, I want to implement merge.

So I have written code here to implement merge you can do many things.

You can write it in some external database from here.

Right.

I can connect to MongoDB database and write the result in the MongoDB.

I can maybe connect to Kafka and write it into the Kafka topic.

We already have a Kafka sink out of the box, so we will not do that.

But I can do.

I can call some Rest API from here and pass the results over the rest API.

Whatever possibilities are enormous.

It's completely up to you.

What do you want to do?

I want to implement merge.

So I've done merge here and we are almost done.

So what we are doing here invoices dot write stream I'm telling the query name.

I'm telling my sink is for each batch.

So you should pass the data frame to this upsert function.

I'm telling the checkpoint details.

I'm telling append mode is there, but where is the action?

The two table was an action.

All this from here to here is configuring the write stream.

But we don't have an action.

So some cases when you don't have an action you can define a start action.

So at the end if you are not using two table we must use start which will start this write stream will

start this query.

Return the query handle which you can return from the function.

And that's all we are done.

Now.

What is next?

The next step is to write a test suit for this and test it.

Right.

So let's.

Let's write a test suit.

So let me create a new.

Notebook.

So I'll go to my directory.

Create a new notebook.

Let's call it idempotent.

Kafka to Brown's test suit.

Test suit is also same, right?

Like earlier.

So let me open Kafka to Brown's test suit in a separate.

And copy things from there.

So instead of ten, I want to.

Import the.

12 idempotent Kafka two bronze notebook and then copy this entire thing.

Close this.

And we will come here.

So Kafka to branch test suit.

Let's keep the same name.

It doesn't matter.

And see what do we have to change in the test suit.

So base directory we will need cleanup remains same drop table if exists.

So before starting our test suit we will drop the table.

Delete the table directory.

Delete the checkpoint.

Assert result remains the same.

Wait for micro-batch.

Also we need this.

So most of the changes are going to come here in the run test.

So what do we want to do in the run test.

First step is to clean the test environment.

So we are calling clean tests.

That's obvious.

Then we create a instance of our class Z stream.

And that's all.

But then.

We start creating our iterations, right?

We start testing our iterations, but we are implementing a merge statement in our application, right?

Remember, in the bronze we are implementing a merge statement we are not using to table action in our

write stream.

The to table action is very powerful.

It does implement only insert.

So whatever result you have got in your micro batch, it will insert that result into the table and

if table does not exist, it will create the table also and it will use your data frame schema.

Your result data frame is schema to create a table to create a target table.

So that is very powerful.

You simply use two table.

And don't worry if table exists or not for the first time, it will automatically create the table and

for next of the rest of the microbatches from next micro-batch onwards, it will keep on inserting records

into the table because table already exists, but we are doing merge statement and we are implementing

it manually using for each batch.

Who will create the table.

Right.

So merge statement expects that table should already be there.

Your target table should be there.

Right.

So we will have to make sure that we include create table statement in our setup script or for testing

we will have to create the table right.

So for creating a table I'll need a schema.

And then I'll need to execute create table statement.

So let me copy some code for the same and I'll explain it.

It's very simple.

Create table statement and you will learn one more trick right.

So what I'm doing is I created this stream which is my bronze table class.

And this stream class.

We already have a function for get schema, which gives me the schema of the target table or schema

expected schema of the target table.

And actually it is not the schema of the target table.

It is a schema of the value field.

Right.

So we can use that schema to define a table here.

So next statement I have is spark SQL.

So which I can use to execute this SQL statement.

And SQL statement is create table invoices Z I'm creating key column and value column is a struct with

the value schema that we have already defined in the in the get schema function right.

So I take the value schema, embed it here to create a struct for the value column, and then we have

topic column and timestamp column.

We add it.

So in one line I can create a create table statement because I already have a schema definition here.

So I'll use the create table statement to create the target table.

And rest is same.

We start with the first Micro-batch or first iteration of.

Start the query.

Wait for 30s, then stop the query and assert result.

Result should be 30.

Right.

And then second iteration.

Also, we expect only 30 records in the target table because we are starting from the point where we

stopped.

So no new records are expected because I'm not sending any new records to Kafka topic.

So same 30 records should remain in the result table.

So assert result.

And in the final scenario we are going back in time and restarting from the previous set of data which

we already processed earlier.

So in the earlier test suit I expected 40 records because we are reprocessing ten records.

30 records are already there in the target table.

Ten more records should enter into the target table.

So we are validating it 40 records.

But now we implemented merge right.

So we don't expect the duplicate records to get inserted there.

So what should be the result at the final state again 30.

Right.

So we expect another again 30 records because we implemented Idempotency in our sink.

So that's all we are done with that.

Let's go to the cluster.

And before executing this code I need to install Kafka Spark Kafka connector.

Right.

So let's install it.

Every new cluster you have to install it again and again.

Right.

So.

Let's install it.

And once it is installed, we will try to execute our test.

So.

Done.

So let's go there.

Connect to the cluster and run.

Let's see if it works.

Okay, so it completed, but I forgot to.

Add code for.

So these two lines are needed there.

Let me add that and run it again.

Oops.

Error.

So test failed.

Actual count is zero.

So looks like we couldn't insert into the target table.

So let me go to my code again.

And.

So this is my merge statement okay.

So we defined the merge statement string here.

But we forgot to execute the merge statement.

So let's do that.

So let me paste code here for executing the merge statement.

So what I'm doing I'm doing a spark session dot SQL and passing the merge statement here so that we

can execute the merge statement.

But instead of doing spark dot SQL I could do something like spark dot here.

That's what we do in normal case.

But instead of doing a spark SQL, what I'm doing here is we are taking the data frame, which is my

invoice DF.

And from the data frame I'm going to DF.

And then from the DF I'm recreating the spark session.

So a spark session method will either create a new spark session or give the existing session.

So basically what I want to do, I want to take the existing session which is used to create this invoice

DF right for the same data frame, whatever the existing spark session was used to create, I want to

take that.

And from that session I want to execute the SQL method and pass the merge statement.

And this is important.

This is necessary if you do spark SQL and try to execute the merge statement, you never know.

It's a distributed system and Upsert is a callback method, right?

So Streamwriter will as a as part of the.

For each batch sync will call your Upsert method as a callback, right, and pass the invoice, which

is your original data frame result data frame.

Right.

So you never know which session is there.

And we created a temp view here, right?

So it's important to make sure that we execute our merge statement in the same session.

And this is the way you can get the same session from the data frame itself.

You extract the same session from the data frame and then use that session to execute your merge statement.

If you try a spark SQL and try to execute the merge statement, it may work.

It may not work right.

So but this will work for sure.

So that's all for executing the merge statement.

We can come back to our this test suit and try running it once again.

So clean up started.

First iteration is started.

Let's see.

Great.

So we passed all the tests and we learned to implement Idempotence in our sink.

And that's all for this lecture.

See you again.

Keep learning and keep growing.
