Welcome back.

In this lecture, I'll give you an introduction to Apache Kafka.

We'll try to understand what Kafka is, how it works, and what we can do as a data engineer using Kafka.

So let's start.

So let's start with a very simple question.

What is Apache Kafka?

Answer is super simple.

Apache Kafka is open source distributed event streaming platform.

They call it event, but it literally means data, right?

So whenever, wherever you see event, you can replace it with data for an easy understanding.

So what is Apache Kafka is an open source distributed data streaming platform.

What we can do using Kafka.

There are two things that we can do.

We can collect or integrate data streams or event streams.

So the first purpose of Kafka, or first thing that we can do using Kafka is data collection and data

integration.

That's what we can do.

The second thing is you can process real time data streams.

So Kafka as a system also allows you to process the real time data streams.

So in total, we can collect and integrate real time data streams, and then we can process them also

using Kafka.

That's what Kafka means.

But what exactly it does and how it works.

Right.

So let's try to understand with some simple examples and it will make little more sense.

So here is a very simple example.

We want to try to understand what can you do with the Kafka.

So there are two parts of it.

One thing said Kafka is good to collect and integrate data, and then Kafka is good to do the data processing

also.

And all this you can do in real time as data is being produced, capture it as soon as possible, as

soon as it is produced, and then process it as soon as it is available for processing so all things

can be done in seconds or microseconds and millisecond is kind of delays.

But what this example represents.

So let's assume you have a smart meters installed in homes or factories or premises.

And these smart meters are designed to send some continuous data.

Right.

Maybe meter readings or current load which they are having from the household or from the premise.

Send all that data, they can send it over Wi-Fi or network whatever.

Right?

So you can collect all that data that these smart meters from different homes or different premises

they are sending.

You can collect all that in Kafka.

Since it is an IoT device, you may have to put a IoT hub or some kind of specialized tool to interact

with these devices so they can send data to IoT hub, and from IoT hub, you can send it to Kafka.

And all this can be done in seconds or in milliseconds, kind of duration.

Right.

So what does it mean?

Smart meters, generating data, sending it to IoT hub.

And from IoT hub, we are sending it to a Kafka cluster.

Right.

And these data events are collected by the Kafka cluster.

And Kafka cluster can store those events.

It's think of it like a database, right?

It can collect data and store that data.

So that's one part of data collection and data integration on the other part, once that data arrived

at the Kafka cluster, you can build some stream processing applications which can read data from the

Kafka in real time.

As data is arriving.

Keep on reading it and start doing some processing.

So end to end is smart.

Meters are producing data.

We are collecting that data into the Kafka, and then we have a stream processing application which

can read the data collected at the Kafka and process it immediately, as soon as it comes.

In simple terms, right?

We are using IoT devices here, so we have a IoT hub in between, but it's not mandatory in many other

cases.

For example, let's say you have mobile devices or apps installed on your mobile, right?

Those apps want to send some data, right?

So those apps can send data directly to Kafka cluster because they have the network, they have the

programming capability.

You can program your app to send data to Kafka cluster directly, and Kafka Cluster can collect all

that data, save it also, and then you can have a stream processing application on the other end,

which reads data from the Kafka cluster, does all the processing.

So Kafka is a central piece in between, right?

In between the data producers and the data consumers and data processing applications.

And it can work as a data collection or data integration tool.

And it's highly scalable.

You can send data from millions of devices or from millions of producers.

Right?

It it is a distributed system.

So you can scale you can horizontally scale the Kafka cluster to handle millions of records per second,

or even GBS of data collection per second.

You can scale it to that extent.

That's what at very high level Kafka is or what we can do with Kafka.

But the next question comes is how it works, how all this works.

We saw some use cases here, but how does it work?

So Kafka is designed to work on the principles of a messaging queue, right?

So maybe if you know a little bit about messaging queue in the messaging queue, we have a central broker

in the system.

In the architecture we will have a central message broker, and then there will be parties.

They can send messages to the broker and they can send a message, or they can send data to the broker

and then there could be other parties on the other side which needs that data, right.

So they can take data from the broker or the messaging queue.

Right.

So in this architecture there are three parties.

And that's how Kafka is also designed.

We have three parties in the Kafka in the center.

We have a Kafka broker or basically it's a cluster of computers where Kafka broker software is installed.

Right.

And it works like a broker.

A broker means it works like a broker between the people who are producing data and the target recipient,

so that they don't have to interact with each other directly.

They can work through a broker.

So that's why we call it a broker.

Right?

So Kafka cluster is a broker system data broker system.

Right.

And it's a distributed system.

So in the center we will have Kafka cluster which is scalable.

You can scale it to maybe hundreds of computers in the cluster and meet your requirements.

So in the center we have a Kafka broker or a cluster of computers.

And then on the left side you can have producers.

These are nothing but applications, right.

These are some systems which we have coded or we have implemented.

So these applications are known as producer applications.

And they use producer API, the Kafka producer API, to send data to the Kafka broker.

So they can use producer API and send data to the Kafka broker.

Sometimes we call it data events, sometimes we call it data, sometimes we call it message right.

But at the end everything is a piece of data.

Packet of data could be one, could be 100 KB, could be one MB.

Right.

But it's a one record or one data packet.

That's what we send.

Right.

So on the left side we can have producer applications.

They can use Kafka producer APIs to send data to the Kafka broker.

Kafka broker will receive that data, save it locally in the cluster for safety, for fault tolerance,

for persistence.

So you don't lose it.

And then you can have some consumers who actually need that data, right?

So consumers are also nothing but applications.

We write them, we code them.

So we code or we build consumer applications, which will use Kafka consumer API to consume or to read

or to receive data from the Kafka broker.

And once you receive the data, you can do whatever you want.

You can process that data, take actions or analyze that data, whatever you want to do.

So that's how Kafka is designed.

Kafka is designed like a messaging queue where you will have producer application sending message,

and that message is targeted for some consumers.

Right.

And then on the other side, those consumers of the message can use Kafka Consumer API to receive data

or collect that message from the broker.

And the broker in between works like a broker between these two parties.

And all of this, all these three parties are highly scalable.

You can scale your Kafka cluster to have thousands of nodes to handle GBS of data per second to receive

and send GBS of data per second.

You can have one producer, or you can have hundreds of producers, or thousands of producers and even

millions of producers sending data.

Parallelly to the Kafka cluster.

Similarly, you can have hundreds of thousands of consumers reading data from Kafka in parallel.

So it's all is highly scalable.

So that's how Kafka works.

Let me quickly summarize it.

Kafka has at the core three components.

One is your broker or the Kafka cluster, which is the central piece.

The left side of it is Kafka Producer API.

It's nothing but a library or a set of functions and methods and objects.

Right.

So that's what we call Kafka Producer API.

You can use those APIs to send data to Kafka cluster on the right side.

The third party is your Kafka consumer API.

These are also simple libraries or APIs, right?

You can use those API to build applications which can receive data or collect data from Kafka and take

actions.

So that's how Kafka works and that's how Kafka is designed.

Now we know that what Kafka is.

Kafka is a data integration system.

Kafka is a data processing system.

It has the capability to process data.

Right.

And it is made up of three core components.

One component is the central component is Kafka cluster or broker.

The one side of this system is Kafka producer APIs.

The other side of the system is Kafka consumer APIs.

But what can we do?

Or how do we fit this into a data engineering platform as a data engineer or as a data engineering solution

architect?

Where do we find Kafka to fit into data engineering platform?

So that's the next question.

So let's try to understand that.

So here is the diagram right.

We have a data engineering platform in between.

Right on the left side we have source systems where data is produced and captured.

And these systems own the data.

On the right side we have data consumers or consumer applications.

They need data and they need data to make data driven decisions.

Right.

So as a data engineering team, we are in between to stitch these two systems, right, to provide data

to these systems.

And in a typical case data engineering platform is in the center which is made up of three components.

The first component is data ingestion.

We ingest data from these source systems.

Then data processing we process data with which we ingested.

And then we prepare produce the result for these consumer applications to take the results, the final

results of the processing.

That's how data engineering systems are designed, right.

So where does Kafka fits into this.

So Kafka can fit at the edge, right at the left edge and at the right edge.

Now let's try to understand.

Kafka has a capability to collect data at very high speed in very huge volumes from literally or virtually

from any system, from anywhere.

Right.

So let's assume you have enterprise applications.

They want to send data or they have some data which we want to collect into our enterprise data engineering

platform so that we can use that.

You have mobile apps, thousands or hundreds of thousands users are using your mobile app.

You have data being generated at the mobile app and you want to collect that data.

You have websites, you have cloud systems, you have some microservices.

You have IoT systems or sensors placed in the field.

You also have data files or log files generated from different apps.

You have click streams, you have some transactional systems.

They produce a lot of transactional data.

Maybe you have databases where you have some tables and data is stored in these tables, or you have

IoT platforms that are sending data or generating data.

So all these variety of systems would be generating and capturing data for your organization.

Right.

And if you want to bring all that data into your data engineering platform, Kafka is an excellent solution.

All you have to do is to put a Kafka cluster in between.

Write one single Kafka cluster in between.

It could be scaled to hundreds of computers, but it's a single cluster in between.

Right.

And then you can configure all these systems to use Kafka Producer APIs and start sending data to the

Kafka cluster.

So Kafka is excellent in collecting data from hundreds of thousands of systems.

So all that data can be collected at one central place.

And that solves a lot of problem for us.

We don't have to pull data or collect data from different different systems and sort out the connectivity

problems with different different systems.

Right.

All that is sorted out once you collected data at one central place, and then your data engineering

platform can pull data from the Kafka cluster, right?

Everything is there in the Kafka cluster.

And there are two approaches to pull data from Kafka cluster.

Sometimes we read data from the Kafka cluster and directly create our raw tables or your bronze layer

tables.

You metabolize them indirectly into the table.

That is an option.

Other option is that you write a small data ingestion application to connect to Kafka, pull the data

and dump it into your landing zone directory.

Both options are there.

Both options are good so you can dump into a directory.

And once you have data in your landing zone, you can write a separate job to read data from the landing

zone and create your proper bronze layer tables or your raw tables in the bronze layer.

And once you have data in your bronze layer, you are done with the Kafka.

Nothing else is required to work with the Kafka, and from here onwards, this bronze layer table or

your raw data table becomes your source of truth and rest.

Entire processing can be done based on the this single source of truth your bronze layer tables.

But when your results are prepared, right when you are done with the processing, your results are

prepared.

You want to save it into some target directory or some target table.

So that is one option.

You can prepare your results, save it in tables so that your consumers can take data from your tables

and do whatever they want to do.

Data analysis or running machine learning and algorithms, everything they can do from taking data from

those tables.

You can also save it to some external systems or databases if your consumers desire that right.

If they feel more comfortable working with some existing database, you can also save them into database.

But in some cases, some consumers want to read data from a Kafka.

Right?

There might be some apps, there might be some other applications which want to read data in real time.

And they are designed to read data from Kafka, so you can produce the result and send the result again

back to the Kafka.

And they can take it from the Kafka.

So following some good design practices and in a data engineering project, Kafka may be sitting at

both the edges of your data engineering platform.

It may not find a place inside the data engineering platform, but it's a excellent data integration

tool.

So most likely your project will be having Kafka sitting at the data collection edge.

And also, it is possible that your project might need Kafka sitting at the data consumption edge,

right?

So Kafka, that's where Kafka fits into Lakehouse or Data Lake kind of projects.

So that's where it is.

And all this can be done in real time.

The data is being generated by these systems.

They can send data as soon as data is generated.

They can send it to Kafka.

Take out the network delay.

Right.

But data can reach from these systems to Kafka cluster.

Maybe in milliseconds or in seconds.

Right?

If your network is not too slow.

So data reaches to the Kafka in maybe in milliseconds, and then you can immediately write a real time

stream processing job here to consume data from the Kafka as soon as it arrives at the Kafka cluster.

So this pipeline or this job can also be a real time job, right?

So within milliseconds, data reaches to Kafka, and then maybe another few milliseconds you consume

it into your bronze table.

And all this also can be developed as a stream processing application.

So as soon as data arrives into your raw tables, you can have a stream processing application to pick

up immediately and take immediate action and produce the results.

Maybe in seconds, right?

So from end to end, everything can be done in seconds or maybe in minutes, right?

We see there are many hops here, and these systems read data and persist data at many places.

And then take the next step.

Right.

So maybe it is a good design for meeting requirements where you have seconds latency.

Right.

So if you have requirements where you need to produce the results in seconds or in minutes, right.

These systems are great.

You can use these systems.

And that's why we call near real time data processing for that.

Right.

Real time is in milliseconds and microseconds.

So these kind of architectures or these data engineering platforms may not be able to it's not hard

and fast.

They may be able to produce results in milliseconds, but most likely they may not be able to produce

results in milliseconds and microseconds.

So these are systems which are great for near real time, where your latency requirement is in seconds

and in minutes.

But in the beginning, I said Kafka is an excellent tool for data collection, data integration, and

it is also an excellent tool or excellent system for real time data processing.

So far, all the examples that we saw, we saw Kafka as a data integration or data collection tool.

Right.

And here also we are collecting data into Kafka.

Here also we are collecting results into the Kafka.

Right.

So it's all data collection.

So how Kafka helps us to develop a real time stream processing application.

So let's try to see that.

So Kafka also comes with a library called Kafka Streams Library.

We call it Kafka Streams API, which is a Java library.

And you can use Kafka Streams API to build scalable distributed stream processing applications.

So usually rest all remains the same, uh, up to collecting data into the Kafka cluster.

And once data is in Kafka cluster, you can adopt microservice architecture where you can build multiple

microservices.

And these microservices can consume data from Kafka and use Kafka Streams library to connect to Kafka,

read data from Kafka, and build stream processing applications.

Kafka streams library is designed with some ready made, ready to use methods and framework for developing

stream processing applications to take care of many streaming challenges.

Right, so you can use Kafka Streams library to build a microservice architecture, which is processing

Kafka data in real time.

Right.

And these can also produce data to some databases or tables if you want to.

They can also send data to a Kafka cluster.

If your consumer wants to consume from Kafka.

They can also use APIs to directly write to or send data to the consumers, notify them and all that.

So it's very flexible.

So this is another architecture approach which might be able to help you take actions on data in milliseconds

and microseconds.

So from end to end everything it is possible.

With a careful design and good scalability feature adoption, it is possible to capture data, process

it in real time and produce results or take actions in maybe in seconds, milliseconds, or even in

microseconds in some cases.

So we have two architecture options, right?

This is one architecture option, which is great for near real time.

It comes with its own advantages because you have there you can mix in batch and streaming.

In this architecture it's much more flexible, but you may be able to handle near real time requirements

with.

This architecture.

But if your requirement is real time in milliseconds and microseconds, maybe this is an approach.

And this is not a kind of data engineering platform.

It's it's more of a, uh, event streaming solution or event driven architecture which follows microservice

architecture pattern to design the solution.

But this is also highly scalable and used in a lot of cases and scenarios.

So I leave you here because this is what we are learning.

And this course is all about data engineering.

So we leave you here and in following lectures we will work on these kind of architectures.

That's all for this lecture.

See you again.

Keep learning and keep growing.
