Welcome back.

In this lecture we will learn to create a Kafka consumer.

But we will not go and create a Python Kafka consumer or in any other language.

We are learning a spark, right.

So in this lecture we will learn to create a spark consumer.

So we will be using spark APIs to connect to a Kafka cluster and read data from a Kafka topic, bring

it into spark and take necessary actions, whatever we want to do with that data.

So let's start.

So let me go to my Databricks community.

Come to work spaces.

Come to my home directory.

My project folder.

And create a new notebook.

Let's call it.

Kafka.

Consumer.

Let me zoom it a little bit.

For creating a Kafka producer, we needed to install a Kafka client library, right?

Similarly, for creating a Kafka consumer, also you need a API.

You need to install the client API in your application.

But in this case, we will be using spark Right to connect to Kafka and consume data from the Kafka.

So for that we will need a spark to Kafka connector.

So do we have a spark to Kafka Connector?

Let's go and do some Google, right?

A Spark Kafka connector and you will see structured Streaming plus Kafka integration guide.

And here you will see some details about the connector available.

So group ID is Apache Spark and the artifact ID is spark.

Kafka zero ten for Scala 2.12.

And this is your version.

So.

We take these details and based on these details, we will install the connector in our cluster.

So come back to your application.

Let me go to my cluster.

So here is my cluster.

Go to cluster, go to libraries and install new.

And we want to use Maven coordinates for installing the connector.

Let me paste the Maven coordinates here.

So what do we want to do.

Install Spark Kafka connector which comes from Apache Spark Colon.

This is your artifact ID.

Write spark SQL Kafka zero 10 to 12 and this is your version.

And just hit the install, and within a minute it will be installed on your cluster.

Once it is installed, you can start using it.

Great.

So it's done.

Let me close all this and come back to my.

Main notebook.

So we already learned a few things while creating Kafka producer that for connecting to Kafka cluster.

Whether you want to produce data there, or you want to consume data from a Kafka cluster, you will

need some additional details.

So what?

All we need.

We need bootstrap server.

We need security protocol.

We need username and password.

Right.

So at least these four things we will need.

So let me define some variables here in the first cell.

And we will.

Use it later.

So this is my bootstrap server.

We already copied it in the earlier lecture, so I'm just reusing the same.

And next thing is JS module.

While connecting Kafka from a python producer.

You don't need this js module, but we will be connecting from the spark to the Kafka cluster.

Behind the scene is Park runs on a JVM, right?

So for for a JVM we need to configure this JS module.

It's kind of a mandatory thing for spark application to connect to the Kafka cluster.

So all the security like your security protocol, your username, your password, all those are handled

by the jazz module in the JVM.

And since spark is runs on a JVM, we need to configure the jazz module.

So we configure the jazz module and then what else you need.

You need your.

Cluster API key.

And cluster API secret.

You call it username and call this one as password.

Or you call them cluster API key or cluster API secret meaning is same.

Names are not important.

So we are now prepared.

We have bootstrap server defined here js module.

We defined a variable for that right and cluster API key we have we have the secret.

Now what.

Simple.

Let's start writing code for connecting to Kafka.

Read data from Kafka and then do whatever we want.

So in spark, whatever we do, wherever from wherever we want to read data, we create a data frame

out of it, right?

So we'll create a data frame and write code to create a data frame from Kafka.

We will write inside this.

So let's not directly jump into Spark Streaming.

Right.

Let's learn first that how to connect to Kafka and read data from Kafka using spark standard read approach

which is batch processing approach.

So let me write spark dot read.

We are not going to use read stream for now.

We will learn how to use it later.

But for now, for the first consumer, let's use spark dot read and what is next?

You already learned about that, right?

You need to tell the format which tells which connector do you want to use for reading data from a source?

We want to use Spark Kafka Connector.

So the format is Kafka, and that's all.

With that little information, your spark data frame reader will be able to figure out which connector

do we want to use and load that connector.

And then maybe we need to configure the connector.

So options a bunch of options are required.

The first option that we want to give is Kafka Bootstrap Server.

So let me quickly type in all those things to save some time.

So Kafka bootstrap server.

And the value comes from this variable that we defined here.

Bootstrap server.

Correct.

Then we need some more options.

Let me paste those things also.

So Kafka security protocol, same SSL that we used while creating the producer Kafka SSL mechanism.

Same.

The value is plain.

Since we are using JVM to connect to Kafka, you need to provide this Kafka SSL config, right?

And that's why we define jazz module name here.

So jazz config is made up of jazz module.

And then you pass required parameters username which is the API key that we defined here.

And password equal to the cluster API secret which we defined here.

And at the end you will have a semicolon.

And that's how your jazz config string is made.

So we have the security credentials configured.

What else do you want.

Right.

We are now ready to.

Read data from Kafka because all the credentials credential details are already given.

So next option is to tell which topic do you want to read from.

So let me type in that.

So option we call subscribe.

Subscribe to our topic name which is.

Invoices in our case.

Right.

So we want to subscribe from the invoices topic.

It's it doesn't go like a topic, option, topic then invoices.

It goes as a subscribe.

That's all.

We are done with all the information.

Just hit the load method and you are done.

And here is my code.

Um, we do a little formatting here, that's all.

So spark dot read connect to Kafka because that's the Kafka.

Connector we want to use provide all the necessary options to connect to the Kafka for the connector,

so we provide the connection details like where is the Kafka hosted my bootstrap server host name,

and then we provide all the security configurations, credentials and all that, and then tell which

topic we want to subscribe so or which topic we want to read data from and then load.

That's all we are done.

And once you are done with that you will get the data frame, whatever data.

We sent ten records in the earlier lecture.

Right.

So you can display that data.

Display.

For the simplicity.

Let's display it and we will learn how to start processing it with proper example.

So I'm ready to execute it.

Let's connect to my cluster and run all.

Let's see what happens.

Waiting to run.

So this is done.

Now it's executing this.

This is also done.

Let's see what display gives.

Pulling in data from Kafka looks like.

Okay, so we got the result.

And what do you see.

Key which looks like junk character.

But that we know that when we receive data from Kafka.

Key and value are always binary right.

We sent a string key.

We sent a string value to the Kafka topic.

But the producer, before sending it will serialize it as a binary string and send it to Kafka.

And Kafka doesn't do any conversion.

The cluster or the broker will take whatever it received.

It received a binary key binary value, which is serialized as a binary value, as a byte code will

save it as it is.

And when you as a consumer want to read it back, it will give you as it is.

It will give you binary values itself, the byte code.

So that's what you see here.

Key is looks like junk because it's binary byte code value also looks like junk.

It's because it's binary byte code.

And then you have other details.

So which topic this record is coming from?

From which partition that record is coming from.

What is the offset of that record in this partition.

Right.

And then what was the timestamp of the message?

Right.

Remember we learned that there are three very critical parts of a Kafka message or a Kafka data packet.

One is key, another one is value and the third one is timestamp.

So what is the timestamp when this message was sent.

And we also have a timestamp type.

So Kafka comes with the at least two different timestamp types.

One is when the message was created by the producer, right.

And then second time stamp type is when the message was received at the Kafka cluster.

At the broker, when it was received at the Kafka cluster.

So the best time is to capture when the message was actually created, right?

We call it event time when the event actually occurred.

So by default, the producer API will put a time stamp on each message.

You can also set it, but if you are not setting it by default, producer will put a time stamp.

So timestamp type zero is represents that the.

Your time stamp.

Whatever time stamp you see here is event time.

The time when this message was created, at the time when producers send it, not when it was received

at the Kafka cluster.

Right.

And if timestamp type is one, then you will.

Whatever time you see here will be the time when message was received at the Kafka cluster.

The best is to have timestamp type zero, which is event time.

So this is the time when the message was created, right?

And we see a bunch of rows here 30 rows more than ten.

I sent ten messages today in the example, but you can see more than that because I sent some messages

earlier also.

But that's fine.

So what are the time stamps?

Let's sort it by timestamp so you will see.

Some messages were sent on 21st October.

Right.

These are the messages I sent yesterday.

Just to make sure we have data for multiple dates.

Do some experiments with the time stamp and then starting from 22nd.

These are the messages right.

So 1234, maybe five, six, seven, eight nine and ten.

So these ten messages we send today.

So those are the ten messages right.

So we got everything whatever was sent to the Kafka topic, whether it was sent yesterday or whether

it was sent today.

So that's all for this lecture.

The objective of this lecture was to help you understand how to connect to Kafka and how to consume

data from Kafka.

We'll learn that, right?

And we'll learn to print that data.

Also.

Key value.

We are not able to understand or interpret the key value here, but that's fine.

We will learn that how to convert it back from binary or byte code.

How do we convert it back to a string?

Because we sent a string and we want to see a string here.

Right?

So all that we will continue learning.

But hope you learned how to connect to Kafka and how to read data from Kafka.

See you again.

Keep learning and keep growing.
