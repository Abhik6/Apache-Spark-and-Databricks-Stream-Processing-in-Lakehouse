Welcome back.

In this lecture we will learn about the watermark and state store cleanup.

Simple idea, simple concept.

But we will start from the earlier example and add the new concept about watermark and state store cleanup.

So let's start.

Let us start our discussion with the earlier example.

In the previous lecture we created Simple Spark streaming job, which reads from the Kafka table, creates

some aggregates windowing aggregates for 15 minute time window and saves that aggregate into a target

table in the trade summary table.

And we saw that this is how the input looks.

Input comes with the created time stamp, which is a time.

And then in the Kafka table we have the raw input.

And this is how our trade summary table looks like the final result.

And we saw that in the final result we are creating four windows for a one hour duration based on our

sample records.

Right.

We are creating ten two 1015 one window, 1015 two 1030, 1032 1045 and 1045 to 11.

So we are creating four window and calculating total by and total sell for those four windows.

So let's go back and try to understand how Spark Streaming will process this, how this job works behind

the scene.

Right.

So let's come to the scenario.

We assume that this is our spark structured streaming job that we have created in the earlier lecture.

And in our test suit, we tested three iterations.

In the first iteration, we ingested two records and processed them.

Then in the second iteration, we again ingested two records, processed it, and in the third iteration,

we again inserted two more records and processed it.

So let's see how behind the scene, how that processing worked.

And we will start understanding the concept of watermarking from there.

So let's assume this is your first iteration right.

So this is your spark structured streaming job.

First Micro-batch is triggered because we have two input records available in the source table.

So how is spark structured streaming will work.

It will look at these two records.

Right.

And since we are doing windowing aggregate based on the created time.

So it will look at the created time field for the input record.

So we have one record coming at 1005 right.

And another record coming at 1012.

So both of these records will fall into a single window between 10 to 1015.

Right.

So spark is structured is streaming will identify that both of these records fall into a window of 10

to 1015.

Because we have configured a spark structured streaming to create 15 minute windows.

And the minimum time that we have is 1005.

So it will smartly figure out that we should create a window for 10 to 1015 and put this record into

the first window.

Similarly, the second record is 1012.

So that also goes into the first window itself.

Same window.

So what is spark structure streaming will do.

It will create a window.

Window start time will be 10 to 1015 and place both the records into the same window.

It's behind the scene.

It's creating a window for calculating the aggregates for these two records.

In this group, right window is nothing but a grouping column.

So in this group.

So once this is done then spark will calculate the aggregate for this aggregate is from 10 to 1015.

Total sell by amount is 800 right.

Both are the buy and 300 plus 500 total by amount is 800.

This goes as an output to the sink right.

The right stream will produce this output, but behind the scene.

Same thing.

Also goes into the state store right for future use.

So this window detail window start end and the total aggregate calculated in the first micro batch for

this window will be kept in the state store for future use.

And that's all your first iteration or first Micro-batch ends here.

Now let's see what happens in the second Micro-batch.

Right?

So let's assume you have a second micro batch where you ingested two more records.

Right.

And state is store already contains the information about whatever happened in the first micro batch.

Right.

So this detail are already kept in the state store.

Now look at these two records in the second micro batch.

In the second micro batch we got one record which happened at 1020, and one more record which happened

at 1040.

So what should be the window for 1020 record 1015 to 1030.

Right.

So this record becomes part of a new window starting from 1015 to 1030.

Why new window?

Because there is no existing window in the state store for 1015 to 1030.

So what is spark will do?

It will create one window which is 1015 to 1030 and put this record into this one window.

Right.

So this record comes into this window for aggregation.

Similarly the second record is 1040.

So this also needs one new window right.

That window should be 1030 to 1045.

Right.

So it will also create one more window 1030 to 1045.

And put this record into the other window.

Now these are aggregation windows right.

So for 1015 to 1030, we got only one record in this iteration.

And for 1030 to 1045 we got one more record in this iteration.

So aggregation for both the windows will be done.

And then finally spark will produce output for both the windows.

So both the this window has got one record for 600 for 1015 to 1030 total by is 600.

This is the output of this aggregation.

And this is the second aggregation for second group.

This this group is 1030 to 1045.

Only one record.

So this 900 goes as the total by for this window.

So two outputs will be produced in the second micro batch.

Because the data the created time belongs to two different windows.

So spark structure streaming will create two new windows produce the output.

And both of these will also go into the state store for future use.

So earlier we had 110 to 1015 window already there in the state store.

Two new windows we created in this iteration.

So those two new windows will also go into the state store from 1015 to 1030, 1030 to 1045.

And their aggregates will be also kept in the state store for future use.

And that's all your second micro-batch ends here.

Now let's come to the third micro batch.

In third micro batch we again ingested two more records.

So how the third micro batch is starting position look like it will look like this.

You have two inputs right.

The first input is 1025.

The second input is 1048.

And we have three states or three windows in the state store.

Now spark Structure Streaming will look at these records.

So it will look at the first record which is 1025.

Which window should it belong?

It belongs to 1015 to 1030 window.

Right.

Is that.

Window.

You mean do we already have ten, 15 to 1030 window created earlier?

Yes.

We have this 1015 to 1030 window already created earlier.

So what it will do, it will pull this 1015 to 1030 window information from the state store and create

one new aggregation window.

Right.

And that aggregation window collects detail from the state store 1015 to 1030.

Earlier aggregate is 600.

Now we got one more record at 1025.

And this is a cell operation for 400.

Right.

So it will pull up this record and recalculate the aggregate.

We are simply doing some.

So 600 by plus 400 cell.

So this zero will be updated as 400 cell as the result.

Right.

And one very critical thing we should note here that this record 1025 record is late.

It is arriving late.

Why it is late?

Because this record is 1025 and it is coming in the third iteration.

We already created a window from 1030 to 1045.

Right.

And this guy belongs to 1015 to 1030 window.

So we in the previous Micro-batch itself, we received a record for this window.

Right.

So now once we received the record for this window, we are not expecting new records coming from the

previous windows.

Right.

It should be always new window.

But we got a late comer here.

We got a late record.

But that's fine.

Spark Structured streaming can handle late comers.

How it will handle it will pull the old window detail from the state store and that's what it is doing.

It is pulling the old window details from the state store, recalculating the aggregate for this window

because of late record, because late record changes the aggregate right.

Recalculating this aggregate here and then it will produce the new output for this window.

But what about this record?

This record is 1048.

Do we have window for this record.

No.

This record should belong to 1045 to 11.

Right.

And this is coming in the proper order.

So this is going to create a new window.

So what is spark structure streaming will do.

We'll create one more window, new window which is 1045 to 11 and put this record into that window for

aggregation.

So now we have two aggregates to be calculated two groups.

Right.

One is 1015 to 1030.

Another one is 1045 to 11.

So what is spark will do is spark will calculate aggregates for these.

So first it will calculate aggregate for this.

Right.

And that aggregate is now revised.

Earlier it was 600 by zero cell.

Now because of this late record it is 600 and cell is 400 taken from this record.

So this aggregate is done right.

And this is the new state for 1015 to 1030 window.

Right.

So it will update the state store also 1015 to 1030 window now contains the new state 600 by and 400

cell.

This guy is left.

This guy is also executed for producing the new aggregate for the new window.

Fourth window 1045 to 1011.

And cell it is a cell.

So 500 cell.

And this goes as an output.

Plus this also comes and sits into the state store for future use.

So that's how spark is structured is streaming will keep on calculating aggregates one after another.

Now let's come to the final learning that we want to take out from this discussion.

We know that these windows can be pulled from the state store for recalculating the aggregates.

Why?

Because these windows may get some late comers late records.

We saw the example here right.

This is 1025.

Guy came late.

This guy came after we created the 1030 to 1045 window.

And then this guy came which actually belongs to 1015 to 1030 window.

Right.

So this window calculation was already done earlier, but this guy came late and because it came late,

a spark needed to pull the old window, record this guy from the state store and recalculate the state

for the old window, produce the new output.

So that's a problem.

That's a small problem.

Why that's a problem?

Because a spark needs to keep all these states forever, because you may get some late record and it

may be required that we need to update the old state.

Right.

Old aggregation, so spark will not be able to do the cleanup of the state store.

So what we discussed earlier that if we are doing time based aggregates right.

This is time based aggregate.

Time bound aggregate.

So in the time bound aggregate spark should be able to do the cleanup.

Because once this window is calculated this window is closed.

We move to the next window.

Right.

The previous window has nothing to do in the future.

So spark should be able to clean this window from the state store.

But spark will not be able to do the cleanup because a late record might come right.

So even if we create a time bound aggregates, a spark will not be able to do the cleanup because of

the possibility of some records coming late.

Spark will have to keep that.

Right.

So how do we make sure that spark is able to do the cleanup for the state store?

There is only one way.

We have to tell spark that we are not expecting any late records, or we have to tell that we are expecting

a late record, but that late record should come within maybe ten minutes, 30 minutes, or maybe one

hour, right?

Or maybe one day.

Right.

So we will have to put a condition there that how long is spark can expect a late record and maintain

the state after that expiry date after that period.

So let's say I'm saying that okay.

In this scenario we want to configure spark structured streaming.

That late record can come maybe in one day.

Right.

So spark will keep all these states for one day.

And after 24 hours are past.

No late record received or even late record received within 24 hours.

A spark will update these windows, recalculate the aggregate for using the late record, but after

24 hours, spark Structure streaming will know that.

Now we don't expect any late record that expiry date or expiry time is past right?

So now it can clean up the states to remove these windows expired windows from the state store.

We don't expect late record after that, but what if still we get a late record right?

So I set one day for this as the expiry.

And by the way that expiry is known as watermark right.

So in technical term we call it watermark.

So let's say for this example we are setting a watermark of one day 24 hours.

So if this window once created and then we have created more windows and the timing is past more than

24 hours.

Right.

And spark will know that okay, this window was created 24 hours ago.

We have new windows, the latest window.

And this window difference is more than 24 hours.

So we don't expect any late comer in this window so it can clean it up.

So that's the watermark right.

So we can set watermark telling that spark that this is the watermark seven hours one hour or half an

hour 15 minute.

Whatever watermark we want we can set a watermark and spark will know okay.

After that watermark we don't expect new records.

So I will clean up the state store for old windows.

But what if a record arrives?

Even if after the watermark, we estimated that, okay, a late record can come within one day, what

if it comes in the next day?

Spark will ignore that record.

It will ignore it, leave it, do any no calculation because that window is already closed and that

window watermark also expired.

And possibly spark has done the cleanup from the state store.

So there is no way to update that.

So spark will ignore that, right?

So we had a lot of discussion.

Let me summarize it.

We can calculate time bound aggregates like this right.

But spark will still not do the state store cleanup because it expects late records.

Right.

So it will keep the state to handle the late arriving records.

We want to make sure that spark is doing the state store cleanup.

Then we have to set a watermark.

Watermark is the duration within which we expect a late record to come.

And after that duration, we don't expect late record to arrive.

Even if it arrives, we want to ignore it.

We don't want to incorporate that record.

We don't care about too late people.

Right.

So that's what the watermark means.

Now the next question comes here.

What is watermark and how to decide your watermark.

So I hope you understood.

What is watermark?

Watermark is the duration.

How long do you want to wait for the late records?

That's the watermark.

So we can set watermark whatever time we want.

But how do we decide your watermark?

It should be one day, or it should be one hour, or it should be 15 months.

It should be one month.

What should be my watermark to decide these watermarks?

Or for your scenario to decide watermark, you have to think discuss with your business team what is

the maximum possible delay we can expect.

So some estimation you have to do.

Let's say your estimate is okay.

One day is the maximum.

Here.

Our records cannot be delayed by more than one day right.

So you can set your watermark equal to maximum possible delay.

Right.

Or alternatively the second approach is when the late comer records are not valuable for us.

Right.

So let's say people are saying we are calculating daily reports and by the end of the day, whatever

report is generated is final tomorrow.

We don't care about the previous day's report.

We care about the today's report.

Right.

So even if yesterday's record is arriving today, we don't care about it because we don't look at the

old report.

We look at the today's report.

So it's not relevant on that day.

So you can set your watermark when your record becomes irrelevant or it doesn't have any value.

That's how we decide.

So let's try to understand it with an example.

So we in this example we wanted to create a daily report like this where one line represents total by

another line represents total sell.

And this line represents the difference between buy and sell.

This is the amount that we need to collect or pay in the settlement.

Right.

And since it is a stock market trading report, it's a daily trade report day trade report.

For example stock market transaction starts from nine.

So maybe first window will be 9 to 915.

So it starts from 915.

And let's assume it's 1530 is the end time.

After that the stock market closed right.

So no transaction will happen after 1530.

Right.

So this report is valid from 9 to 1530 right after 1530.

We don't have any value for this report or maybe max by the end of the day.

Next day we will look at the daily trade for the next day.

Right.

So what is the window?

This entire report is valid from nine to or 915 to 1530, maybe possibly six hours, five hours, whatever

duration is.

Right.

So six hours is the maximum duration we can expect a late record.

It is possible that some transaction happened at maybe ten, but it is arriving late by maybe 1550,

so that's fine.

We want to recalculate the sums at the ten and adjust this chart.

Refresh this chart.

But after 330 or maybe after four, we don't care.

We don't look at this report.

It's a real time report so we don't care about it.

So if record is coming more than six hours late we don't care about it.

So your watermark could be six hours.

Another scenario is let's say your management or whoever is looking at this report, they say okay,

we need 99.99% accuracy.

We don't want 100% accuracy in this report.

This is just for a tracking purpose or to get a sense of how trades are going and how my settlement

amount is moving, right, because this is the amount that we'll have to actually settle.

So 99.99% accuracy is more than enough for us.

Right.

And then we do an analysis on the data ingestion and data arrival.

And we realized, okay, 99.99% records are received within 30 minutes.

Right.

That's how this 30 minute number comes.

So within 30 minutes, whatever records are received, if we do the calculation on that, 99.99% of

the records received within 30 minutes and maybe 0.001% records are arrived even more than 30 minutes.

So if we take a watermark of 30 minutes, we can still achieve 99.99% accuracy.

So in that case, my watermark could be 30 minutes.

Right?

So these are the approaches how we decide how what should be the watermark.

Right.

So that's what it is.

So let's assume for this example our watermark is 30 minutes.

We have implemented this example right.

This scenario we coded earlier.

So watermark is 30 minutes.

Next point how do we implement the watermark.

Right.

How do we code the watermark.

How do we configure the watermark.

So let's go to our code again and modify it to add 30 minute watermark to that example.

So let's do that.

So here I am on my work space.

Let's go to the example that we created earlier.

So tumbling time window.

This is the example that we created earlier.

Let me go to that notebook.

Okay so here is the notebook.

So where we are calculating aggregate.

Let's try to find that.

So get aggregate is the method which is calculating the aggregate.

And it takes trades df dataframe and on trades df.

We are doing group by this window.

It is a 15 minute window.

And then we are aggregating for some of by and some of cell.

So watermark should be implemented just before the group by right.

So you are doing group by on a window just before that group by you should implement the watermark.

So let me make some space here and how to implement watermark.

The function name is with watermark.

So with.

With watermark and with watermark takes two parameters.

The first parameter is the timestamp column on which time stamp you are calculating the window, right?

So we are calculating our window on the created time, right?

So same timestamp column should be given, right?

Make sure that your window.

A time column and your watermark.

Time columns are same.

Both should be same, and with watermark is called before the group by.

It should not be called after the group by.

That's the first column.

And second parameter is the amount of the watermark right.

So we want to implement 30 minutes.

Right.

So just say 30 minutes.

I guess minute or minutes.

Both.

Way it works.

So you are writing 30 minute or 30 minutes.

Both should work.

Same for here also.

So that's all we are done with implementing with watermark.

Now we know that whenever a window is older than 30 minutes, right?

Uh, spark is now allowed to clean up that window from the state store.

It can remove it from the state store, but when a spark structured streaming will do that cleanup,

that's on the structured streaming itself, right?

It may clean up immediately.

It may clean up later.

It will find the right time to do the cleanup from the state store, and it will keep on cleaning.

We don't have to worry too much about those states.

We we now define 30 minutes, watermark and then spark structure.

Streaming will take care of cleaning all the windows, but we know that within 30 minutes window will

be cleaned.

So we successfully implemented a mechanism to restrict the size of the state store.

So I've made these changes.

Nothing else is required.

We already implemented this uh, once.

We already tested this once.

So maybe you can go and open the test suit for this example.

Connect with your cluster.

My cluster is still starting and rerun this scenario.

This test case and it should work.

I'm not going to run it again, but assuming that it will work.

That's all.

Thanks for your time.

And in this lecture we learned about the watermarked watermark is to define how late do you expect your

records to arrive.

And Spark Structured Streaming will use the watermark to do the state store cleanup when the window

expired, right when window passed the watermark duration.

After that, spark will start considering those states to be cleaned from the state store.

And if late record comes after cleaning the state store, spark will ignore that it will not take that

record, won't throw any error, but it will ignore simply ignore those records because they are too

late.

That's all.

Keep learning and keep growing.
