Welcome back.

In the previous lecture we learned about the streaming aggregates.

I talked about datastore and I talked about the complete output mode.

In this lecture we will take it to the next level.

We will learn to implement incremental aggregation in Spark Structured Streaming, and will also talk

about the update mode of the Spark Structured streaming.

So let's start.

Okay, so let me go to my workspace and clone my streaming aggregates.

We want to clone this notebook because we want to modify this same example with the incremental aggregates.

So let me change the name.

And I will also clone the test suit for the streaming aggregates.

So this one.

Okay.

So let me open the streaming aggregates and then streaming aggregates.

Test for.

You don't need.

This will be 18.

Great.

So what do we want to change?

We implemented a version of this requirement where we are.

Overwriting the complete result, and every time we are recalculating or rewriting the table for the

entire data set.

So what do we want to change?

Our bronze layer remains as it is because everything is same.

We don't want to make any changes in the bronze layer, so this remains the same.

Basically we want to change the gold layer code.

And what do we want to implement.

We want to implement incremental aggregate right.

So how do we implement incremental processing or incremental aggregate in spark.

We already learned that.

We learned the basics of implementing incremental results or calculating incremental results using the

merge statement.

Right.

And we know that this two table is not going to work for us because two table will always either insert

the target table or insert overwrite the target table.

So but we want to implement merge.

So how do we implement merge.

We already learned that we can use the Swiss Army knife.

Think of spark structured streaming for each batch and implement the merge statement inside our for

each batch callback method.

So let's take that approach.

We will be using write stream.

Query name remains the same, checkpoint location remains the same.

And since we are going to implement incremental update, we can change our output mode to update mode.

That's the first thing I would want to do.

So what is Spark Structured Streaming will do?

When we change this output mode to update mode, it will look at the result right, the result set,

and whatever new results has arrived.

If there are new customers for which we are calculating total amount.

Are there are any customers for which total amount is modified updated?

Right.

So only those results, it will send in the output so that we can merge those results into the customer

rewards table.

That's what the update mode does.

So we learned about three output modes for the write stream the append mode, which will simply take

everything, every new data record and send it in the output stream complete mode, which will always

send the entire result to the output stream, and complete mode is mainly used for aggregation and for

stateful operations where we are using a state store, right?

Because the complete result comes from the state store, it cannot come without a status.

The third type of mode is our update mode.

So update mode is a little advanced compared to complete mode.

In complete mode, we always get the entire output every time.

Every Micro-batch will send the complete output right and in update mode in that particular micro batch

in one particular Micro-batch whatever results are updated or new results are added, only those records

will be sent to the output mode.

And this is best.

This.

This performs very well and we can use this update mode to implement for each batch and then implement

merge statement behind the scene.

So you already learned right?

So we don't want to do the to table instead of to table.

We want to implement for each batch.

And in the for each batch we have to provide a callback method which is spark structured streaming will

call.

So let's call it.

Upsert method.

Right?

And if we are giving for each batch and we remove two tables.

So we must provide some action here, right?

Otherwise this query will not start.

So in the for each batch we are calling Upsert method, we will define the Upsert method somewhere here.

And you already learned how to implement Upsert method.

Basically we want to implement merge statement in this Upsert method.

So what will happen is spark.

This data frame dot write stream will execute for a micro batch, calculate all the aggregates and since

we are using output mode.

So it will give us only those records from the result df that are new or the result or the totals are

modified, so only modified and new records we will get.

And then that data frame is passed to the Upsert method.

And in the Upsert method we will implement a merge statement using when matched and when not matched.

When not matched, we will insert.

That means it was a new record.

When matched, we will update the result in our target table.

So I will not type in the this Upsert method because we already implemented one Upsert method in an

earlier example approach is same, so I'll just paste it here and explain it to you.

So this is our Upsert method.

Structure for the callback method is fixed.

It must have two parameters.

One is the result dataframe, another one is the batch id and we want to implement a merge statement

using SQL.

Because it's easy.

And since our source is this data frame and SQL requires a view or table.

So we take that data frame, use create or replace temp view and create a temporary view with this name.

Right.

And then we write our merge statement.

Merge into our target table is customer rewards alias T.

So T is our target table using customer rewards df temp view.

That's our view which will be our source.

So merge into this table using this table as a source and compare data for customer card number or customer

ID.

And if there is match and mismatch, that's what we write.

When matched, it means it's an existing record as sum total for existing customer is changed.

So we want to update the total.

So target dot total amount equal to source dot total amount.

And then total target dot total points equal to source dot total points.

That's what we want to do in the merge when the records are matched.

When not matched then insert.

All because it's a new customer who purchased something from us.

We got his total and then we insert it, that's all.

And finally we take out our spark session from this data frame.

This data frame is a special purpose data frame which is designed for the callback method.

The callback method always takes a data frame, which is packed with the spark session inside the data

frame itself.

So we take the data frame, go to DF, get the spark session, and then we can execute the spark SQL.

So we are executing our merge statement and that's all we are done.

It should work right.

So that's all we wanted to change.

And this is the best approach.

This is the best practice for implementing aggregate results in a stream processing mode, or in near

real time or in real time mode.

You should always prefer to use update mode.

Output mode should be update so that you get only those records which are modified or changed, or new

records, right, and implement a merge statement.

So you are not every time overwriting your table right?

Or you are not overwriting the result table every time, because that will be efficient.

And that's all we are done with the changes here.

I don't think there is any other change here.

What is next?

The next step is to implement test suit for this, right.

So let me go ahead and implement the test suit.

So here is my cloned notebook for the test suit for streaming aggregation.

So what do you think.

What do we need to change in the test suit.

Almost nothing right.

Clean test remains the same because example is same.

Ingest data will also remain the same.

Assert bronze will remain the same as gold.

Assertion also remains the same.

Wait for Microbatches.

We are sleeping for 30s.

Let me change it to 60s to be on safer side.

Sometimes when we are chaining multiple microbatches or multiple streams one after other in the same

cluster, it takes a little extra time, extra resources, because we are working on a single node compute

cluster.

So it might take a little extra time.

So I want to change it to 60s that wait time so that we are safe.

We don't get microbatches not yet started and we are trying to validate the result.

So run test remains same.

I want to implement one more best practice here before we start both our streams, Bronze Stream and

Gold Stream.

So we know that we are doing aggregation here.

And aggregation always goes to State Store, right?

Each micro batch will store its aggregated results in the state store, so that next micro batch can

take it from the state store and combine results for two micro batches.

Right.

So we will be using a state store.

A state store is little slow because it is stored in the distributed storage directory.

We we are storing.

Our state is in the checkpoint directory itself, right.

That's the default location.

But reading data from a state store and writing data into a state store, all that is a little time

consuming.

It requires IO.

It requires bringing data into memory.

All that operations are slow.

So spark has now implemented Rocksdb as a state store provider.

The default provider is.

A state store provider, which is a little slow.

They have implemented Rocksdb state store provider, which is a little better comparatively.

So it's a good practice to enable Rocksdb state store provider.

It should be by default enabled, but unfortunately that's not the case yet, so we will have to.

Set the configuration to change our state stored provided to Rocksdb state store provider as a best

practice.

Gives some performance boost to your streaming queries if you are using a stateful operation.

So how do we do it?

We can use spark Dconf dot set and this is the configuration.

Spark SQL is streaming state is stored provider class.

That's all.

So for using Rocksdb state store provider, this is the class name that we have to set.

This should have been done by default, but looks like they are still not ready to set it as a default

state store provider.

Maybe in the newer versions or in the future versions, you will see this as a default.

So that's all.

Then we start our bronze layer process and then gold layer process, ingest data and validate ingest

data and validate three iterations.

At the end we stop.

And then I have a code here to execute the test suite.

So let me connect to my cluster and run this notebook.

Hopefully it will work without any error.

So I'll pause the video and come back when this iteration is over.

So first iteration already passed.

We are waiting for the second iteration.

Second iteration also passed.

We are on the third iteration.

So third iteration also passed.

And that's all for this lecture.

In this lecture we learned implementing incremental aggregation in spark.

Structured streaming by default gives you an incremental read and incremental data processing.

But when you start implementing aggregations then the result set is always given as a complete result

set.

And we have an option to use complete output mode.

But that's not efficient.

We want to implement incremental aggregation.

We want to work only on those aggregates that are either modified or they are new.

Right.

And you can do that using the output mode as update.

And since two table doesn't implement a merge statement by default.

So we have to use for each batch for implementing the merge into our target table.

And that's how you can implement incremental aggregates in spark structured streaming.

Hope that made sense.

And that's all for this lecture.

In the next lecture we will learn further and I'll talk about windowing and a little bit more about

the data store and some problems associated with the state store.

See you again.

Keep learning and keep growing.
