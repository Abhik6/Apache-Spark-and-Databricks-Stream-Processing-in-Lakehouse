Welcome back.

In this lecture we will learn to implement sliding window aggregates.

In the earlier lectures, we already implemented a type of aggregation which is known as tumbling window

aggregates or fixed size window aggregates.

In this lecture, we will learn about a new type of windowing aggregate, which is known as sliding

window aggregates.

So let's start.

We learned about the event time and created tumbling window aggregates using the event time.

We also learned about the watermark and state cleanup.

In this video I'll talk about the sliding window aggregates and create one example to help you understand

the mechanics of implementing them.

So let's start.

Let me define a simple scenario and related requirements.

Suppose you are collecting readings from a sensor.

The sensor emits a reading at a regular interval which goes to a Kafka topic.

Each reading looks like the following Json message.

The key is the sensor ID created.

Time is your event time and the reading is the value.

This could be a patient monitoring system where the sensor is attached to a patient measuring vital

value.

It could be a sensor attached to an engine of a machine measuring some parameters.

Whatever it is, you are now asked to generate an output that looks like this.

The first two columns are the start and the end time.

The third column is the max reading value, which you received in the most recent 15 minutes.

What does it mean?

Let me explain.

Let's say you are monitoring patient temperature.

So you are asked to report the maximum temperature in the 15 minute window.

So when we say the max temperature is 36.5 at 10 a.m., it means you received some temperature readings

from 945 to 10, but the maximum observation was 36.5.

Makes sense.

However, you are also asked to report the outcome every five minutes.

Now let's think about the solution.

You will immediately start thinking in terms of a tumbling window of 15 minutes and a trigger time of

five minutes, right?

Your first window starts at 940 and ends at 955.

You got your readings and the maximum reading was 36.2.

So far, so good.

What is the second window?

It starts at 945 and ends at ten.

Wait a minute.

This is not a tumbling window.

Tumbling windows has got two properties.

Fixed size, but they are non-overlapping.

But in this case, windows are overlapping.

Let me show you a pictorial representation for the same.

Here it is.

So my first window starts at 940 and ends at 955.

The max reading in this window is 36.2.

Similarly, the second window starts at 945 and ends at ten.

The max reading in this window is 36.5.

Both the windows are of fixed size.

They are 15 minute windows, right?

But they are overlapping for ten minutes.

And such windows are known as sliding windows or sometimes hopping windows.

If you look at the first window, it starts at 940.

But the next window is also a 15 minute window, which slides or jumps by five minutes.

The next window again slides by another five minutes.

Make sense?

Sliding windows are an excellent tool for computing moving aggregates.

This example is asked to generate a moving maximum for a 15 minute window, continuously moving by five

minutes.

Such aggregates are also popular as sliding window aggregates.

So you can also read the requirement to compute the max reading in a 15 minute window, which is continuously

sliding by five minutes.

Make sense?

Great.

Now let me also add some events to this diagram.

Let's try to understand the sliding window aggregates using these eight events.

Now if you look at the first window boundary we received only one event in this window which becomes

the maximum.

Now look at the second window boundary.

You currently have two events in this window.

The maximum value is 36.5.

Right.

Similarly the third window has got three records and the maximum is 36.8.

How many records do we have for 1010 to 10?

25.

Window three events.

Right.

And what is the maximum 37.5.

You should also notice one more observation.

Each event could be part of more than one sliding window.

For example, 36.8 event is part of the three windows 950 to 10 five, then 955 to 1010, then 10 to

1015.

And this is different than the tumbling window in a tumbling window.

Each event could be part of one and only one window.

Why?

Because we do not have an overlap there.

However, in a sliding window, each event is most likely to participate in more than one window due

to the overlap.

Great.

Implementing a sliding window is exactly the same as the tumbling window.

All the learning about watermark and state store cleanup also applies in the same way.

Let me create the code for this requirement.

Okay, so before we start coding, let me explain the setup that we have for this example.

So assume we are capturing data from the sensor.

And it is coming to a Kafka topic.

And we already have one job which reads data from the Kafka topic and brings it into the Kafka table,

which is our raw layer table.

The topic data looks like this.

But when it comes to the raw layer table, the table structure looks like this.

So we have key value.

Key is the sensor ID and value contains the Json message for created time and the reading.

That's what we have.

Now what we want to do.

We want to create a job starting from the Kafka Bgit.

Calculate maximum temperature in most recent 15 minutes and save it to Sensor Summary table.

The sensor summary table should look like this.

Right?

So we have sensor ID, a window start and window end, and the maximum reading within this window.

That's what it looks like right.

So we will code for this application I'll create a Databricks notebook and we'll write code for this.

But how do we want to test that.

So for doing the testing or for writing an automated test suit I've prepared these input sets.

So we have eight input sets.

If we give these eight inputs calculate the aggregates moving aggregate or sliding window aggregate

15 minutes.

Sliding window aggregate every five minutes.

Right.

So that's what we want to do.

So if we do that we will on this input we will generate these 11 outputs.

So I have already prepared these expected output and saved it as a CSV file.

So that file is also included into your data set.

So we will use that data set file as our expected output will calculate the actual output based on these

inputs.

And then we will compare them to do the validation.

So that's the plan.

Let's get started with the coding.

So here I am on my Databricks workspace environment.

Let's go to workspace.

Come to my user home directory my project directory and create a new notebook.

Let's give a name to it.

Let's call it sliding window.

And yeah, we are ready to start coding.

So as I mentioned earlier, that tumbling window and sliding window are almost the same.

The only difference between tumbling window and sliding window is that next window time would be different.

Right.

So the only parameter that we need to specify additions to the tumbling window is what is the slide

interval.

That's all we want to specify.

So assuming that you already learned how to create tumbling window aggregates, let's assume we are

going to create a tumbling window aggregate for this scenario.

Right.

So I have already created the code I'll paste that here which is nothing but a tumbling window aggregate

code for tumbling window aggregate.

And I'll quickly walk you through.

And then we will transform this code or change this code into a sliding window aggregate.

So the class name is sliding aggregate.

We have a constructor which is same like earlier.

All we are doing is setting the base data directory.

Then I've created a get schema method which returns a schema right.

And we know we will be parsing a Json string into a struct type, and that Json comes with only two

fields created time and reading.

So I have defined a schema method for that purpose.

And then we will be reading data from our Kafka table.

The read bronze is super simple.

Just one line, few words, a spark read stream table and table name.

And then after reading data, we will be transforming that data, putting a proper structure on the

data so that we can apply aggregates or we can work with that.

So get sensor data function or method will do the job which will receive raw data from Kafka.

Dessert table.

So that DataFrame comes here as a Kafka.

And on that Kafka df we will apply.

Some transformation.

Transformation is also simple.

What we want to do take Kafka DF dot key cast is as a string and give an alias as sensor ID.

So we will be creating key which is sensor ID and then what we want to do take Kafka DF dot value,

cast it as a string, and then apply a schema on that and use from Json so that Json is converted into

a struct and again give a value as alias.

So we will get two fields key and value.

Key will be string.

Value will be a struct of two fields of this schema.

And then we want to do a select star on that.

So we take the key and on the value dot star so that all the fields come out from the struct into the

main dataframe.

And we create a proper dataframe.

But we will be calculating tumbling window or sliding window aggregates and windowing aggregates or

any kind of aggregates require a timestamp field.

We have the created time field, but that created time is a string.

So we want to convert it into a timestamp.

So I'm using with column transformation for converting the created time into a proper timestamp field.

That's all.

Finally we have the aggregate method get aggregate method, which will take this sensor data frame which

we created here, and apply the aggregates.

We know we already learned how to calculate aggregate.

So on this sensor, first thing I want to do is to put a watermark for 30 minutes.

So we created based on the created time.

We are putting a 30 minute watermark so that our window state or our state store is cleaned every 30

minutes.

So after 30 minutes, once the window is older than 30 minutes, then spark will be cleaning out those

so we can limit our state store size.

And then comes the grouping.

So group by we want to group by using sensor ID plus a 15 minute tumbling window.

Right.

So the first grouping column is the sensor ID sensor dot sensor ID the second grouping column is window.

So window we are using window function.

And window should be based on the created time.

And the size of the window should be 15 minutes.

And this is where we can decide whether we want to create a tumbling window or a sliding window.

If it is a sliding window, we can pass a third parameter in the window, and that third parameter is

the slide interval.

So we want to put a slide interval of.

Five minutes.

Right.

So window size is 15 minutes, but it should slide by five minutes.

So that's what we give here.

And that's all we are done with the sliding window aggregate.

What do we want to calculate.

So we are done with the grouping.

And then we want to calculate the max reading.

That's all.

And once the max reading is calculated we take four fields sensor ID window dot start, window dot end

and max reading.

That's all we are.

Aggregation is done.

Now.

We have saved results which will save the final result.

We are using complete output mode and the target table is sensor summary.

That's all.

Here is the process method.

So we read data using the read bronze into Kafka DF and pass that Kafka to get sensor DF, and then

pass that sensor DF into the get aggregate to get our results DF, and finally pass that result to the

save results.

Return the query.

That's all we are done right now.

What is the next?

The next step is to implement a test suit for this.

So let me go ahead and create a new notebook.

So go create a new notebook.

And I also want to open this in the separate tab.

Let's give a new name for this notebook.

That's all.

So test suit is also going to be similar to what we have already done in the earlier lectures.

Right.

So the first thing is to implement the import.

Right.

So if we imported our notebook and then let me copy paste a few things which are similar to earlier

test suits and whatever is different we will do it later.

So the class name is Sensor Summary Test suit.

I have a constructor which defines the base directory.

And then I have clean tests.

And in this example we are using Kafka table and sensor summary table.

Right.

So we clean both the tables, clean the base directories for both the tables.

And I'm assuming that I'm starting my application from the bronze layer table.

So bronze layer table should already exist.

So I'm creating that bronze layer table key and value.

Both are a string.

And then finally I'm cleaning the checkpoint directory.

Right.

So that's all the cleanup.

I have a wait for Micro-batch method which waits for 60s.

You have been using it now the main part of a test suit rest all remains the same.

The main part of any test suit is a assert method.

How do you want to assert?

How do you want to validate your results?

So I already mentioned that in this example we are taking a different strategy for validating the results.

So how we will validate the result?

I already prepared a result sample or expected output result file which is a CSV file.

So that file is already there.

What do we want to do?

We want to take the actual results from the summary table sensor summary table, which is my final output

table, and load the expected result from the test data.

Write the test results data.

We will create two data frames right and then compare both these data frames.

If both are same, we pass the validation right.

We don't want to validate one record or one by one.

One record right.

Record by record.

We want to validate the entire expected data frame with the actual data frame.

Compare it together at once.

Right.

And that's what is different in this.

So let's write a code for this.

So what I want to do.

So I want to create a method here.

The method name is assert sensor summary which doesn't take any parameter because we are going to load

and verify the entire data set the entire result set.

So print a message.

And then all we want to do is to do two three steps.

The first step is to read the actual result and create a data frame out of it.

So let me.

Copy paste code for reading the actual data frame.

So what we are doing is.

Spark table and sensor summary table.

So create a data frame of it.

Read that table and create a data frame and collect that data frame into a Python list.

Right.

So we will have actual result Python list where the actual result is is stored.

And then what is the next step?

Next step is to read the expected output data set from my test data directory and create the expected

result list in Python.

So let me copy paste the code for the same.

It is simple code.

I hope you can easily understand that.

So here it is.

Right?

So what do our what are we doing spark dot read format.

Because the expected result file that I have prepared is a CSV file.

And it comes with the header.

So I'm putting a header equal to true and loading data from this location in your base directory.

We already have datasets directory.

In that directory you will see a results uh directory.

And inside the results I have saved the sliding window result dot csv file, which is the expected outcome

for this example.

And we do the collect finally.

So that expected results comes into a Python list.

Right.

And that's all.

What is the last step.

Last step is to assert it.

And that's all we are done right.

So assert expected result equal to actual result test failed.

And um actual result is or actual uh uh.

Actual result is.

Actual result.

We can print that entire list.

And that's all done.

We are done.

So what is the last step?

Last step is to write your run tests method.

So let me write the run test.

It's going to be simple.

I really think that I'm going to do here.

You have already learned, but for completion.

Uh, so in the run test method, first thing we want to do is to clean our environment and then create

an instance of my this application that we created.

And simply from that instance, call the process method so that your application starts, your streaming

query starts in the background and we keep the streaming query.

Streaming query will do nothing unless until we produce input in our uh.

Kafka is a table, right?

So what do we want to do in the Kafka table?

Already prepared eight sample records, right.

So what I want to do insert those eight records into the Kafka table so that my streaming query can

pick up that.

How do you insert it?

I've written it here.

Insert statement is spark dot SQL insert into Kafka values and all the input eight input sets I'm inserting

from here.

That's all.

That's the approach.

Sometimes what happens, we want to implement a little larger data set.

Input data set I have eight input data set.

So I've prepared insert statement and I'm inserting it from here.

But let's say you want to prepare a data set of 10,000 records.

So 10,000 input records you have.

You might not find it helpful to write a Insert statement for 10,000 records in your code.

So what you can do, you can save those records into a CSV or Json or parquet data file, keep it somewhere

in your directory, and for inserting into the input table, you can load that file, bring it into

a data frame, and simply save that data frame into your input directory or input table.

So you may not want to write a huge Insert statement.

So same way that we are saving a expected output as a file and loading it for validation.

Expected input file also could be there in your directory somewhere.

If it is large input data set, load it from there, bring it into a spark data frame and insert it

into your input table.

And that's all.

So you can take that approach.

It's up to you how you want to produce the input for your testing.

Once the inputs are produced, we want to print some message and just stop the streaming query.

So that's all we are done.

So that's all my test suit is done.

Let's put a few lines at the end so that we can execute it.

Connect to a cluster.

Cluster is a stopped.

So I'll have to create a cluster and then attach this notebook to the cluster and run it.

So let me create a new cluster I'll.

Let's take the latest runtime.

I'll pause the video here.

Create the cluster.

Once Create Cluster is created, I'll connect my notebook with the cluster, run it, and validate whether

everything is working fine or not.

So my cluster is up.

Now let me connect my notebook with the cluster.

Run all and you can wait for the outcome.

So it's running now.

The clean up method is started.

Let's wait for a few more minutes.

So the validation passed.

And that's all for this lecture.

See you again.

Keep learning and keep growing.
