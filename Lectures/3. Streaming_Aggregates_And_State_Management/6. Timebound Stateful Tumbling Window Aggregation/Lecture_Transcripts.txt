Welcome back.

In this lecture we will try to learn time bound aggregation.

Time bound aggregation is also known as time bound windowing aggregation, or time window aggregation.

So I'll explain you one scenario and we will try to develop one example using that scenario, and understand

how time bound aggregates or time window based aggregates are calculated.

So let's start.

So let's start with the simple question what is time bound aggregate.

So time bound aggregation is also popularly known as window aggregates or time window aggregates.

And the window is a time window.

And that time or the size of the window could be as small as microseconds, milliseconds, seconds,

minutes, hours, or even days.

So you can create a time window of let's say 30s, right?

Or even as like seven days or 30 days like that.

So that is what the window we are talking about.

It's a time window.

And to implement time based aggregation or time window aggregation, you must have a timestamp column

in your data set.

So whatever your data is which you are going to use to calculate some aggregates must have at least

one field to represent the time.

It must have at least one timestamp column.

So to understand this idea in a better way, let's try to frame a scenario and we will understand it

a little more.

So here is the scenario.

Let's assume you are working for a trading organization.

They are stockbrokers or they facilitate stock trades so their customers can use some trading terminals

or web based apps or some kind of application to buy and sell stocks.

Right?

So as soon as you, their customers are buying or selling a stock, that transaction, that trade transaction

is recorded in the Kafka cluster in a Kafka topic.

Right?

So if customer is buying something that immediately goes to Kafka topic and Kafka topic will capture

that.

Similarly, sell transaction is also being captured at the Kafka topic.

What we want to do.

We want to ingest data from the Kafka topic and bring it into our raw data layer.

We call it a raw data table named Kafka Bset.

Right.

So this simple job is super simple.

Read from Kafka topic and save into the Kafka table without any transformation.

Save the raw data as it is, that's all.

But there is a second part of the job.

We want to build a separate new job, which reads data from the Kafka table.

As soon as data enters into the Kafka table, this job will pick up that record or those transactions.

Apply some transformations and aggregation logic to calculate the trade summary, and then save that

trade summary into a table called Trade Summary table.

And we have our consumers or our customers.

They want to build a dashboard, a real time dashboard which reads data from the trade summary and prepares

this trade summary chart, which refreshes every 15 minutes.

So it's kind of near real time chart that they want to prepare.

And this chart should prepare refresh automatically after every 15 minutes.

So that's the requirement.

Now let's try to understand the data set or data model.

So the trade transaction whether it is buy or sell looks like this.

So this is a Json message or a Json record.

We have four fields here created time which is time date time stamp.

We have type which could be buy or sell.

We have amount which is the transaction value and broker code.

So these four things are there.

So when these terminals send data to Kafka topic it comes like a Json message.

When we read from Kafka topic and save into the Kafka table, the Kafka table structure looks like this.

So we have two fields key and value.

Key is the date.

So we take out the date from the created time itself and make it as a key.

And then value is the entire Json message.

Right.

So that's your Json message stored as a value.

So we know now what is there in the Kafka table.

Now we read data from Kafka table.

Prepare a trade summary which looks like this.

So trade summary is.

Four columns start and total by and total sell.

So this is start and end together represents a time frame or a time window.

Our customers want to refresh this report every 15 minutes.

So we created a time window of 15 minutes.

Right.

So if you look at this, the first window starts at ten right.

And ends at 1050.

The second window starts at 1015 and at 1030.

Third window starts at 1030 and ends at 1045.

Then 1045 is start and 11 end.

So within an hour of time we have four 15 minutes windows.

That's what it represent, right?

And for each window we have total buy.

So whatever buy transaction happened between 10 and 1015 are added to calculate the total buy.

And similarly total sell is a total amount for sell transactions within this time frame.

And this timeframe is based on the created time.

So for example, this record, if you look at this record, this happened on at 1005.

Right.

So 1005 means it.

Which window it will fall.

It will fall into the first window right between 10 to 1015.

So this 500 is added to the total buy of the first window.

That's how we will be calculating.

That's what the requirement is right.

So whatever.

Our customers have purchased.

That should be total buy.

If they have purchased that within the ten and 1015 window.

Right.

So that's what the windowing aggregate we want to calculate or time bound aggregate.

So this aggregate is valid for this window or this aggregate belongs to this window.

Right.

That's what we want to do.

And once we have this trade summary table here our consumers will be creating this kind of report.

So how do you think they will create this report.

Well let me explain that also.

This is how our trade summary table will look like right.

We have start end to represent the window total buy total sell.

And on top of this our customers can simply write this kind of SQL query, select end date, format

it with only hour and minute because they don't care about the date.

So for ten minute, call it at time and then take total buy as buy total sell as sell and they are doing

it windowing aggregate so over order by end so that these totals are calculated in a proper order right.

First one will be 1015, then 1030, then 1045.

It comes like an order.

That's all simple.

From this trade summary, the result of this query can be represented as a table, right?

The result of this query will look like this, where you will have at time 1015 total buy is 800, sell

is zero and net result is 800.

At 1030, total buy is 1400, sell is 400 and net is 1000.

So we keep on calculating the running total over a period of time and running total for the sell and

final net value.

That's what this query will do.

Or result of this same query can be represented as a table, but it can also be represented as a dashboard

or a chart as a visualization.

Right.

So this is a sample visualization that I've prepared for this data.

So your visualization will look like this.

This blue line represents the buy this green rain line sorry red line represents the sell.

And this green line is in between is the net value.

So.

Let's code it right.

But before we start coding, let me shrink the requirements so we can focus only on the windowing aggregate.

So in this requirement the first part is to ingest data from Kafka topic and save it into Kafka base

table.

We are not going to code this because that's simple.

We have already done similar kind of things in the earlier examples, so I'll not code this.

We will focus on this part.

We make an assumption that data is already coming into the Kafka base table, and we are writing this

job to calculate the trade summary.

So we will focus on this part only and code for only for this part and create the trade summary using

the Kafka table data.

That's all.

So let's start coding.

So I am in my work space.

Let me go to my project directory and create a new notebook.

Let's call it tumbling time window.

So what do we want to do?

We want to create a class for this requirement.

So.

Let's call it trade summary and we will most likely need a constructor.

So let me copy paste the constructor.

We have been doing that since a long time.

So.

Here is my constructor.

So far so good.

Simple.

And the constructor I've defined my base directory.

We want to read data from the Kafka table.

Right.

And Kafka a table has two columns only key and value.

The value column is a Json record, right?

It's a string which represents a Json message.

And we will need to parse the Json into a proper structure because that's a simple string, right?

We need to parse it or transform it into a proper record.

So for that we will need to define the schema for that Json message.

And we know there are four columns created time stamp or transaction type amount and the broker code.

So let's define one function which gives us the schema for the Json message so that we can call it whenever

wherever it is required to get the schema.

So I hope you already learned this part well.

And you know how to define a schema.

So I'll simply paste code here with a function which returns the schema the desired schema for the Json

message.

So the function name is get schema.

And we are using a struct type here to define the schema.

And we have four fields created time, type, amount and broker code.

So we have four struct fields and.

Created.

Time is a string.

Type is also a string.

Amount is double and broker code is also a string.

So that's all about the get schema.

Now what do we want to do?

We want to start by reading data from the Kafka table.

Right.

So how do you read data from the table in the streaming mode.

Simple spark dot read stream from a table.

So let me copy paste a simple function for that.

Read bronze.

So here is my code.

Read bronze and it will return a data frame after calling the spark dot read stream table.

Table name is b z.

Super simple right?

All this we have already learned in the earlier lecture.

So nothing new here.

What is the next step?

After reading the data from the table, we need to start doing our processing right.

And for doing that processing the entire processing is based on the Json event, the trade event, right,

which is a Json string.

So we will need one method which takes the value field from this Kafka table data frame and puts a proper

schema on top of that value.

Right.

So let's create one method for that.

Let's call it.

Get trade.

And this method will take one data frame, which should be Kafka DF.

Right.

So we will read data from Kafka table.

Create a data frame.

This read bronze will return the data frame and that data frame we will pass here as a Kafka DF.

And now we want to transform that data frame into a proper structure, put up a proper structure which

we can use to do the processing right.

So we will create a proper trade data frame and return it from here.

So let's.

Create a structure.

So I'll return a data frame.

And my transformation starts with the data frame.

And then maybe what do you want to do.

I want to use select method right.

And I'm interested into Kafka DF dot value field.

But I know that value field is a string right.

So I want to transform it into a proper structure.

And that string represents a Json string right.

So I can use from Json math.

Function on that Kafka dot on the value field, right?

And transform this value into a proper record.

But from Json takes a schema.

Json schema, which I can get from here, right?

So what do we do?

Take the Kafka dot value and use from Json.

Give the schema.

Convert it into a proper structure.

Write.

What is next?

Simple.

Give it an alias.

So.

Let's give it an alias and let's again call it value.

Right.

But the earlier value and this value is different.

The earlier value was a string.

This time the value is a struct right.

And once we are done with that then we want to separate out all the fields of the struct.

So let me do one more select.

And this time in this select I can do value dot star.

Uh, forgot the dot here.

So what is next by now?

What we have done, we have created a data frame of these four fields created time, type, amount and

broker code.

Right.

The value dot star will give me these four fields only.

That's what we wanted.

We wanted to create a proper trade data frame.

But there is a small problem here.

The created time is a string, but I want to convert it into a proper timestamp, right?

How to do it?

You can use the width column transformation, right?

Use with column transformation on created time and convert it into a proper time stamp.

We have learned that in your spark course.

So let me copy paste some code here.

And.

Uh.

That's all.

So create a time.

And I'm using two time stamp on the created time.

And this is the format for the date time stamp in my data.

So this will convert it into a proper time stamp.

But what is next.

So I have type and amount right.

One field is type which tells sell or buy.

Amount is the amount of the sell or buy, right and end of the day.

What do we want to do?

We want to sum up the sell and sum up the buy.

But the structure, the current structure doesn't look meaningful or easy to implement for the aggregation

and sum right.

We have two columns type and amount.

Type is sell.

Amount is the amount.

Then type is buy and amount is the amount.

So how do I calculate the sum.

So idea is to separate the sell and buy.

So what do we want to do.

I want to implement a with column here.

And create a column as by and then implement one more.

With column here and implement or create a column cell.

Right.

So what I want I want a simple data frame with three columns created time buy and sell.

What do I have?

I have a data frame with four column created time, type, amount and broker code.

Broker code may not be useful for me for my aggregation, so I'm leaving broker code here.

But how do I get the buy or sell from type and amount?

I want to convert type and amount combination into buy or sell two columns.

Simple if type is.

By take the amount as by if type is sell, take the amount as sell.

That's all.

How do you implement that?

We can implement it using case statement.

Right.

So let me copy paste the case case expression here and then it will make more sense.

So here is the case expression for buy.

And here is the case expression for sell.

Right.

So what do we want to do or what what we are doing here.

Case when type equal to buy then take the amount as the buy amount.

Else the buy amount is zero.

If the type is sell then take the amount as sell column.

Otherwise it is zero right.

Simple.

So that's a simple transformation that we use in SQL.

I hope you know already.

So that's all.

That's what I wanted to do.

I think I'm done here.

Uh, creating a proper trade record.

Let's do a little formatting, and then this function is.

I'm done with this function.

So get trade what it will do.

It will take the Kafka.

Restructure the dataset.

Create a proper DataFrame with three columns.

Create a time which is a time stamp data type.

Then by which will be the amount of buy and sell.

Right.

So for each record, either it is a buy or it is a sell.

Right.

So either we will have buy and sell will be zero.

And or if it is a sell amount then we will have sell and buy will be zero.

So we transform it into a simple plain dataframe now.

Aggregation is easy.

I can simply do some of buy and some of sell, right?

So that's why we created a proper dataframe.

That's the transformation we needed.

And get rid is doing that.

Okay, so I think I forgot to import the functions I'm using from Json here.

Right.

So we need to import the from Jason I'm using expr, so we should also import the expr.

What is next?

Once I have my trade data frame, we are left with the final step of calculating the aggregate right.

We want to calculate total by and total sum in 15 minute window right.

So let's function create a function for that and get.

Aggregate.

And this will be trade data frame.

Right.

So let's assume the result of get trade.

I'll pass here and do the aggregation and return the aggregation result.

So whatever I want to do I want to do inside this parentheses.

And my coding will start with the trade DF.

So trade off, we will start implementing the aggregation.

So what do we want to do?

We want to.

Aggregate means we want to calculate sum of buy and sell.

But what is the grouping column?

We want to group it in 15 minute windows.

Remember I showed you start and stop and start and end two columns.

And those two columns were representing 15 minute window.

Right.

So my group by column is a window of 15 minutes.

So let's do the group by.

And in the group by what do we want to do?

We want to group by a window of 15 minutes.

So how do we do that.

So we have a window function for that.

This function takes the field name which field you want to use for creating a window.

And this field must be a time stamp type.

So my field name is created time.

And what is the window size.

So my window size is 15 minutes.

That's all.

Maybe created.

Time will not work like this here.

So what I can do is I can do trade df dot create a time.

Right.

So what it will do, it will take the time stamp and create a 15 minute window based on that time stamp,

the created time field.

Right.

And that window will be like starting from 11 to 11, 1511 15 to 1130, 1130 to 1145.

Like that.

Right.

And this window function is magical.

It what all it takes is tell me the timestamp field and tell me the window size.

What is the size of your window?

If it is 15 minutes, it will create a window of 15 minutes and group your data by that window.

Once your data is grouped, what is next?

What do you want to do next?

Simple aggregate right?

So aggregation code is simple.

Let me copy paste the aggregates.

You already know how to aggregate it.

Right?

So, uh.

We'll use ag method ag, and in the AG we want to calculate the sum of by alias total by sum of cell

alias total cell.

And that's all we are done with that right.

So what will be the result data frame.

How would it look like when you do group by and aggregation.

The result is the grouping column plus the aggregation columns.

So what we will get we will get this.

Window as the first column.

Second column will be total by and third column will be.

Total cell.

That's the record set we are going to get.

But window is composite type.

It's a struct of two fields start and end.

Start is the window start timestamp and end.

Is the window end timestamp.

So start and end is there.

I don't want to take the window.

I want the start and end and total by and total cell.

So maybe we can do a select statement here.

Select this expression and in the select I want window dot start and I want.

Window dot end.

And what I want, I want total.

By.

And total sell.

That's all I want.

So we are done with this aggregate method.

Um.

That's all.

But don't forget to import your functions.

So I'm using window function here.

So we must import it I'm using some here.

So we must import this some also.

That's all we are done implementing all the methods.

Now the last step is to stitch all that together right.

So maybe we will create a process method.

Process method doesn't take anything.

What do we want to do in the process method?

The first is read the data.

Write read data from the bronze layer.

So we will call it Kafka df equal to.

Read from the bronze layer, we got the Kafka, and then we want to transform the Kafka into a proper

trade off.

So let's.

Call it DF.

And then.

Call the trade method and pass the Kafka inside the trade DF.

It will transform the Kafka DF give us the trade DF and finally we want to aggregate it right?

So let's call it.

Result.

Death that will come from the.

Aggregate.

And what do we want to pass?

We want to pass the.

Yeah that's all result calculated.

What is the last step?

Save the result into the target table.

So you can write code here.

Or you can define a method and call that method.

Let me copy paste the simple method you already learned about how to save the table into target table

target.

So I'll simply copy paste the method here.

Save results which takes the result data frame and print some message and we use the write stream.

Query name is straight summary.

We give the checkpoint allocation and we give the.

Use the complete mode for simplicity.

And we want to save it into the trade summary.

So that's all simple code.

And what do we want to do.

We want to call it from here.

Save results and pass the results.

Here.

That's all.

That's all.

We are done with the tumbling window example.

And hope you got what we are doing here.

So.

What we are doing here.

Let me quickly walk you through it.

So what we are doing, we are.

We created a get schema so we can define a schema and get it wherever we need.

And then defined read bronze method to read from the Kafka table, then pass that data frame to get

read.

And we do some necessary transformations here to take the Json value, transform it into proper columns.

And we are interested in three column data frame created time buy and sell so that we can simply calculate

the aggregate on the buy some of buy some of sell based on the created time.

And then we created the get aggregate.

So the all the learning is here right.

What do we want to do.

We want to create an aggregate a time bound aggregate a aggregate which is valid for 15 minutes.

Right.

And how do we calculate the time bound aggregate using the time window.

How do you create a time window using the window function?

A window function takes a timestamp field, which should be the event time, which should represent

when this record was created or when this event happened.

Right?

So this created time represents when the trade actually happened.

Right?

So based on that time it will create time window.

We are giving size of the window.

We want 15 minutes.

We gave 15 minutes.

We want seven days.

We can say seven days here instead of 15 minutes.

It will create a seven day window.

So what it will do it will create a window.

And if the record fits in the window, then it will group it based on the where the record fits right.

So for example, in this scenario it will create multiple 15 15 minutes window and look at the created

time.

And if the record fits into the first window, it will group it by the first window or fit it into the

second window if that time belongs to the second window.

Like that, your record is grouped by the window.

So in windowing aggregate window is nothing but simple a grouping column.

Think of it as simple as grouping column right.

So we group it by the time window and then calculate the aggregates, and then take the do a little

more transformation to take a start.

And total by and total cell.

That's all.

And finally we have the save method to save it.

And here I have a process method which saves the result.

And possibly we should.

Capture the streaming query and return the streaming query from here so that we can implement it.

We can stop it or manage it during the testing or whatever.

Right.

So that's all what is next?

Next step is to write a test suit for this.

Run it and test it.

So let's see how do we create a test suit for this example.

Or how can we test the scenario.

So let me.

Create a new notebook for writing the.

Test suit for this.

And.

Let me open this somewhere in a new tab.

Copy this guy's name.

And.

That's all.

And we want to import our test suit here.

Right.

So percent run.

And.

22.

Tumbling window.

Right.

That's all.

So we are ready to write our test suit.

We have done test suit many times.

So for the interest of time, I'll copy paste a few things.

So let's call it trade Summary test suit.

This is my constructor where I define my landing zone.

What do we do in the test suit?

The first step is to write a clean method or clean test method, which we will use to do the cleanup.

So let me copy paste the clean test method and simple what all things we have to do as a cleanup.

We are creating two tables here.

Bronze Kafka is at bronze layer table and trade summary.

So Kafka is at trade summary.

Drop these tables.

Make sure table directory is also deleted.

But we want to start our testing from the Kafka base, right?

We skipped the Kafka to Kafka table ingestion.

So for writing our test I'll have to create the Kafka table.

So we have a create table here so that at least our input table is pre created.

And we are deleting the checkpoint.

We are using only one single write stream.

So we have only one checkpoint used in our code.

So I'm cleaning up the checkpoint.

What is the next step.

Our next step is to add few more additional functions like wait for micro-batch.

So let me copy the wait for Micro-batch.

It's going to be same as like earlier example.

So wait for micro-batch sleep time.

30s.

That's all right.

Writing test suit.

The most important part is to design your assertion, right?

How do you want to assert the test results?

So for doing that assertion I have prepared some sample data set.

Right.

I've prepared just five records.

Let me show you that.

So here it is.

So I've prepared these input data sets.

123456.

We have six records here.

So six trade transactions.

Four of them are buy two or sell.

You have the amount right.

But look at the created time right.

They are well crafted on a timeline between 10 to 11.

Right.

So one hour window.

I want to test with this and simulate some scenarios.

So first record was.

Created at 1005.

Right?

This second one adds 10 or 12.

Third one at 1020 fourth one at 1040 fifth one at 1025.

So that's a little odd 512, 20, 40 and then 25.

That means this record should happen between 20 and 40, right.

But we are assuming that this record actually happened at 1025.

But it reached us a little late.

By the time 1025 record reaches our system at 1040, record already reached.

So this was a little slow, right?

And then we got one record at 1048.

So everything is happening in order in the same order.

We are getting data in the same order in each event executing or event happening.

Right.

But there is one outlier here which is like coming late.

So that's fine.

Spark structure is streaming can handle late arriving records also.

No problem.

Let it come late.

So we simulated a proper order plus a late scenario.

And then with these five records coming in the same order, arriving our system at our system in the

same order, what will be the final outcome.

So have also manually calculated the final outcome and kept it here.

And we will be writing our test case based on that.

So what is the final outcome in the one hour duration from 10 to 11, I expect four windows from 10

to 10, 15, ten, 15 to 30, 30 to 45, 45 to 11.

Right.

So I have created four windows.

What should be the total by for each window.

So look at the time this record 1005 and 1012.

Then 2040 2548.

So only these two records belong to the first window.

Right?

Ten five and 1012 is the only two records that should be part of 10 to 1015 windows.

So both are bye.

So 500, 300 total 800 is the bye for the first window.

Similarly you can calculate for the 1015 to 1030.

What are the records.

This guy 1020 and this late guy 1025.

Right.

These two guys should fall in the second window.

1015 to 1030 this guy is a bye 600.

So we have 600 here.

This guy is a cell 400.

So we have cell 400 here.

So that's how we calculate.

And 1030 to 1045 should be total should be 900 because we have 40.

And this guy is 48, right.

So only this guy will fall in that window.

So 900 and this guy will fall in the last window.

So 500 that's a sell.

So we see 500 here.

So that's my curated input and expected output.

Based on that we will implement our test cases.

So let's go back to our.

Code.

And let's try to write some assertion based on this.

Right.

So.

I want to design an assert method which takes a window time, right?

Maybe window start and end and expected.

Total and expected.

Uh, expected buy and expected sell.

Right.

And based on that, it will go and fetch the actual buy and sell for the given window.

And if expected values are matching, we pass the test case.

If expected values are not matching, we fail the test case.

So how do you do that?

Let's define a function.

Let's call it assert trade summary.

And what are the parameters?

Window start.

Window and.

Expected by and.

Expected sell.

Right.

And we will calculate the we will fetch the actual buy and actual sell from the final result table and

compare.

That's all simple.

Right.

So let me paste the code for that.

Here it is starting trade summary validation and we are executing a SQL.

What do we want to fetch.

Total buy.

Total sell from the trade summary where date start.

We do some formatting here because this start will be a string.

This end will be a string.

So we have to.

Convert the date in the table also as a string for comparison.

So where is start equal to start and equal to end?

That's all.

Collect.

Collect it into a Python list.

Write.

The collect will send it to the driver, which will be a python, and we will get a Python list.

Now what do we want to do.

We want to take out the total buy and total sell as our expected value.

So how to do that?

Simple.

From a list we will take it out.

So actual buy is from this result.

First row first column that's all.

And actual sell is from this result first row and second column.

So buy is the first column, sell is the second column.

I got the actual buy and actual sell.

Rest is super simple.

You implement the assert right.

So.

Asset expected by equal to actual by.

Pass the test case.

Else fail the test case with the actual by in the message and expected sell equal to actual sell if

passed.

If the match, great otherwise print the message test failed actual sell.

Is this match right?

That's all.

We are done with the assertion method.

What is next?

The next step is to run test method, right?

So let me paste few things from the run test method and then we will I'll keep on explaining and pasting

few other things.

So run tests is it starts with the cleaning right.

So we run the clean tests.

Then we create an instance of the trade summary.

This is my trade summary class that I created.

And then from the this trade summary is a stream I call the process method.

Process is my final method, right?

Which actually does everything.

So call the process method, get the query.

And then we want to start running our iteration.

So your stream is running right.

This your process your streaming process is already running.

Now what do we want to do.

Insert event wait for micro-batch to complete and then assert the result.

That's all we do.

So first iteration.

If you recall those six messages, I want to send first two messages, send it and then wait for Micro-batch

to complete.

And then we will assert the result.

So we will do.

One by one.

How do I send those messages?

My input table is Kafka table, right?

So I will simply insert those two records in my Kafka table.

And that's done.

Ingestion is done.

My query will pick up it from there.

So let's do that.

So insert into Kafka table values.

And these are the values first two messages.

Key is this.

And this is my Json record.

So 1005 event and 1012 event.

These two we are sending.

And these two will be good enough for me to validate 10 to 1015 windows total cell and total by right.

So let's wait for the Micro-batch to finish.

Wait for Micro-batch to pick up these two records and complete the processing.

And then I'll do the assertion.

And my window start is ten and window end is 1015.

My expected total is 800.

You already saw the PowerPoint slide where I've already done all that calculation.

My expected total by is 800.

Total cell should be zero with these two events ingested into the bronze layer.

That's all.

If this passes, my first iteration is done and my other iterations are also done in the same way.

So let me paste the rest of the code.

That's all.

So third event I'm and fourth event two events.

Again I'm sending in the next iteration.

So I'm sending 1020 and 1040 records.

Right.

Wait for 30s and then I'm asserting I this time I expect two windows to get updated right.

And 1015 to 1030 and 1030 to 1045.

Because I have record which belongs to 1015 to 1030 window.

And I also have a record which belongs to 1015, 1033 to 1045.

So I'm doing both asserting both the windows here, 1015 to 1030 window total by should be 610 30 to

1045.

Window total by should be 900 and the last one in the last event or iteration.

I'm again inserting two more records.

This time I'm inserting 1048 record and 1025 record.

This guy is coming late.

This should have come before 1045 record has arrived, but that's fine.

Spark structured streaming can handle native events, right?

So this guy will go back and update the result for the the second window.

Right.

So it's 1025.

So 1015 to 1030 window results will be updated.

We already calculated 1015 to 1030 window earlier because we got some records for that window.

The total by was 600 and total cell was zero.

But once we have this late record, that result should be updated.

That should result should be corrected.

So 1015 to 1030 window.

This is a cell type for 400.

So that windows result should now be updated with 600 as a bye and 400 as a cell.

Right.

So that's all.

That's all we are doing and we are done.

So maybe let's try to run it right.

Okay.

My cluster is not up, so let me.

Okay, so I'll pause the video and come back when my cluster is ready and run it and verify all the

test cases are passing.

So my cluster is now up.

Let me run this and let's see how it goes.

I'll pause the video once again and come back when this is complete.

So two of my iterations already passed.

It's working on the third iteration, but let me give you some additional bonus learning.

So let's see how can we create the dashboard in Databricks notebook itself.

So once this is done uh, all done.

Great.

So maybe this is the SQL query that I assume that my reporting team or my consumers will be using to

build the report.

So okay.

So let me.

Run that query and show you the results.

And hopefully we can create a visualization also.

Right.

So this is the tabular representation of the final result.

And this is what I expected right.

Maybe you it's already sorted.

But if you want you can sort it right in whatever order you want.

You want to see that visualization.

See that plus button?

Click here and add a visualization.

We wanted a line chart right?

So choose the chart type and x column.

What is my x column at the x axis we wanted time, right?

So let's take the time here.

And uh on the Y column, uh, let's disable aggregation because we are already doing that aggregation

on the y axis we wanted uh, by.

And sell.

And net all three.

Right.

And we don't want to use grouping and all that from the visualization.

But that's all you can see it on the x axis you have ten 1530 4511.

And on the y axis you see the sums right.

You have a little more flexibility here.

Maybe you can apply sort.

It is already sorted.

You can say show labels on the x axis.

So these labels are showing on y axis maybe.

You can change this level here.

So on the y axis you maybe you can say.

Amount on the x axis.

It is showing at time, so maybe I can change it.

Time on the series.

You want to do something or data labels.

Let's call show labels so you can see these numbers showing up here.

Simple.

But you have the facility to create visualization in the notebook itself.

Just hit the save and it will appear here.

Right.

So that's your visualization.

You can.

See how your output will be used by your consumers in your notebook itself.

That's all.

That's all for this lecture.

We will continue this discussion on the time window in the next lecture and learn something new about

the time window aggregates.

And we will talk about the treatment of the late comers or the late records in the coming lectures.

So see you again.

Keep learning and keep growing.
