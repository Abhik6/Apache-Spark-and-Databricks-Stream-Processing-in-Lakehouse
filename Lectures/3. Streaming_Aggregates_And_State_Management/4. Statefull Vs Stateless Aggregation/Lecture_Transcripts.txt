Welcome back.

So far in this course, you learn to create a streaming application.

Will learn to read the stream as an incremental data source, apply some processing, apply some transformations

and produce result and write the result into a streaming sync.

We also learned how to implement aggregations.

We calculated aggregates and we created a couple of examples for calculating incremental aggregates

also.

So far everything looks very simple straight forward right?

In fact it's part transformations are simple, super simple and straight forward.

If you already know a spark DataFrame API, if you have already worked with the DataFrame API, all

those skills knowledge is useful for creating a streaming application.

However, a Spark Structured streaming offers you some additional distinctive capabilities, and one

of those capabilities are stateful and stateless transformations.

So all those transformations that you learned in spark DataFrame API and spark programming can be classified

into stateless or stateful transformations.

So in this lecture we will try to understand the stateless and stateful transformation properly.

And we will see when to implement a stateless transformation.

When to implement a stateful transformation.

We will also talk a little bit about the state store and how do you manage the state store.

So let's start.

A spark.

Structured streaming transformations are broadly classified into two categories stateless transformations

and stateful transformations.

To understand the difference between these two types of transformations, you must clearly understand

what is a state or what is the state.

Right.

So let's try to understand that with an example.

We know that spark is structured.

Streaming runs as a series of micro batches.

Each micro batch will read some input data for that micro batch.

Do the processing produce the output data which goes to a sink?

That's how Spark structured streaming works.

We already implemented an example for calculating customer's total purchase and his total rewards.

So let's take that example here again and try to understand how it works.

So let's assume you got a one new customer.

He came to your store, purchased something.

You captured the invoice and that invoice is landed into your streaming source.

Now you have a streaming application which keeps on running as micro batches to calculate his total

purchases and total rewards.

So let's assume you got an invoice for the customer.

It looks like this.

Minimize the information here to just focus on few things right.

So invoice number one customer ID is 101.

And he purchased for $500 or rupees whatever.

Right.

So his purchase amount is 500.

This is your new input.

So your streaming job your reading stream will capture this input.

You are applying transformations.

And in the transformations you are calculating sum of amount and you are calculating, let's say 2%

of his total purchase amount as his rewards, right?

Or maybe some percentage of his total purchase amount as a reward points.

Right.

So your read stream will read this invoice transformation will calculate the aggregate, send the result

to right stream, and right stream will produce the output.

The output looks like this.

Right?

It is same as input, right?

Customer ID 101.

Total purchases 500.

Because his total purchase in this invoice was 500, so total purchase is 500.

This is new customer.

First time we are calculating his total purchase.

So total purchase is 500.

And then we calculate some percentage of the total purchase as rewards.

So his rewards is ten.

Output is done.

We know or we see only this.

We are reading input.

We are producing output.

But since you are implementing aggregation in real time aggregation in streaming application, streaming

application means you are reading using read stream and writing using right stream in between.

Whatever transformations are being applied are all streaming transformations.

So since you are doing an aggregate as an streaming transformation, behind the scene is.

Spark will also save this result into a state store.

So state store may look like this for the same customer customer ID 101.

His total purchase so far is 500, and his total rewards so far is ten.

This goes in the state store.

State store remains in the memory of the executor, but it is also saved in the checkpoint directory

in the same directory where you are creating your checkpoint for your streaming application.

This is also saved so that we don't lose the information right?

In case of failure or in case of restarts, the state is preserved, persisted permanently and your

application can reload it from the state store inside the checkpoint directory to the memory.

So that's all the first iteration is done.

First micro batch is done.

You got a new customer.

You calculated his total purchase, and behind the scene is spark.

Structured streaming saved that same information in the state store.

Now let's assume the customer comes back again.

Maybe next day, or maybe next week.

Or some day he comes back again and he makes another purchase.

Right?

This time the invoice number is two, customer ID is same 101 and he purchased for 300.

This time he purchased for 300.

Now we will read this invoice right read stream will read only this invoice.

He will not read his previous invoice because a spark structured streaming only reads the incremental

data, only the new data.

So it will read only this invoice.

You will again calculate the aggregate aggregate how much it will be 300.

Right.

So 300.

And then it goes to the right stream.

And right stream will produce the output.

But you will see output as total purchase 800 and reward point 16.

How come we have.

Only 300 worth of invoice in the input, but the output is showing total purchase 800.

That's the power of State Store.

So what is Spark structure streaming will do.

We'll calculate the total for the current Microbatch which comes as 300 plus.

It will go back in the state store search for the same customer 101 finds 500 is his total till the

previous microbatch right.

So it will sum.

It will add this 500 to the current iterations or current Microbatch total 300 total will be five 800

500.

This plus 300.

This total 800 and write stream output will show you 800 total purchase as 800.

Similarly, his rewards will be 16, maybe six, calculated as part of the current purchase ten he already

accumulated till the current microbatch and we can find it in the state store.

So ten plus 616 the output will be 16 and you get neat and clean result without worrying what is happening

behind the scenes.

That's the power of spark.

Structured streaming.

Stateful aggregation.

This kind of aggregation is known as stateful aggregation because behind the scene, spark is maintaining

a state, maintaining past results, and these results are across Microbatches right.

You calculated his total 500 for the first Microbatch, and in the second Microbatch you calculated

that customer's total as 300.

But how do we pass information across Microbatches?

That's where State Store is coming, right?

So 500 plus 300 800, we got the correct result.

Let's assume that customer comes back again and he created one more invoice.

He purchased something more.

This time invoice number is three but customer ID is same 101.

It's an existing customer.

And this time he is purchasing for 800.

What do you expect?

What will be the outcome of this time outcome of some right 800 for this iteration or for this micro

batch?

His total is 800 for this micro batch and 800.

He already accumulated total purchases up to 800 till the previous micro batch.

So 800 plus 800 it becomes 1600.

Similarly, his points for this current purchase comes to 1616.

Reward points were already accumulated.

We can find it in the state store and his total output will be 32 points.

So that's how Spark Structured Streaming works.

We already implemented this example in an earlier lecture.

Right.

So let me quickly show you the code and help you understand a few more things.

So here is my streaming incremental aggregates example that we created in the previous lecture.

Let's quickly scroll down and see where do we calculate the aggregate.

So we have a function here get aggregates.

This function is calculating the aggregate and how we are calculating aggregate invoices dot df dot

group by customer card number and then aggregate sum of total.

That's all.

That's all we are doing.

And what is this invoices DF if you look at the process method we are reading invoices from the bronze

layer, read bronze table and we know that it is Reed bronze table is using.

Let me spark dot read stream.

So Reed stream will read only new data.

It won't read the entire bronze table.

It will read only new data.

So we will get only new invoices.

We will not get the previous invoices.

And with that new invoices only if we are doing sum of total amount, we will get the sum of the total

amount in this invoice, in this data frame, in whatever invoices are there.

So basically we write code only for one micro batch.

Write all the code is executing on this data frame, which is only one micro batch worth of data.

Right.

So we are not writing any code here to take his previous total and calculate his current total like

this.

And then sum up the previous total and current total together.

We are not doing that.

That is behind the scenes happening automatically.

That's a feature offered by Spark Structured Streaming, and that feature works with the help of the

state store.

So I hope now it is crystal clear to you what is State Store.

Right.

And why do we need a state store?

We need a state store for calculating aggregates because correctly calculating aggregates is impossible

without knowing the previous totals right in in the incremental manner, we must know the previous total

and then current total and then sum it up together.

And that's what Spark Structured Streaming does behind the scenes for us.

So now let's go back to our main topic.

So now let's summarize it.

So we have two types of transformations in spark structured streaming.

One type is known as stateless transformation.

Another type is known as stateful transformations.

All the projection transformations like select filter, map, flat map, explode, etcetera.

There are many more.

All those are stateless.

Transformation.

Why is stateless transformation?

Because these guys, these transformations have nothing to do with the previous data.

They always work on the current data.

Right.

So for example let's say select or let's say map transformation.

What they will do if you think of it as a transformation in between, they will take some n number of

records, for example, ten records to apply the transformation on those ten records and produce maybe

ten another ten records, or sometimes more than ten, for example, explode or flatMap can produce

more than ten records.

But all that happens based on the current data or current input.

That's all.

It doesn't require information across the micro-batch or it doesn't require information.

What all the records we received earlier.

And that's why storing a state is not necessary, right?

So a spark structured streaming does not store any information regarding these transformations in the

state store.

And that's why they are known as stateless transformation.

Hope you understand that.

Come to stateful transformations.

All the grouping aggregation windowing joins all these transformations are stateful transformation.

Why?

Because it requires knowing data before the current Micro-batch you want to calculate aggregates or

averages, totals, sums all that requires what we have calculated till previous micro-batch what is

there in the current micro-batch, and use that previous micro-batch state to correctly calculate the

new state, right?

So that's why they are known as stateful transformations.

For all the stateful transformations, spark structured streaming behind the scene automatically creates

a state store.

It stores the information in the state store and manages that state store.

Right.

So that's how we classify all the transformations into stateless and stateful transformations.

But stateless and stateful has their side effects, right?

So let's try to understand those side effects.

The stateless transformation as a side effect does not support complete output mode.

We learned about three output modes right.

Append output mode, update output mode, and complete output mode.

These output modes are applicable to the right stream right.

So stateless transformations do not support complete output mode.

Why?

Because they do not maintain state.

They do not remember what we processed in the previous micro-batch.

So they do not have complete information, so they cannot support complete output mode.

If you are writing code and if you are implementing only stateless transformation and in the write stream,

if you are giving output mode as complete, you will see an error.

Spark will throw an exception stating that complete mode is not supported for stateless transformation.

So that's one side effect.

Another side effect is in the stateful transformations, all the stateful transformations behind the

scene implement a state.

A state is information its data, and that data is stored in the executor memory.

It is also saved in the checkpoint directory as a backup, so that it can be reloaded in the event of

restart, but it exists in the executor memory.

The state is always loaded into the executor memory.

That's an expensive operation, right?

You have some data in these executor memory, so.

But that's fine.

We are anyway loading data frame and bringing it into the executor memory so we can even load and bring

state information into the memory.

But the point is you have to manage the state.

You have to make sure that your state is not exponentially growing and you or you don't have an excessive

amount of state.

If that is the case, then you are going to run out of memory.

You will your job, your performance will degrade and eventually it may throw an out of memory exception.

So managing a state information is a critical decision.

You have to make sure that you are in control.

You know how much state you are going to need for this kind of operation, or whatever logic you are

implementing.

You should know you should have an estimate of how much state you are going.

The need for this?

And will your executors be able to handle that much data in the memory?

So that's an important design decision.

And as the result of this side effect, risk of causing out of memory exceptions at the executor due

to excessive size of a state, we have two approaches of implementing aggregations.

Right.

The first approach is manage the stateful aggregation.

That means you let spark manage the state and you create your logic using stateful operations.

And in that case, spark will create the state.

It will keep data in the state store, it will manage the state, and it will also do the cleanup when

it can do the cleanup.

I'll come back to the cleanup.

State store cleanup is an important concept, important idea to understand.

But spark should know when to clean up your state and when your state is, you useless or it expired.

And it is no further needed if spark knows that, it will also do the state store cleanup and your state

will remain in control, right?

It will not keep on growing excessively forever.

So we will come back to how state store cleanup works or what is state store cleanup.

But as an approach, we have two approaches.

One approach for implementing aggregation is use it managed state store by the spark.

Let spark manage the state store and the previous example that we created that is using the same approach.

We are allowing spark to create a state and manage it.

The second approach is custom stateless aggregation.

So in this approach we want to calculate the aggregates.

But we don't want to use the state store.

We don't want spark to create a state store right.

So in this we implement a custom logic.

We take a custom approach to implement the aggregation without using the state store.

And I call it custom stateless aggregation approach.

So we have two approaches to implementing aggregations.

One is using a stateful operations.

Another one is building custom logic to implement stateless aggregations.

Now, you might be wondering if we have two approaches when to use which approach?

When should I simply use stateful aggregations?

Let a spark create a state store and manage it.

And when we should implement a custom logic stateless transformation and implement the aggregation,

when should we use what?

So let's try to understand that aggregations are of two types.

The first type is called unbounded continuous aggregation, and second type of aggregation is known

as time bound aggregation.

What is the difference?

Let's try to understand with the example.

So in the previous lecture we implemented that example where we ingest the invoices, bring it into

our raw table.

And then we implemented a job from raw table to the aggregation table.

And the requirement was to calculate customer wise total sales and rewards.

That's all.

That's the requirement.

Calculate total sales and rewards for each customer.

So that kind of aggregation is unbounded continuous aggregation.

This is this guy, right?

Why?

It is unbounded?

Because our requirement doesn't says when do we stop calculating the aggregation.

Right.

So it is unbounded and continuous.

We keep on calculating the the total purchase and total rewards for a customer forever.

Right.

We keep on doing that.

So assume in a maybe a few months you got 1 million customers.

So what will happen?

Your state store size will grow up.

Will maintain 1 million records in the state store.

That's massive.

That's massive for your tiny spark structured streaming application, right.

You have to provide enough memory to hold that data.

So that kind of aggregation where we don't have an expiry condition for your aggregates, is known as

unbounded continuous aggregation.

Let's change that same requirement.

Right.

Change it a little bit.

Let's say complete monthly loyalty points for the customer.

Or compute monthly total sales and loyalty points for the customer.

Now we are putting a boundary around it.

Right.

And that boundary is defined by the time.

So the aggregate or the total that we are calculating.

Is valid only for one month period of time.

You start calculating it from the first of the month, right?

And keep on calculating total purchase for each customer.

Total rewards for each customer.

Keep on aggregating that till the month end, but at the month end it becomes invalid.

So idea is at the month end, whatever total rewards are accumulated for that customer, we post it

into a separate table called Customer Rewards.

And that's all.

Aggregation is done this now, this aggregation or this entire estate for all the customers for this

month becomes invalid.

Now, from the first of the next month, we restart calculating new aggregates for all the customers,

right?

So the first month's aggregate expires at the end of the month.

Next month's aggregate will be calculated from the zero.

It restarts from the zero.

We want to calculate it for one more month and at the end of the month that also expires.

And then we go and start calculating new aggregates for third month.

And this approach gives us many benefits.

The first benefit that we get is our aggregates will not grow up, even if you have 1 million customers

within a particular month, not all 1 million customers might be active, right?

They might not be purchasing something from you.

So even though you have a total customer base of 1 million, maybe 50,000 100,000 customers are active

in a given month.

They visited your store in that given month.

So your state size is limited to only 50,000 or 100,000 or whatever.

Your customers active customers are.

Right.

So that's a small right.

And next month you again focus on active customers.

Those are active in the next month.

And second benefit is that now since you have put in boundary, a time boundary is spark knows.

A spark engine knows okay, this month's aggregation is done.

Now we are calculating new month's aggregation.

This aggregation is now not valid, so it can clean up all those outdated aggregates from the state

store and shrink the state store.

So at any given point of time, you will have only 1 or 2 months active states maintained in the state

store.

So that's what the time bound aggregation means.

So if you have unbounded continuous aggregation requirement right, then it is recommended that you

implement the this custom stateless aggregation approach.

But if you have time bound aggregation requirement, if you can put a time boundary so that spark can

clean up the state, then it is safe to implement managed stateful aggregations.

You use the power of state store and aggregations by the.

A spark structured streaming.

Now how to implement the managed stateful aggregation you already learned.

We created an example.

It's super easy.

Use the group by and aggregate transformation.

Do your calculation.

It works flawlessly.

How to implement custom cells.

Stateless aggregation.

How to build your own logic for calculating aggregate without relying on the spark state store is the

topic for the next lecture.

We will implement one example to help you understand how to do that.

That's all for this lecture.

Hope you learned something meaningful.

See you again in the next lecture.

Keep learning and keep growing.
