Welcome back.

In this lecture we want to learn an approach for calculating aggregates without using state store.

So we want to develop a small application to understand how can we calculate the aggregates without

using spark structured streaming state store capability.

And we use these techniques for unbounded continuous aggregations.

So if you have a requirement where you want to calculate aggregates which is unbounded continuous which

is not time bound, you can apply these techniques to implement aggregation in your applications.

So let's start with the example.

Let me go to my workspace and show you an example that we created in the earlier lecture.

So we created streaming incremental aggregates in the earlier lecture.

This application calculates incremental aggregates for a customer rewards and customers total sale.

And it is unbounded.

So it keeps on calculating forever.

We don't have any time boundary to this kind of aggregate.

So I want to take that example, copy that same example and modify it to remove state store.

So let me clone this notebook.

And.

Give a new name to this.

Let's call it.

A streaming unbounded incremental aggregates.

Let's give it a name and then let me open this.

And let's call it 20.

Right.

We will also need a test suit for this.

So test suit remains the same.

Let me clone the test suit for the previous example.

And.

Call it.

Unbounded incremental aggregates.

Test suit.

Clone that.

Open that also.

That's all.

So let's come back to the code.

So just to recall a few things, we implemented two layers.

We implemented two classes.

The first class is known as bronze class.

The second one is known as gold class.

In the bronze class we are reading from the landing zone and without applying any transformations,

we are writing into the bronze layer table.

So that's the code is we wrote read invoices, will read data from the landing zone and return a data

frame.

And then the process method.

We will simply create the invoice and use the write stream to write it to the invoices table.

That's all.

So there is no change in this part of the code because we anyway need the raw layer.

So we ingest data from the landing zone insert into the invoices table or first part of the data.

Ingestion into the bronze layer or raw data layer is done.

The second class is the gold class.

What we are doing here we are reading using the read bronze method.

We are reading from the invoices table.

We have written a get aggregates function which will calculate total amount, sum of total amount and

also calculate reward points.

And we are doing group by customer card number.

So for each customer we are calculating sum of total and sum of rewards.

And this function is simple.

Then we have Upsert method here and save results method here and the process method here.

So in the process method we are doing everything.

We read data from the bronze layer using read bronze.

Then we calculate the aggregates and then we save the results, get the query and return it.

Now we know that.

If you look at the high level process, we start with the read stream in the read bronze and we finish

with the write stream in the save results.

Right?

Save results is doing write, stream and read bronze is doing read stream anything between read stream

and write stream.

Whatever logic, whatever code we write that is always a stream processing API, even if we use standard

spark data frame API.

Since read is read stream and write is write, stream is spark knows in between.

Whatever we are doing should be done as a streaming API, and if we are doing aggregates there, then

also spark knows that it is structured streaming aggregation.

So it will create a state store for the aggregation.

So what we want to do, we want to implement aggregation, but we don't want to use the state store.

So we will kind of take out this line.

Let me comment this right.

So we will read the bronze layer and we will not calculate aggregates between the read and the right.

Right.

So we will we are not going to calculate aggregates there.

So if we remove the aggregate aggregation will not happen.

And same way because there is no aggregation happening between the read and write, a spark will not

do any state store creation or it will not create a state store.

The spark will not do the aggregation.

We want to do the aggregation, but we will implement it later after the write stream.

That's the trick.

So let's do that.

So what we are doing in the save results, we get the results DF and we have results df dot write stream

query name is gold.

Update option.

We are giving checkpoint output mode is update.

All that is fine and we are using for each batch and the method name is Upsert method.

So let's change the method name.

I'm changing the name of the method as aggregate and Upsert.

Right.

So what do we want to do?

Instead of simply asserting the result into the target table, we want to create a different method

which will first calculate the aggregate and then upsert the result into the target table.

So we are planning to shift this aggregation after the write stream.

So we will not do aggregation.

We will not call this method here for calculating the aggregates.

We will simply read the stream and call the save results right.

So this should be invoices right?

So we are reading invoices DF and simply passing it to the write stream.

Not doing any processing not doing any aggregation in between.

Right.

So what we want to do, this result will come to the right stream and right stream will pass the data

frame of this micro.

Right the entire data to the Upsert aggregate Upsert method.

And in the aggregate Upsert method we will do the aggregation.

We will take care of everything and then we will do the Upsert.

So that's how we want to do it.

So let's implement it and it will make more sense.

So I change the method name to aggregate Upsert.

So this method name will change aggregate Upsert and aggregate Upsert takes one data frame as input.

From here we are passing the results df.

We have the results which is not an aggregated result, which is simply invoices DF right?

So we in fact we want to pass the invoice DF here.

So let's change the name to invoices DF so that we understand that it is invoices.

It is not a result, it's raw invoices which we received here.

And then once we have the invoices DF we want to calculate the aggregate there.

So this code I will cut from here.

Write and call here.

Right aggregate df.

So self dot get aggregates and pass the invoices and we get the aggregate df let's call it.

Rewards leave.

Or result, whatever.

So we got the reverse here, our result here.

So the idea is take the data, read the data, read the raw data.

If you have any stateless transformations you can implement here.

Because the stateless transformation doesn't create a state store.

And that's what we want to avoid.

We want to avoid creating a state store because finally we want to calculate a continuous unbounded

aggregate.

So you can put it here.

In our example we don't have any stateless transformation required.

So we don't have anything here.

So we don't do anything.

And after applying all these stateless transformation you can create one final dataframe and pass that

dataframe to save results.

Save results will simply pass that data frame to your callback method.

Aggregate Upsert.

And once you get the data in the aggregate upsert, then calculate the aggregate.

So calculated rewards df, which is my aggregate dataframe, and then we create a temp view of that

aggregate data frame.

We already have a merge statement here, and we are writing it to executing that merge statement.

So merge statement should be executed using this dataframe.

Right.

So because this is the dataframe which comes with the df right.

Rewards df is something I calculated here I created here.

So if I want to take out the spark session then I must be using this dataframe, the dataframe which

is coming as input to the function.

So I use this dataframe to get this session.

And in this session dot SQL I'm executing this merge statement and this merge statement will upsert

the results into the target table.

My target table is customer rewards, but we have a small problem here.

The get aggregate will calculate aggregates on the invoices, DF and invoices.

DF is only microbatch data, only incremental data.

Right?

So I calculated aggregate when I call get aggregates.

I'm calculating aggregates for this micro batch for only one micro batch.

But what I need to do I need passed aggregates plus this micro batch aggregate right until unless we

sum up both we are not calculating correct aggregates.

And that's why Spark Structured Streaming implements this data store.

It takes passed aggregates from the state store and adds the micro batches.

The current iterations aggregate to the passed aggregate and then gives us the result.

So we can do that here in the merge statement.

Right.

So what is my merge statement?

Merge into customer rewards where my final aggregates are there using customer rewards DF this temp

view, which becomes my source and my customer rewards is target search the data for existing customer

using customer card number when matched.

If we found a match.

If there is an existing customer, then target total amount equal to source total amount plus target

total amount.

Right.

And then similarly.

Target total points equal to source total points plus target total points.

So this target amount is as good as state stores target amount right.

It state is stores information.

That's what Spark Structured Streaming keeps on doing.

Whatever result we calculate the latest output.

It keeps a copy of that latest output in the state store.

We also keep that same copy in our target table.

So instead of taking it from the state store, I can simply take it from the target table, sum up for

the current iterations total, and update it back.

That's all simple, right?

And when not matched, if it is a new customer, insert a star, insert it's aggregate because for aggregate

for the current micro-batch is the actual aggregate.

That's all.

So that's the trick.

And it's super simple.

You compare it with the stateful aggregation.

In the stateful aggregation, we are reading data from the state store, and for that we are maintaining

a state store in the memory of an executor.

That is wasteful resource.

We are reading it from the state store.

That's a wasteful operation.

And ultimately, end of the day, we are anyway implementing this merge, right?

So we added some additional logic in the merge.

So we in this approach we are not doing any extra operation.

It performs in the same way.

But we are not using a state store.

So we are saving some resources from there.

And we are saving some operations, state store operations.

And my purpose should be solved here.

Right?

So that's all we are done with this and it should work perfectly fine.

That's all with the changes.

And now what is the next step?

Next step is to test it.

So let me go to my test suit and change this.

We are importing.

This code here should be importing this notebook.

My new notebook.

And here is my.

Aggregation test suit class.

I'm creating base directory, then doing cleanup tests and dropping invoice Z, dropping customer rewards

table, deleting the directories and then deleting the checkpoints, deleting the landing zone, and

recreating the landing zone.

I think we missed one thing.

Let me.

We are using.

A merge statement.

Right?

So merge statement expects that target table should already exist.

And our target table is customer rewards table.

So in the cleanup itself after dropping the customer rewards table I'm recreating the customer rewards

table here so that we don't get an error.

Merge statement complaining that table does not exist.

So that's all.

Ingestion remains the same assertion remains the same for bronze layer.

Assertion for gold layer also remains the same.

Await for Micro-batch.

I've changed it to 60s to be on safer side.

Sometimes 30s is not enough on a single node cluster.

Then run tests.

We are calling clean tests.

This one we are calling to enable Rocksdb status store.

Now we are not using a state store.

I eliminated the need for state store.

So this is not required.

So I'll remove that.

Right.

And then we are creating bronze calling the bronze, creating gold, calling the gold and rest.

All is standard.

Nothing new.

So I have code here.

Let's connect to a cluster and try to run it.

Let's see if it works.

Let's run it.

So clean up is started.

So branch stream started.

Do you want to see that?

Go to your cluster.

Spark UI and on the spark UI.

You can come to structured streaming and you will see your branch ingestion is running.

Your gold update is running.

Sometimes you may have to come here to see if there are any errors.

Sometimes everything looks like working here, and your validation failed because the query behind the

scene, the streaming query, failed due to some error and it might not show you error here.

So if you come here, you will see status has failed and you will also see error message somewhere in

that row.

So sometimes for even debugging or understanding why your queries are failing is you have to come here.

So both are running.

Let me refresh it once again.

Zero zero micro batch is already finished, so at least one micro batch for both the queries are finished

and we can see first iteration passed.

It's waiting for the second iteration when both the queries finished.

Second iteration you will see last batch updated as one.

So it's done first.

Micro-batch is also done, but we are still waiting for 60s.

I gave a little extra time.

So it shouldn't happen.

That one query finished, another query didn't finish.

And we are validating our results and we get some.

Assertion error.

So let's wait.

Okay.

Second iteration also passed it, ingested the third iteration data, and again waiting for 60s.

We go here, refresh it.

And see.

Okay, so for bronze injection next micro batch is done for gold is still next Micro-batch need to start.

So we keep on refreshing here and you will okay.

Second micro batch is done for the gold also, but we are still waiting for 60s so let's wait.

Hopefully last iteration will also pass the validation.

Then.

So all three iterations passed their validation.

What does it mean?

Even if we are not using state Store.

We managed to calculate the correct aggregates, the same kind of aggregates that we were getting when

we used the data store.

And that's the trick.

The idea is instead of using aggregation between read and write, you do all these stateless transformation,

whatever business logic you want to implement there, leave the aggregation for the last write.

Try to save the result in the right query and the right stream.

You call your callback method.

Pass the final data set there.

Calculate the aggregate for the current batch and some of it with the latest aggregate in your target

table in the merge statement itself.

And that's all.

Boom, you are done.

That's how we do stateless aggregation in streaming.

I hope you learned something new.

That's all for this lecture.

See you again.

Keep learning and keep growing.
