Welcome back.

In this lecture we want to understand two things.

The first thing is how do we calculate aggregates in real time or in near real time using spark structured

streaming.

How is spark calculates aggregates across the microbatches right.

And second thing that we want to understand is what is state store, what does it mean, where it is

stored and how it works.

So we will create one example and try to understand these two things in using an example.

So let's start.

Okay, so let me give you a quick introduction to the requirement that we are going to develop in this

lecture.

We already created a chain of streams in an earlier example.

So we created a bronze layer stream.

And then we created a silver layer stream.

And we chained both of these streams.

So similar example I'm going to take here.

Once again we will be using same dataset also.

We'll be using the same dataset also, and it's going to be your invoice stream.

Right.

So we want to develop two jobs here.

The first job reads data from the landing zone and creates a raw table in your bronze layer.

That's your first job.

You have already done that many times earlier, so hope that will be easy.

And then second thing that we want to do is to read data from the bronze layer, which is our raw table

layer.

Start reading it from there and calculate customer wise total sales and rewards.

Very simple aggregation.

Example.

We want to read data from our bronze layer, and we want to read it in a streaming mode.

As soon as data comes in the bronze layer table, we pick up it from there and calculate very simple

two aggregates.

One is total sales per customer.

So solution is simple.

Group by the customer and calculate the aggregate on the total sales amount.

And we also want to calculate the total rewards for that customer, which could be 1% or 2% of the total

sales amount.

That's calculated as rewarded points.

Right.

So very simple application.

We want to learn that.

We want to develop that.

And along with that example we want to learn three things here.

How is spark streaming calculates aggregates right.

We will be calculating aggregates here.

So we want to understand how Spark streaming calculates the aggregates behind the scene.

And what is Spark Streaming.

State store for calculating aggregates in real time or in near real time is part will behind the scene.

It will use the state store.

So we want to understand what is a state store, what is its role and how spark handles it.

The third thing we want to understand is to output modes, append mode, and complete mode.

Spark Structured Streaming gives you three output modes append mode, complete mode, and update mode.

So update mode we will cover in the next lecture.

In this lecture we will try to understand append and complete mode.

So let's start coding.

So let me go to my workspace and my project directory and create a new node.

Let's call it streaming aggregates.

The example is super simple and most of the things you have already done it, so I'm not going to type

in everything.

I'll copy and paste the code here and then explain it.

I hope you can understand that.

So let's create the first class here.

That's my bronze layer class.

The requirement for bronze layer is to read data from the landing zone.

And without implementing any transformation, insert that raw data into the bronze layer table.

That's all we want to do.

You have already done it many times.

Same code I have copied and pasted here, but let me quickly walk you through it.

So class name is bronze.

I have my constructor here which defines my landing zone base directory.

I have a get schema method here which defines the schema of an invoice.

We will be reading Json data file from the landing zone, and this method returns the schema of the

Json file.

And then requirement is super simple.

We want to read it and write it into a table.

So I have a read invoices method here which will be using read stream format is Json and we want to

explicitly specify the schema for the Json data file.

So we use get schema method and pass it in the schema option.

And then we load it from the landing zone.

That's all.

This is enough to read data from a landing zone.

But I want to add one more column here.

So this will after the load.

This much code will give me a data frame.

So now I can start applying data frame transformations on top of it.

Right.

So I'm adding one more transformation here with column transformation to add one extra file which is

input file and spark gives me a method or a function data frame function named as input file name.

So what it will do when we are reading data from a landing zone or from some directory, right?

And creating a data frame at that time, spark will capture which data file you are reading right for

each record.

So right.

So each record will be tagged with the data file name, and that file name we are storing into the input

file column.

It's a simple best practice when you are reading data from landing zone or ingesting data from a landing

zone source and building your raw layer or your bronze layer, it is always a good practice to keep

on adding the file name in your record so that you know from where you are getting this record.

So I'm doing that here.

Input file name will give me a file name.

When we are reading data from multiple files, each record will identify itself from which file we are

getting it.

And that's all.

We return the data frame from the read invoices.

And I have a process method here which will call the read invoices this guy and create a invoices DF.

And once I have invoices we don't want to apply any transformation.

It's a bronze layer process.

So we simply capture the raw data into a delta table.

So I have a write stream method here which will write the output to invoices underscore table.

And I'm applying necessary options like query name I'm giving query name as bronze ingestion.

I'm telling checkpoint directory where the spark streaming should create a checkpoint for this.

Invoices underscore under under the checkpoint directory.

And here is my output mode.

Output mode is append.

So Spark Streaming offers us three output modes for the write stream.

Output node modes are not applicable for read stream right?

It's not applicable here.

Output mode is applicable only for the write stream because we want to tell write stream method that.

How do you write or what do you give me as an output?

Right.

So when we say append mode, output mode is append.

That means whatever records you are reading in the read stream and applying transformation or whatever

records are finalized in the invoices here.

Right.

This is the data frame we want to write.

Right.

So invoices dot write stream.

So whatever records are there in the write stream, simply give me all the records, give me everything.

And and that's why we call it append mode.

And then we are using two table to insert everything into the table.

So if table already exists, spark will insert all the records from the invoices into the table.

If table does not exist, it will create the table and then insert the all the records.

So Spark Streaming runs as a microbatches, right?

So each micro batch will be reading some records, let's say 5000 records.

First micro batch is reading 5000 records.

So we will append 5000 records into this table.

Next micro batch is reading another five 6000 records.

So next micro batch will append all those records in the table.

So it's an insert only scenario.

And we use append mode wherever we have a insert only scenario.

We know that we are not going to update or we don't want to overwrite the complete table.

We use append mode.

Simple as that.

Whenever you have a requirement where you want to simply insert all the results of each micro-batch

into a target table, you can use append mode.

That's all.

So my bronze layer process is defined.

The next step that we want to do is to create a one more streaming job, which reads from the bronze

layer table.

From this invoices table calculates aggregates to aggregates.

We want to calculate.

One is total sales and another one is total rewards, which is 2% of the total sales.

Simple.

And we want to calculate these aggregates for each customer.

Right.

So it's simple as group by customer name.

And then calculate the sum of total.

And.

Sum of totals 2%.

And for that I want to create a new class and let's call it gold class.

And this is also simple.

There is nothing new which you haven't already learned, so I'll paste that code again here for the

simplicity and then explain.

So let's see what do we have in the gold class.

So this is my constructor I'm defining base data directory.

And first thing we want to read data from our bronze layer table where the previous job right.

The previous bronze class is inserting data.

So reading from a table is super simple.

We don't even have to tell too many options.

So spark dot read stream and you can use table method.

Table should be data table and you can use table method and tell the table name.

That's all.

The spark will start reading from this table right.

And then next step we want to do is once we are able to create a data frame reading data from a table,

we want to apply aggregate.

So next method I have defined is get aggregates and get aggregates.

Takes the invoices DF as an input and using that invoices DF.

What I'm doing group by customer card number because that's my customer ID group by customer ID and

calculate to aggregates.

The first aggregate is sum of total amount and we again give it an alias as total amount.

And the second aggregate is total amount into 0.02, which is 2% of the total amount, and give an alias

as total points.

This is an expression.

So we have to wrap this expression into expr function.

I hope you learned that in your spark programming course.

And that's all.

I'm using some method and method.

So I've imported those two functions here.

Some and expr.

Don't forget to import these functions, otherwise you will see an error.

So that's all.

My aggregate method is done.

Then once aggregate is calculated we want to save that into the target table.

Aggregate table.

So I have a save results method here which will take results data frame.

After calculating the aggregates and that results data frame simple we want to write it into a table.

So we write it as a write stream.

I give query name give the checkpoint location and I tell okay write it to a table.

Customer rewards write to table.

Customer rewards means create a delta table if it doesn't exist, and insert all the records there.

Or if table already exists, that's fine.

Insert the records there.

But we know that to table.

Write to table method is a insert only method.

We have already learned that in the earlier lecture right.

We know to table does not have the capability to update an existing record or implement merge kind of

requirements.

Right.

So two table will always insert new records.

And we are calculating aggregates in every micro-batch right.

In first Micro-batch I will read some files, calculate the aggregate for each customer and then write

it into the customer rewards table.

In next Micro-batch I will read some new files, whatever I have ingested.

Calculate aggregates.

And then what do we want to do?

We don't want to insert new aggregates in the same table, because some customers might be purchasing

again in the second file.

Also, I can get some purchases for the same customer.

So it is possible that customer 101 has purchased one item and I calculated his aggregates reading file

one reading the first data file and in the second data file.

Also that customer has purchased something else, right?

So we have two invoices for the same customer.

One invoice is coming in the first data file.

Another invoice is coming in the second data file.

First Micro-batch is reading first data file and second Micro-batch is reading second data file.

So we are calculating aggregate twice, right?

First time aggregated total sales for that customer inserted into the two table.

That's fine.

But second time his aggregate is changed.

Previous total sales plus this total sales.

So we have a new total sales for the same amount.

We cannot insert one more total sales for the same customer.

It must be merged.

Right.

But for the simplicity we don't want to merge.

What do we want to do for every micro-batch we want to overwrite the previous micro-batch result, and

for that we can use complete mode.

So complete mode is used.

With the right stream.

It's not allowed.

Output modes are not for read stream, right?

We cannot use it here.

Output modes are for right stream.

So we complete output mode which tells spark that give me complete data.

Complete data means data for this Microbatch plus data for the previous Microbatch plus data for the

previous two previous Microbatch.

So give me complete output for all Microbatches so that I can overwrite the table right without losing

the results.

So that's why I'm using complete output mode.

So what complete output mode will do every time it will overwrite the table?

It's not append right.

It's not insert operation.

It's an insert overwrite method.

That's what the two table will implement if you have an output mode as complete.

So what we will do every time we will overwrite the customer rewards table with the latest aggregates.

And that latest aggregates includes everything.

Everything, not only for the current Microbatch all the results calculated in the previous Microbatch

plus current Microbatch.

I'll show you that, but that's how complete mode works.

So if you have a requirement where you want to overwrite the results again and again with each micro

batch, you can use complete mode.

It's not very efficient, but if your target result is small, right?

Your target table customer rewards table is a small, maybe few thousands of records you can implement

complete mode.

It's easy to manage and easy to implement.

So that's all save results will do that.

And then finally I have a process method which will read data, read invoices from the landing zone

and create invoices.

And then we pass that invoices into get aggregates to calculate the aggregates, get the aggregated

and pass that aggregated to save results, to save it to the customer rewards table.

And in return we will get the streaming query which we will return from here.

That's all.

We are done with this.

What is next?

Let's write a test case.

For this.

We will implement test suit and so that we can test it.

So let's do that.

So let me create a new notebook.

And in parallel, also open my streaming aggregates notebook in a separate tab.

Let's call it streaming aggregates.

Test suit.

You have already created so many test suits.

Code is essentially same unless until something new is there in the test suit.

I want to simply copy paste the code and then explain it to you.

So let me copy paste some code here and then I will explain.

Just to save some time.

So test suit class name is aggregation test suit.

In the constructor I'm defining the base directory usual thing.

And I have a clean test method which will clean two tables, one bronze table, one your aggregate table.

And then we also delete the directories for both the tables.

We have two checkpoints.

So I'm cleaning both the checkpoints.

We have a landing zone.

So I'm cleaning directory for the landing zone and recreating the landing zone directory.

That's all useful I have ingest data method which will ingest data file into the landing zone from my

test data directory.

It will ingest into the landing zone.

That's simple.

Then I have written two methods.

One is assert bronze, another one is assert gold.

Because you have two classes, right?

One is your bronze layer class and another one is your gold layer class.

So assert bronze is a standard.

We read data from the bronze layer table invoices table.

Take account and compare it with the expected count.

So that's what I'm doing.

Select count star from invoices.

Take the actual count and compare actual count with the expected count.

That's.

A third gold is also same.

Kind of same.

So what do we want to do in the gold?

We want to take out the total amount for one customer.

We will be calculating total amount and total rewards for many customers in the result set right.

But for validation purpose I can take, only one customer can take the total amount for that one customer

and compare it with the expected total amount.

So that's what I'm doing in this assert goal.

What we are doing select total amount from customer rewards where customer card number is this.

So we'll take one total for only for one customer.

Take it in the actual count and compare it with the expected count to validate our results.

That's all.

Wait for Micro-batch is standard.

We want to wait for Micro-batch to pick up all the data and do the processing.

And then here is my run test method, which is simple.

We start with the clean up and then I.

Create an instance of bronze layer and start the bronze layer process.

Then create an instance of gold layer and start the gold layer process.

So both these streaming jobs are started even before we ingest data.

They keep on running.

They'll keep on running microbatches, or they will keep on waiting for data to arrive so that they

can start the micro batch.

Right?

So we start them and then we start testing our iterations.

So in the first iteration I want to ingest one data file, wait for both the micro batches to pick up

the data because they will be waiting for data.

As soon as I ingest, they will trigger the micro batch and start processing data.

So wait for 30s and then assert bronze layer.

I expect 501 records in the bronze layer after ingesting first data file and assert gold, so I expect

three 36,859 total total sale amount for this customer.

Right?

So I have manually calculated that and I'm using as an actual value and expected value.

And for our assertion of the gold layer.

So that's all that's the first iteration.

If all this passes we are done with the first iteration.

Similarly I have second iteration where 501 plus 500 more records are expected in the bronze layer and

20,740 more sales is expected from the in the gold layer for the same customer.

So in the second iteration, I should get total sales, whether it was purchased in the previous data

file, or we have an invoice for that same customer in the current data file sum of both the micro batches.

Right.

So that's what we want.

It should not be that each micro batch is giving its own total.

No we don't want that.

That's useless for us.

Total of the micro batch is not meaningful as for us right.

The meaningful total is for for a particular customer.

Give me final total as on this micro batch or as on this time.

Right.

So in the first micro batch total was 36,859.

In the second micro batch total is 20,740.

But what I expect the total for the customer should be sum of both.

Similarly, after ingesting third micro batch, the total for the customer should be sum of all three

micro batches.

Right?

And if that happens, we are good.

We are happy.

So that's all that is done.

Let me connect to my cluster and run this right.

I've pasted code from somewhere else which is already tested, so I don't expect any typos or errors

here, so it should work in one shot.

Let me run it.

I'll pause the video and resume when all tests are done.

So first iteration is done.

It's all passed.

Let's wait for the second iteration.

Second iteration is much more important than the first iteration, right?

Because we are calculating aggregates and we want spark to give us aggregate across micro batches.

We don't want spark to give us an aggregate for the micro batch.

And it passed.

So it looks like we are good right?

Let's wait for the third iteration also.

What's going on?

Oh, it's already done.

So good.

So we are done.

We created an example.

Calculated aggregates.

But one thing is still left.

Let's see how all this worked.

Right?

What happened behind the scene when we are calculating aggregates using a spark structured streaming.

So let's try to understand that.

So here is an Excel sheet that I have prepared.

So basically what I did in the example we are reading three data files in three iterations.

So I took first data file and taken out all the records for one particular customer.

This is the customer ID that we are testing.

Right.

So all the green records shown here are coming from the invoices one dot Json file.

Right for this particular customer.

So I have taken all these records here and taken only few fields like customer card number, invoice

number, created time and total amount.

I also took the input file name, so I am sure that these are coming from the first file, right?

And similarly all these pink records are for the second data file for the same customer.

And these blue ones are for coming from the third data file for the same customer.

So how is spark will do?

I am expecting that spark will trigger at least three micro batches for processing these three files.

Right?

When we ingest first file, spark will trigger one micro batch.

The first micro batch will see these many records for this customer in the invoices table.

That is our bronze layer table.

Right.

And my gold layer job will see all these customers and calculate the total amount.

So we have these many invoices.

Think 123456789 invoices in the first file for this customer.

Right.

And these are the amounts.

So if you do this sum of all these numbers you will end up with 3685, nine.

And that's the number I used for validating my first micro batch.

Total sum expected value of total sum.

Right.

So that's my total.

So first micro batch will see only these records nine records.

Do this sum and calculate 36,859 as the sum of the micro-batch.

That's all this result will go.

This total will go into the target table.

Done.

Then second Micro-batch will start when we ingest the second data file.

Second Micro-batch will start.

And the second Micro-batch will see these four pink records only.

Right.

And we'll do the sum of all these four invoices.

The sum is 20,740.

Right.

So sum of the total amount for the second micro-batch is 20,740.

But we don't want that.

We want this sum plus this sum for this customer.

Total sum till now.

Right.

So result of this second micro batch for this customer.

Total sum should be 57,599 which is sum of these two.

Right.

But how spark is getting these two sums together.

First micro batch will see only these records right.

It's an incremental read so it will calculate this sum.

Second micro batch will not read all this data once again because by default Spark structured streaming

is an incremental read.

It will not go back and read all data again and recalculate sum once again.

It will read only these four files, four records for this customer and calculate sum.

That sum is 20,740, but output is giving 57,000 9599.

How is spark is knowing this value?

That's the question.

And that's where State Store comes in.

So we already learned about the checkpoint directory.

Right.

In the checkpoint directory, spark is stores information about what file I processed in the previous

micro batch.

What are the new files we have so that it can differentiate what is new and process only new.

But along with that, spark, structured streaming also stores some state information, especially in

case of aggregations or joins.

Or there are some types of operations which require knowing data across micro batches.

Not for only one particular micro-batch, but across Microbatches.

All that information is also stored in the checkpoint directory as a state information.

So what will happen?

Let's try to understand it from the beginning.

First micro batch will trigger will read this nine invoices.

Calculate total 36,859.

It will give you that output in the target table.

But it will note down that information in the state store inside the checkpoint directory.

It will keep that information.

That first micro-batch output was 36,859.

Aggregate total amount for this customer aggregate was this.

Then we ingest the second file.

Spark streaming will trigger a new micro batch second micro batch which will read only these four records.

Right.

And then calculate total total is 20,740.

Right.

But what it will do, it will take what was the total for this customer from the state store for the

previous Micro-batch, which is 36,859.

So spark will smartly sum these with this and give you 57,599in the output, which goes to your target

table as an insert, and then save this also in the state store.

Basically, it will overwrite this value with this new value in the state store total sum up to the

previous successfully completed Micro-batch.

That's what goes in the state store.

So now state store will store 57,599.

Then we ingest the third file.

A new Micro-batch third micro batch will trigger which will read these files.

How many these records?

How many records are there?

1234567.

So it will read these seven records.

Calculate the total amount for these seven records, which is 31,959.

And then what it will do, take this 31,959, take the previous sum from the state store sum of these

two things and give you 89,558 as a result.

So and that's very powerful, right, for calculating aggregates across micro batches.

We don't have to.

Keep this estate right?

We don't have to remember the sum or the aggregates for the previous microbatches.

And then take some of the current micro-batch and combine both.

All that is automatically taken care of by the spark structure streaming.

And that's that's super powerful.

Right.

That's that simplifies our life.

We simply code okay.

Calculate aggregate.

That's all.

Give me complete result every time.

And that's all.

Spark will take care of everything.

We don't care whether behind the scene there are microbatches running, or it's processed by a single

micro-batch, or it's streaming or it is batch.

Whatever it is, we get our results, right.

That's how it works.

So in the target table, in your target table, you will see for this customer ID at the end of all

three Microbatches total reward is this, which is the latest value stored in the state store.

And total points is this which is correct?

Right.

And we are using complete output mode.

Right.

So what Spark Streaming will do will take all the totals from the state store.

I'm showing you a total for one customer ID right.

But if you have 5000 customer IDs in each batch, totals for all 5000 customer IDs will be stored in

the state store.

Right?

And when you are doing complete output, then spark will go take whatever is there in the state store.

Take everything.

And give you an as an output.

That's what the complete output mode means.

So we learned about append mode.

We learned about complete output mode.

Append mode will give you everything that is produced for the current micro batch.

That's all it will give you.

And assume that you are going to insert it into your target table.

That's all the complete mode.

After every micro batch, it will give you everything from the state store, all the aggregates for

all the customers or all the keys, everything it will give you from the state store whether it is changed

or not.

Irrespective of that, we will give you complete result every time and it can give because it has.

Everything is stored in the state store, so it will give you complete result so that you can override

the target table with the current state, right?

Current results.

That's how complete output mode works.

Hope it made sense, but complete output mode is a problem.

Why?

It is a performance problem, right?

Think of it I have I'm an e-commerce website.

I have 20 million customers and I'm calculating reward points for all those customers.

Every day, 20 million records.

I'm recalculating again and again and overwriting my target table.

That's going to take a lot of time, right?

And stream processing is expected to give me results quickly, work very fast and all that.

Right.

So complete mode is not suitable if your result set is too huge.

If it is a small result set 5000 10,000, 20,000, 100,000 records, it should be good.

But if your result set is in millions, if you want to maintain a state of millions of keys, then it's

a problem, right?

So complete mode is not there good for that purpose.

So how do we overcome that?

That's the topic for the next lecture.

We will see how do we overcome and implement these kind of aggregates in incremental mode or in update

mode using merge statement.

So see you in the next lecture.

Keep learning and keep growing.
