# Why Stream Processing?  
## Challenges When Moving from Batch to Streaming (Spark Structured Streaming Context)

---

## Context & Problem Statement

Traditional **batch data processing** works well when data freshness requirements are coarse-grained (hourly, daily, weekly). However, modern data products—dashboards, alerts, financial summaries—often require **near-real-time updates** (minutes, seconds, or “as soon as possible”).

This lecture establishes **why stream processing is needed**, and more importantly, **what breaks when we naïvely push batch architectures toward lower latencies**. It sets the foundation for understanding **why Spark Structured Streaming exists** and what problems it solves by design.

---

## Learning Objectives

By the end of this lecture, you should be able to:

- Distinguish **batch processing vs stream processing** based on latency requirements
- Identify **core challenges** that arise when batch pipelines are run at high frequency
- Understand why **incremental processing, checkpointing, and state management** are mandatory in streaming
- Recognize scenarios where **batch-based orchestration fails** and streaming becomes the correct abstraction
- Build intuition for how **Spark Structured Streaming addresses these challenges**

---

## Business Scenario: Trade Summary Reporting System

### Use Case

A **stock brokerage company** wants to generate a continuously updating trade summary chart:

- **X-axis:** Time (e.g., 9:00, 9:15, 9:30, …)
- **Y-axis:** Amount
- Metrics:
  - Total Buy Amount
  - Total Sell Amount
  - Settlement Amount = Buy − Sell

### Data Sources

Trades are generated from multiple channels:

- Desktop trading terminals
- Web applications
- Laptops
- Mobile apps

Each trade event represents either a **buy** or **sell** transaction with a timestamp and amount.

---

## Initial Solution: Batch-Oriented Data Engineering Design

### Standard Data Engineering Decomposition

The solution is designed using common data engineering principles:

Sources → Ingestion → Processing → Storage → Consumption


### Logical Architecture (Batch-Oriented)

1. **Ingestion Layer**
   - External systems push trades into a central collection system
   - A batch ingestion job pulls data every 15 minutes
   - Data is written as files into a **landing zone**

2. **Bronze Layer (Raw Data)**
   - Landing zone data is ingested into raw tables
   - Acts as the **system of record / source of truth**

3. **Silver Layer (Cleaned / Quality Data)**
   - Data quality checks
   - Schema standardization
   - Filtering invalid or corrupt records

4. **Gold Layer (Aggregated Results)**
   - Aggregations for:
     - Total buy
     - Total sell
     - Settlement amount
   - Stored for reporting and visualization

5. **Consumption**
   - BI tools or applications read from final tables to render charts

This follows the **Medallion Architecture (Bronze → Silver → Gold)**.

---

## Orchestration Model (Batch Thinking)

- Jobs are executed sequentially:

Job 0 → Job 1 → Job 2 → Job 3 → Job 4


- A workflow/orchestration tool (e.g., Airflow-like) triggers the pipeline
- The pipeline is scheduled at a fixed frequency

---

## The Critical Question: Pipeline Frequency

The customer requires:

> “Refresh the report **every 15 minutes**, continuously throughout the trading day.”

This requirement is the **pivot point** where batch thinking begins to break.

---

## Problem #1: Frequency vs Execution Time (Back Pressure)

### What Changes at High Frequency?

With a 15-minute refresh requirement:

- **Each pipeline run must fully complete within 15 minutes**
- This includes:
  - Cluster startup time
  - Data ingestion
  - Processing
  - Aggregations
  - Housekeeping tasks

### Real-World Constraint Example

- On-demand Spark cluster startup: **5–7 minutes**
- Remaining time for processing: **~8 minutes**
- Data volume keeps increasing (100K → 1M → 10M records)

### Back Pressure Defined

⚠️ **Back Pressure** occurs when:

- Processing time > scheduling interval
- Each run starts late
- Delays accumulate over time
- Eventually, the system falls hours behind real time

#### Symptoms

- Increasing lag
- Unbounded data backlog
- SLA violations

---

## Problem #2: Scheduling for Continuous or Near-Real-Time Processing

Batch schedulers are designed for:

- Daily
- Hourly
- Large, discrete runs

They **do not scale** to:

- Every second
- Millisecond-level responsiveness
- Continuous listening for new data

At this point, **schedule-driven execution is the wrong abstraction**.

---

## Problem #3: Incremental Data Processing

### Why Full Reprocessing Fails

Reprocessing all historical data every 15 minutes is:

- Wasteful
- Slow
- Costly

### Required Shift

Only process **new data since the last run**.

### Key Requirement

Each job must know:

- What was processed last time
- What is new now

This introduces **stateful execution**.

---

## Checkpointing: Core Streaming Primitive

### What Is Checkpointing?

Checkpointing is the mechanism that:

- Records **progress/state**
- Enables **incremental processing**
- Enables **restart from last known good state**

### Conceptual Flow

1. First run:
   - Process all available data
   - Record progress in checkpoint

2. Subsequent runs:
   - Read checkpoint
   - Compute delta (new data only)
   - Update checkpoint after successful processing

---

## Problem #4: Fault Tolerance & Exactly-Once Semantics

### Failure Scenario

- Pipeline runs successfully for 10 iterations
- Iteration 11 fails mid-way
- Job is restarted

### Correct Behavior

- Resume from iteration 11
- Do NOT:
  - Reprocess old data
  - Skip unprocessed data

### Core Challenge

Checkpoint updates and data writes must behave like a **single atomic transaction**:

Either:

Data + checkpoint are both committed
Or:

Neither is committed


Failure to do this leads to:

- Duplicates
- Data loss
- Inconsistent aggregates

---

## Problem #5: Late-Arriving Data & State Management

### Late Data Explained

A record:

- Occurs at time `T`
- Arrives at the system at `T + Δ`

Reasons:

- Network delays
- Retries
- Temporary system outages

---

### Why This Is Dangerous

Aggregations are typically **time-based**.

Example:

- At 10:00 → aggregate all trades between 9:00–10:00
- A trade from 9:59 arrives at 10:30

Previously computed results are now **incorrect** and must be corrected.

---

## Implications of Late Data

To fix late data correctly, the system must:

- Know **which window** the record belongs to
- Recompute affected aggregates
- Update previously published results
- Maintain historical aggregation **state**

This introduces:

- Stateful aggregations
- Windowing logic
- Result updates (not append-only)

---

## Summary of Streaming Challenges

When job frequency shrinks (minutes → seconds → milliseconds), **five fundamental problems emerge**:

1. **Back Pressure**
2. **Continuous Scheduling**
3. **Incremental Processing**
4. **Fault Tolerance**
5. **Late Data & State Management**

---

## Why Batch Alone Is Not Enough

Attempting to solve these problems in batch systems leads to:

- Complex custom logic
- Error-prone implementations
- Reinventing the same mechanisms per project

These challenges are **universal**, not application-specific.

---

## How Spark Structured Streaming Fits In

Spark Structured Streaming provides **built-in abstractions** for:

- Incremental processing
- Checkpointing
- Fault tolerance
- Late data handling
- Stateful aggregations
- Micro-batch and continuous execution

While preserving:

- **DataFrame APIs**
- **Spark SQL semantics**

---

## Key Takeaways

- Streaming is not “faster batch”
- Low latency fundamentally changes system design
- Checkpointing and state are unavoidable
- Late data invalidates naive aggregates
- Streaming frameworks standardize hard problems

---

## When to Use / When NOT to Use Streaming

### Use Streaming When:

- Data freshness is required in minutes or seconds
- Continuous updates are needed
- Back pressure and late data matter

### Do NOT Use Streaming When:

- Daily or hourly latency is acceptable
- Data volumes are small
- Simpler batch pipelines meet SLAs

---

## Common Mistakes

- Running batch jobs more frequently instead of switching paradigms
- Ignoring late-arriving data
- Writing custom checkpoint logic incorrectly
- Assuming streaming is append-only

---

## Interview-Relevant Insights

- Why batch fails at real-time
- Explain back pressure
- What is checkpointing and why it matters
- Handling late-arriving events
- Why state is essential in streaming systems
