# Lecture: Building a Word Count Pipeline ‚Äî From Batch Processing to Streaming Foundations

## Context & Problem Statement

The goal is to design and implement a **Word Count data processing pipeline** that evolves incrementally:

1. Start by clearly framing the **problem statement**
2. Implement a **batch processing solution**
3. Identify **limitations of batch processing**
4. Prepare the foundation to **transition to a streaming solution**

This lecture focuses on **steps 1‚Äì3**, laying a strong architectural and engineering base before introducing streaming.

---

## Learning Objectives

By the end of this lecture, you should be able to:

- Design a batch-oriented data processing pipeline using Spark
- Apply data quality rules before aggregation
- Structure Spark code for **testability and maintainability**
- Implement **automated batch test suites**
- Understand why batch processing struggles with low-latency requirements
- Prepare batch pipelines for seamless migration to streaming

---

## High-Level Solution Overview

### Problem Definition: Word Count Application

We want to build a system that:

- Ingests **text files** into a landing zone
- Reads and processes the files using Spark
- Applies **data quality rules**
- Computes **word counts**
- Stores results in a **Word Count table**
- Enables downstream reporting and analytics

### Reporting Requirement

- **X-axis:** Alphabet letters (A‚ÄìZ)
- **Y-axis:** Count of words starting with each letter  
  Example:
  - `A ‚Üí count of words starting with 'a'`
  - `S ‚Üí count of words starting with 's'`

---

## Architecture & Data Flow

### Batch Processing Architecture

Source Files
‚Üì
Landing Zone (DBFS: /data/text)
‚Üì
Raw DataFrame (lines)
‚Üì
Raw Words DataFrame
‚Üì
Data Quality Rules
‚Üì
Quality Words DataFrame
‚Üì
Aggregation (Word Count)
‚Üì
Delta Table (word_count_table)
‚Üì
Reporting / Consumers


### Key Characteristics

- **Processing Type:** Batch
- **Trigger:** Scheduled (e.g., every 15 minutes)
- **State Management:** Stateless between runs
- **Storage Format:** Delta Lake
- **Execution Model:** Spark DAG (Stages ‚Üí Tasks)

---

## Data Sources & Sinks

| Component | Type | Description |
|---------|-----|-------------|
| Source | Text files | Uploaded to DBFS landing zone |
| Landing Zone | File system | `/data/text` |
| Processing Engine | Apache Spark | Batch execution |
| Sink | Delta Table | `word_count_table` |
| Consumers | BI / Reports | Aggregate-level queries |

---

## Batch Processing Implementation Details

### Step 1: Read Raw Data

- Input format: **Text**
- Line separator: `.` (dot)
- Each line loaded into a `value` column

**Pseudo-code:**
```python
spark.read
     .format("text")
     .option("lineSep", ".")
     .load(base_dir + "/data/text")
```
### Step 2: Create Raw Words DataFrame

- Select value column
- Split lines into words
- Explode arrays into rows

**Pseudo-code:**
```python
lines.select(
  explode(split(col("value"), " ")).alias("word")
)
```

### Step 3: Apply Data Quality Rules

#### Quality Rules Applied:
- Trim whitespace
- Convert to lowercase
- Remove nulls
- Retain only words starting with alphabetical characters (a-z)

#### Why This Matters:
-Prevents garbage data from polluting aggregates
- Ensures consistent grouping and accurate counts
- Reduces downstream data anomalies

**Pseudo-code:**
```python
words
  .select(lower(trim(col("word"))).alias("word"))
  .filter(col("word").isNotNull())
  .filter(col("word").rlike("^[a-z]"))
```
### Step 4: Aggregation Logic

- Group by word
- Compute count

**Pseudo-code:**
```python
quality_words.groupBy("word").count()
```

### Step 5: Persist Results

- Format: Delta
- Mode: Overwrite
- Target: Managed table

**Pseudo-code:**
```python
word_counts.write
           .format("delta")
           .mode("overwrite")
           .saveAsTable("word_count_table")
```

### Code Structuring for Testability
### Why Refactoring Is Required

A linear notebook-style script:
- ‚ùå Hard to unit test
- ‚ùå Hard to reuse
- ‚ùå Hard to validate intermediate outputs

#### Refactored Design: Class-Based Architecture
BatchWordCount
 ‚îú‚îÄ‚îÄ get_raw_data()
 ‚îú‚îÄ‚îÄ get_quality_data()
 ‚îú‚îÄ‚îÄ get_word_count()
 ‚îú‚îÄ‚îÄ overwrite_word_count()
 ‚îî‚îÄ‚îÄ word_count()   # Orchestration method

#### Benefits
- Enables unit testing
- Improves readability and maintainability
- Supports integration and regression testing
- Production-ready structure

### Automated Testing Strategy

#### Test Suite Responsibilities
- Ensure clean environment before execution
- Simulate multiple ingestion iterations
- Validate correctness after each batch run

### Test Suite Architecture
BatchWordCountTestSuite
 ‚îú‚îÄ‚îÄ clean_tests()
 ‚îú‚îÄ‚îÄ ingest_data(iteration)
 ‚îú‚îÄ‚îÄ assert_result(expected_count)
 ‚îî‚îÄ‚îÄ run_tests()

### Test Data Ingestion Logic

- Copies predefined test files into the landing zone
- Supports multiple iterations:
    - Iteration 1 ‚Üí text_data_1.txt
    - Iteration 2 ‚Üí text_data_2.txt
    - Iteration 3 ‚Üí text_data_3.txt

**Pseudo-code:**
```python
dbutils.fs.cp(
  f"/test-data/text_data_{iteration}.txt",
  base_dir + "/data/text"
)
```

### Assertion Logic
- Query word_count_table
- Filter words starting with 's'
- Compare expected vs actual counts

**Pseudo-code:**
```python
actual = spark.sql("""
  SELECT SUM(count)
  FROM word_count_table
  WHERE word LIKE 's%'
""").collect()[0][0]

assert actual == expected
```

### Incremental Batch Testing
|Iteration|	Files Ingested|	Expected Count (words starting with 's')|
|---------|---------------|-----------------------------------------|
|1 |1 file|	25|
|2 |2 files|32|
|3 |3 files|37|

Each iteration validates:
- Idempotent execution
- Correct aggregation behavior
- Stability across incremental runs

### Performance Considerations ‚öôÔ∏è

- Entire dataset is reprocessed every run
- Increasing data volume increases latency linearly
- Overwrite mode rewrites full Delta table
- High cluster startup and scheduling overhead

### Design Trade-offs ‚öñÔ∏è
|Choice|	Benefit|	Limitation|
|------|---------|------------|
|Batch Processing|	Simpler logic|	High latency|
|Overwrite Writes|	Easy correctness|	Inefficient for large data|
|Stateless Runs|	Predictable|	No incremental efficiency|


### Production Workflow Design
#### Scheduled Batch Pipeline

- Job 1: Data ingestion from source ‚Üí Landing Zone
- Job 2: Batch Word Count pipeline
- Workflow: Job 1 ‚Üí Job 2
- Schedule: Every 15 minutes

### Limitations of Batch Processing

- Cannot support:
    - Near real-time updates
    - Sub-minute latency
    - Event-driven refreshes

- Reducing schedule frequency increases:
    - Cluster costs
    - Duplicate processing
    - Resource contention

### Why Streaming Becomes Necessary

#### Business Requirement Shift:
Refresh reports immediately after data arrives
(seconds or milliseconds, not minutes)

Batch processing fundamentally cannot satisfy:
- Continuous ingestion
- Event-time processing
- Stateful incremental updates

This motivates the transition to stream processing, which is addressed in the next lecture.

### Key Takeaways

- Batch pipelines are ideal for foundational correctness
- Code structure determines testability and scalability
- Automated testing is essential for data pipelines
- Batch processing has inherent latency limits
- Proper batch design simplifies future streaming migration

### When to Use / When NOT to Use

#### Use Batch When:
- Latency tolerance is minutes or more
- Data arrives in bulk
- Simplicity and reliability are priorities

#### Avoid Batch When:
- Real-time or near-real-time insights are required
- Data arrives continuously
- SLA demands sub-minute freshness

### Common Mistakes
- Writing monolithic notebook code
- Skipping automated testing
- Ignoring data quality rules
- Using batch pipelines for real-time use cases

### Interview-Relevant Insights üéØ

- Explain why batch fails for low latency
- Discuss testable Spark design patterns
- Highlight Delta overwrite vs incremental updates
- Describe path from batch to structured streaming
- Emphasize code structure over raw Spark APIs


